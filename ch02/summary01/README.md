# 2 CUDA Programming Model (Part I)

> [Managing Devices](https://github.com/erectbranch/CUDA_Basic/blob/master/ch02/management.md)

ì´ë²ˆ ì¥ì€ vector addition, matrix addition ì˜ˆì œë¥¼ CUDA programìœ¼ë¡œ ì‘ì„±í•˜ë©° ì‚´í´ë³¼ ê²ƒì´ë‹¤. 

---

## 2.1 Graphics card

> [A complete anatomy of a graphics card: Case study of the NVIDIA A100](https://blog.paperspace.com/a-complete-anatomy-of-a-graphics-card-case-study-of-the-nvidia-a100/)

ë“¤ì–´ê°€ê¸° ì•ì„œ ì ì‹œ GPUì˜ PCB ê¸°íŒ, ì¦‰ Graphics cardê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. í”íˆ GPUë¥¼ Graphics card ìì²´ë¡œ ì°©ê°í•˜ëŠ” ê²½ìš°ê°€ ë§ì§€ë§Œ, GPUëŠ” Graphics cardì˜ ì¼ë¶€ë¶„ì´ë‹¤.

![A100 PCB](images/A100_PCB.png)

- GPU chip ë‚´ë¶€ì—, CUDA coreì™€ SRAM ë“±ì´ ì¡´ì¬í•œë‹¤.

- ì™¸ë¶€ëŠ” GPUì—ì„œì˜ DRAMì— í•´ë‹¹ë˜ëŠ” Video RAM(VRAM)ìœ¼ë¡œ GDDR SGRAM memoryë¥¼ ê°–ëŠ”ë‹¤.

  > [DDRê³¼ GDDRì˜ ì°¨ì´](https://news.skhynix.co.kr/post/ddr-gddr)

ì—¬ê¸°ì„œ GDDR SGRAMì€ **Graphics Double Data Rate** SGRAM(Synchronous Graphics DRAM)ì˜ ì•½ìë‹¤. 3D graphic ì²˜ë¦¬ë¥¼ ë³´ë‹¤ ì›í™œí•˜ê²Œ í•  ìˆ˜ ìˆëŠ” (pixelì˜ ê¹Šì´ ì •ë³´ë¥¼ ë‹´ëŠ”) Z bufferë¥¼ ì¥ì°©í•˜ëŠ” ë“±, GPUëŠ” graphic ì²˜ë¦¬ì— íŠ¹í™”ëœ RAM(SGRAM)ì„ ê°–ê²Œ ë˜ì—ˆë‹¤.

- GDDRì€ dataë¥¼ ì½ê³  ì“¸ ìˆ˜ ìˆëŠ” í†µë¡œì¸ **strobe**(ìŠ¤íŠ¸ë¡œë¸Œ)ë¥¼ ì¼ë°˜ DRAMë³´ë‹¤ í›¨ì”¬ ë§ì´ ê°–ê³  ìˆë‹¤.

  > DDR(Double Data Rate)ë€, ê¸°ì¡´ SDR(Single Data Rate)ê°€ clock rising edgeì—ì„œë§Œ dataë¥¼ ì „ì†¡í•˜ëŠ” ë°©ì‹ì—ì„œ, clock rising, falling edgeë¥¼ ì´ìš©í•´ 2ë°°ì˜ dataë¥¼ ì „ì†¡í•  ìˆ˜ ìˆê²Œ ëœ ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤. ì„¸ëŒ€ê°€ ì§€ë‚ ìˆ˜ë¡ bus clock rateëŠ” í–¥ìƒì‹œí‚¤ê³ , ì†Œëª¨ voltageëŠ” ë‚®ì¶”ê³  ìˆë‹¤.

> ìµœì‹  ê¸°ìˆ ë¡œ ë” ë‚®ì€ latencyì™€ ë†’ì€ bandwidthë¥¼ ì–»ê¸° ìœ„í•´, GDDR ëŒ€ì‹  **HBM**(High Bandwidth Memory)ë¥¼ ì¥ì°©í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. í•˜ì§€ë§Œ ë¹„ìš©ì´ë‚˜ êµ¬í˜„ ë‚œì´ë„, ìš©ëŸ‰ì˜ í™•ì¥ ë¬¸ì œ ë“±ì„ ì•ˆê³  ìˆë‹¤.

---

### 2.1.1 spectrum of GPU

**GPU**(Graphics Processing Unit)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ graphic renderingì— í•„ìš”í•œ ë³µì¡í•œ mathematical, geometric calculationì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ processorì´ë‹¤.

- CPUì™€ ë¹„êµí–ˆì„ ë•Œ ë” ë§ì€ transistorì™€ ALUë¥¼ ê°€ì§„ë‹¤.

ì´ëŸ¬í•œ ì„¤ê³„ ëª©ì  ë•Œë¬¸ì— ì´ˆê¸° GPUëŠ” ì˜¤ë¡œì§€ graphics pipelineì˜ íŠ¹ì • ë¶€ë¶„ì„ accelerateí•˜ê¸° ìœ„í•œ deviceë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ GPUê°€ ë°œì „í•˜ë©´ì„œ ì¼ë°˜ì ìœ¼ë¡œ CPUê°€ ì²˜ë¦¬í–ˆë˜ ê³„ì‚°ì„ ì²˜ë¦¬í•˜ê¸° ì‹œì‘í•˜ê³ , general purpose computingì—ë„ ì‚¬ìš©ë˜ê¸° ì‹œì‘í–ˆë‹¤.

> ì´ˆê¸°ì—ëŠ” **GPGPU**(General Purpose GPU)ë¼ê³  êµ¬ë¶„í•´ì„œ ë¶€ë¥´ê¸°ë„ í–ˆë‹¤.

ë”°ë¼ì„œ í˜„ì¬ì˜ GPUëŠ”, 'íŠ¹ì • ëª©ì ì— **specialized**ëœ' **accelerator**(ê°€ì†ê¸°)ê°€ ì•„ë‹ˆë¼, general purpose computingì— ì‚¬ìš©ë˜ëŠ” (data parallelismì— íŠ¹í™”ëœ) **processor**ë¡œ ë³´ëŠ” í¸ì´ ë” ì •í™•í•˜ë‹¤.

![flexibillity, performance/power efficiency](images/devices_flexibillity_performance.png)

- processor: **flexibillity**ê°€ ë†’ë‹¤.

  - CPU, GPU

- accerelator: **performance**ì™€ **power efficiency**ê°€ ë†’ë‹¤.

  - FPGA(Field Programmable Gate Array), ASIC(Application Specific Integrated Circuit)

> í˜„ ì‹œì ì—ì„œëŠ” one-size-fits-all(ëª¨ë“  ì¼ì— ë§ŒëŠ¥ì¸) processorëŠ” ë‹¹ì¥ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ë³´ë©°, ëŒ€í˜• chipì— ì„œë¡œ ë‹¤ë¥¸ deviceë¥¼ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ë°©ì‹ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.

---

## 2.2 GPU architecture

GPUëŠ” unitì„ ë„¤ ê°€ì§€ ë¶„ë¥˜ë¡œ ë‚˜ëˆˆë‹¤.

- **Streaming Multiprocessors**(SMs)

- **Load/Store**(LD/ST) units

  dataì˜ asynchronous copyê°€ ê°€ëŠ¥í•˜ë‹¤. ì¶”ê°€ thread resourceì˜ ì‚¬ìš© ì—†ì´, threadë“¤ì´ globalí•˜ê²Œ shareí•  ìˆ˜ ìˆë„ë¡ dataë¥¼ loadí•œë‹¤.

- **Special Function Units**(SFU)

  vectored dataì—ì„œ sine, cosine, reciprocal, square root ë“±ì˜ functionì„ ê³„ì‚°í•œë‹¤.

- **Texture Mapping Units**(TMUs)

  image rotation, resizing, adding distortion, noise, moving 3D plane objects ë“±ì˜ taskë¥¼ ì²˜ë¦¬í•œë‹¤.

---

### 2.2.1 Streaming Multiprocessors

**SM**(Streaming Multiprocessor)ì€ ë‹¤ìŒ ìš”ì†Œë¡œ êµ¬ì„±ëœ execution entityì´ë‹¤.

- register spaceë¥¼ ê³µìœ í•˜ëŠ” core ë¬¶ìŒ.

  > NVIDIAì—ì„œëŠ” CUDA coresì™€ Tensor cores, AMDì—ì„œëŠ” Stream processorsë¼ê³  ë¶€ë¥¸ë‹¤.

  > Tensor coreëŠ” ML applicationì— íŠ¹í™”ëœ coreë¡œ, MLì—ì„œëŠ” í›¨ì”¬ ë¹ ë¥¸ ì—°ì‚° ì†ë„ë¥¼ ë³´ì´ì§€ë§Œ í‰ë²”í•œ ì—°ì‚°ì€ ì œëŒ€ë¡œ ìˆ˜í–‰í•˜ì§€ ëª»í•œë‹¤.(CUDA coreê°€ clock cycleë‹¹ í•˜ë‚˜ì˜ operationë§Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì— ë¹„í•´, Tensor coreëŠ” cycleë‹¹ ì—¬ëŸ¬ ê°œì˜ operationì„ ìˆ˜í–‰í•œë‹¤.)

- shared memoryì™€ L1 cache

ì•„ë˜ëŠ” A100ì˜ cache hierarchyë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. SMì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆê³ , VRAMê³¼ ì–´ë–»ê²Œ data transferê°€ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì‚´í´ë³´ì.

![A100 cache hierarchy](images/A100_cache_hierarchy.png)

- ê° SMì€ shared memoryì™€ L1 cacheë¥¼ ê°–ëŠ”ë‹¤.

- L2 cacheëŠ” ëª¨ë“  SMì— unified, sharedë˜ì–´ ìˆë‹¤. HBM2(VRAM)ì˜ ê°€êµ ì—­í• ì„ í•œë‹¤.

---

### 2.2.2 benifits of using GPUs, latency hiding

CPUì˜ í•œ coreê°€ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ threadë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë“¯ì´, GPU ì—­ì‹œ SM ë‚´ë¶€ì˜ í•œ coreê°€ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ threadë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì„¤ê³„ ì² í•™ì— ë”°ë¼ ì„¸ë¶€ì ì¸ ì°¨ì´ëŠ” í¬ë‹¤.

- CPU: ì†Œìˆ˜ì˜ coreë¡œ ìˆ˜ì‹­ ê°œì˜ threadë¥¼ parallelí•˜ê²Œ ìˆ˜í–‰.

- GPU: ìˆ˜ë°± ê°œì—ì„œ ìˆ˜ì²œ ê°œì— ë‹¬í•˜ëŠ” CUDA coreë¡œ, ìˆ˜ì²œ ê°œì˜ threadë¥¼ parallelí•˜ê²Œ ìˆ˜í–‰.

CPUì™€ GPUì˜ ì°¨ì´ë¥¼ cache hierarchyë¥¼ ë¹„êµí•˜ë©° ì‚´í´ë³´ì.

![CPU vs GPU](images/cpu_vs_gpu.png)

- CPU: chip ë‚´ë¶€ì˜ ì ˆë°˜ì„ cacheê°€ ì°¨ì§€í•˜ê³  ìˆë‹¤. 

  - **latency stall**ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì¤‘ì ìœ¼ë¡œ ì„¤ê³„í•˜ì˜€ë‹¤.

  - ë”°ë¼ì„œ ì¬ì‚¬ìš©ì„±ì´ ë†’ì€ dataë¥¼ cacheì— ì €ì¥í•´ë‘ê³ , cache hitê°€ ë°œìƒí•˜ë©´ ë¹ ë¥´ê²Œ dataë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. 

  - branch prediction, prefetching ë“± pipeline hazardë¥¼ ë§‰ê¸° ìœ„í•œ ì—¬ëŸ¬ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤.

- GPU: cacheê°€ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì´ ì ì€ ëŒ€ì‹ , ëŒ€ë¶€ë¶„ì„ core(ALU)ê°€ ì°¨ì§€í•˜ê³  ìˆë‹¤.

GPUëŠ” cacheê°€ ì ì–´ì„œ VRAMì— ì ‘ê·¼í•  ë•Œë§ˆë‹¤ penaltyë¥¼ ê²ªê²Œ ëœë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê²ªì„ ë•Œ ë‹¤ë¥¸ warpê°€ ëŒ€ì‹  taskë¥¼ ì´ì–´ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ **latency hiding**ì„ í•  ìˆ˜ ìˆë‹¤.

GPUê°€ instructionì„ ì‹¤í–‰í•´ì•¼ í•œë‹¤ê³  í•˜ì. ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ idle(í˜¹ì€ wasted) timeì„ ë°œìƒì‹œí‚¤ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤.

- ê¸°ì¡´ì˜ long-letency operation(memory access ë“±)ìœ¼ë¡œ ê¸¸ê²Œ ê¸°ë‹¤ë ¤ì•¼ í•˜ëŠ” warpëŠ” ì„ íƒí•˜ì§€ ì•ŠëŠ”ë‹¤.

- ì‹¤í–‰ ì¤€ë¹„ê°€ ëœ warpë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰í•œë‹¤.

  > (readyì¸ warpê°€ ì—¬ëŸ¬ ê°œ ìˆë‹¤ë©´, priority mechanismì— ë”°ë¼ ì„ íƒí•œë‹¤.) 

> [warp ì •ë¦¬](https://alpaka.readthedocs.io/en/0.5.0/usage/abstraction/warp.html): **warp**ë€ <U>ë™ì¼í•œ instructionì„ (parallelí•˜ê²Œ) ìˆ˜í–‰í•˜ëŠ” thread ë¬¶ìŒ</U>ì„ ì˜ë¯¸í•œë‹¤.(ì´ thread 32ê°œë¡œ êµ¬ì„±ëœ single execution unit)

> ë‹¤ì‹œ ë§í•´ warp ë‚´ ì–´ë–¤ í•œ threadê°€ ì–´ë–¤ instructionì„ ì‹¤í–‰í•˜ë©´, ë‚˜ë¨¸ì§€ ë‚´ë¶€ì˜ threadë“¤ë„ ë™ì¼í•œ instructionì„ ì‹¤í–‰í•´ì•¼ í•œë‹¤. **SIMT**(Single Instruction Multiple Thread)ë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ .

```
// ë§Œì•½ 128ê°œì˜ ê°€ëŠ¥í•œ threadê°€ ìˆë‹¤ë©´, 4ê°œì˜ warpë¡œ partitionëœë‹¤.
Warp 0: thread  0, thread  1, thread  2, ... thread 31
Warp 1: thread 32, thread 33, thread 34, ... thread 63
Warp 2: thread 64, thread 65, thread 66, ... thread 95
Warp 3: thread 96, thread 97, thread 98, ... thread 127
```

> processê°€ ì€í–‰ì´ë¼ë©´, warpëŠ” ì€í–‰ ì°½êµ¬ì˜ ì€í–‰ì›ì— í•´ë‹¹ëœë‹¤.

![latency hiding](images/latency_hiding.png)

> ë‹¨, warpê°€ ë¶€ì¡±í•˜ë©´ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ latency hidingì— ì‹¤íŒ¨í•  ìˆ˜ë„ ìˆë‹¤.

---

## 2.3 CUDA programming model

programming modelì´ë€, hardwareìƒì—ì„œ ë™ì‘í•˜ëŠ” applicationì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ computer architectureì„ abstractioní•œ ê²ƒì„ ì˜ë¯¸í•œë‹¤.

> programming languageë‚˜ programming environment í˜•íƒœë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ programê³¼ programming model êµ¬í˜„ì— ìˆì–´ì„œì˜ abstractionì„ ê³„ì¸µ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

> CUDA programming modelì„ ë³´ë©´ GPU architectureê°€ ê°–ëŠ” memory hierarchyì˜ abstractionì„ ì•Œ ìˆ˜ ìˆë‹¤.

![abstraction layer](images/programming_model_layer.png)

ë˜í•œ ë‹¤ë¥¸ parallel programming modelì—ì„œ abstractionë“¤ì„ shareí•˜ê¸° ìœ„í•´ì„œ, CUDA programming modelì€ ë‹¤ìŒê³¼ ê°™ì´ GPUë¥¼ ì œì–´í•˜ëŠ” íŠ¹ì§•ë“¤ì„ ê°–ëŠ”ë‹¤.

- organize threads on the GPU through a hierarchy structure

- access memory on the GPU through a hierarchy structure


í”„ë¡œê·¸ë˜ë¨¸ ê´€ì ì—ì„œëŠ” parallel computationì„ ë‹¤ìŒê³¼ ê°™ì€ levelë¡œ ë³¸ë‹¤. 

- domain level

    programê³¼ algorithmì„ ë””ìì¸í•  ë•Œ í•´ë‹¹í•˜ëŠ” levelì´ë‹¤. ì–´ë–»ê²Œ dataì™€ functionì„ **decompose**(ë¶„í•´)í•´ì•¼ íš¨ìœ¨ì ìœ¼ë¡œ programì´ parallelí•˜ê²Œ ìˆ˜í–‰ë ì§€ ê³ ë¯¼í•œë‹¤.

- logic level

    programê³¼ algorithm ë””ìì¸ì´ ëë‚˜ë©´ programming ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤. ì–´ë–»ê²Œ êµ¬ì„±í•´ì•¼ logicì„ concurrent threadë“¤ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•œë‹¤. 

- hardware level

    threadë¥¼ íš¨ìœ¨ ì¢‹ê²Œ coreì— mappingí•˜ëŠ” ë°©ë²• ë“±ì„ ê³ ë ¤í•œë‹¤.

---

## 2.4 CUDA Programming Structure

> [CUDA ê¸°ì´ˆ](https://velog.io/@lunarainproject/CUDA-%EA%B8%B0%EC%B4%88)

- ì±…(2014 ë°œê°„)ì—ì„œëŠ” CUDA 6ìœ¼ë¡œ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

- host(CPU) memoryëŠ” variable ì´ë¦„ ì•ì— h_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

- device(GPU) memoryëŠ” variable ì´ë¦„ ì•ì— d_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œ CPUì™€ GPU ì‚¬ì´ì— ê³µìœ ë˜ëŠ” managed memory poolì„ í˜•ì„±í•˜ëŠ” **unified memory**ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ë•ë¶„ì— CPUì™€ GPU memory ëª¨ë‘ ë‹¨ì¼ pointerë¥¼ ì‚¬ìš©í•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. unified memoryì— allocateëœ dataë¥¼ hostì™€ device ì‚¬ì´ì— ìë™ìœ¼ë¡œ **migrate**í•œë‹¤.

CUDAì˜ í•µì‹¬ì€ kernelì´ë‹¤. CUDAëŠ” GPU threadì—ì„œ ë™ì‘í•˜ëŠ” kernelë“¤ì„ schedulingí•œë‹¤. 

![serial code execute](images/execute_on_CPU_GPU.png)

hostëŠ” ëŒ€ë¶€ë¶„ì˜ operationì—ì„œ deviceì™€ independentí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤. kernelì´ **launch**(êµ¬ë™)ì„ ì‹œì‘í•˜ë©´, hostëŠ” data parallel codeë¥¼ GPUì—ì„œ ì‘ë™í•˜ê²Œ ë§Œë“œëŠ” additional taskì—ì„œ ë²—ì–´ë‚˜ ì¦‰ì‹œ control ì‘ì—…ìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ë‹¤ì‹œ ë§í•´ kernelì€ **asynchronous**(ë¹„ë™ê¸°ì )ìœ¼ë¡œ launchëœë‹¤. hostëŠ” kernel launchê°€ ì™„ë£Œë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

> CUDA runtimeì—ì„œ ì œê³µí•˜ëŠ” cudaDeviceSynchronizeë¥¼ ì´ìš©í•´ì„œ CPUê°€ device codeì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.

---

## 2.5 managing memory

CUDA runtimeì€ device memoryë¥¼ allocateí•˜ëŠ” functionë“¤ì„ ì œê³µí•œë‹¤.

| í‘œì¤€ C function | CUDA C function |
| --- | --- |
| malloc | cudaMalloc |
| memcpy | cudaMemcpy |
| memset | cudaMemset |
| free | cudaFree |

ìš°ì„  GPU memory allocationì„ ìœ„í•œ cudaMallocì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤. ë‘ ê°€ì§€ parameterê°€ í•„ìš”í•˜ë‹¤.

```c
cudaError_t cudaMalloc ( void** devPtr, size_t size )
```

- cudaError_të¥¼ ì…ë ¥í•˜ë©´ error ë°œìƒ ì‹œ ì´ìœ ë¥¼ ì¶œë ¥í•´ ì¤€ë‹¤.

- allocateí•œ memoryëŠ” í•´ë‹¹ device memory addressë¥¼ ê°€ë¦¬í‚¤ëŠ” pointerì¸ devPtrì„ í†µí•´ returnëœë‹¤.

hostì™€ device ì‚¬ì´ì— dataë¥¼ transferí•˜ê¸° ìœ„í•œ functionìœ¼ë¡œëŠ” cudaMemcpyë¥¼ ì‚¬ìš©í•œë‹¤. unified memoryì— ì¡´ì¬í•˜ëŠ” dataê°€ ì•„ë‹ˆë¼ë©´, dataë¥¼ ì‚¬ì „ì— device memoryë¡œ transferí•´ì•¼ í•œë‹¤.

```c
cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )
```

- dst: destination memory address pointer

- src: source memory address pointer

- count: copyí•  byte size

- kind: data transfer type

  - cudaMemcpyHostToHost

  - cudaMemcpyHostToDevice

  - cudaMemcpyDeviceToHost

  - cudaMemcpyDeviceToDevice

<U>cudaMemcpyëŠ” synchronous behavior</U>ì´ë‹¤. host applicationì€ cudaMemcpyì˜ return/transferê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ë©ˆì¶˜ë‹¤.

ì°¸ê³ ë¡œ kernel launchë¥¼ ì œì™¸í•œ ëª¨ë“  CUDA callì€, enumerated type cudaError_tìœ¼ë¡œ error codeë¥¼ returní•œë‹¤. 

- ë§Œì•½ GPU memoryì— ì„±ê³µì ìœ¼ë¡œ allocateí–ˆë‹¤ë©´, 'cudaSuccess'ë¥¼ returní•œë‹¤.

- ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ cudaErrorMemoryAllocationì„ returní•œë‹¤.

ë‹¤ìŒ functionì„ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ error messageë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.(Cì˜ strerror functionê³¼ ë¹„ìŠ·í•˜ë‹¤.)

```c
char* cudaGetErrorString(cudaError_t error)
```

![GPU memory hierarchy](images/GPU_memory_hierarchy.png)

ì´ì œ ì˜ˆì œë¥¼ ë³´ë©° hostì™€ deviceê°„ì˜ data movement ê´€ë¦¬ë¥¼ ì‚´í´ë³´ì. 

![array summation](images/array_summation.png)

ìš°ì„  ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ array ì—°ì‚°(host-based array summation)ì„ ì˜¤ì§ Cë§Œ ì‚¬ìš©í•´ì„œ êµ¬í˜„í•œë‹¤. íŒŒì¼ëª…ì€ sumArraysOnHost.cì´ë‹¤.

> [int main(int argc, char **argv)ë€?](https://iamaman.tistory.com/364)

```c
#include <stdlib.h>
#include <string.h>
#include <time.h>

// Hostì—ì„œ array sum ìˆ˜í–‰
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    for (int idx=0; idx<N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

// arrayì— random numberë¡œ ì´ˆê¸°ê°’ì„ ì„¤ì •
void initialData(float *ip, int size) {
    // random number ìƒì„±
    time_t t;
    srand((unsigned int) time(&t));

    for (int i=0; i<size; i++) {
        ip[i] = (float) ( rand() & 0xFF )/10.0f;
    }
}

int main(int argc, char **argv) {
    int nElem = 1024;
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *h_C;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    h_C = (float *)malloc(nBytes);

    initialData(h_A, nElem);
    initialData(h_B, nElem);

    sumArraysOnHost(h_A, h_B, h_C, nElem);

    free(h_A);
    free(h_B);
    free(h_C);

    return(0);
}
```

pure C programì´ë¯€ë¡œ C compilerë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ nvcc compilerë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
$ nvcc -Xcompiler -std=c99 sumArraysOnHost.c -o sum
$ ./sum
```

ì°¸ê³ ë¡œ ìœ„ compile ëª…ë ¹ì˜ **flag**(ì˜µì…˜)ì€ ë‹¤ìŒ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.(ìì„¸íˆëŠ” CUDA compiler documentë¥¼ ì‚´í”¼ë©´ ì•Œ ìˆ˜ ìˆë‹¤.)

- -Xcompiler: ë°”ë¡œ C compiler ë˜ëŠ” preprocessorë¡œ ì²˜ë¦¬í•œë‹¤.

- -std=c99: code styleì´ c99 standardì„ì„ ì•Œë¦°ë‹¤.

ì´ ì—°ì‚°ì„ GPUì—ì„œ êµ¬ë™í•˜ê²Œë” ë°”ê¾¸ëŠ” ê²ƒì€ ì‰½ë‹¤. 

1. GPU memory allocation

```c
float *d_A, *d_B, *d_C;
cudaMalloc((float**) &d_A, nBytes);
cudaMalloc((float**) &d_B, nBytes);
cudaMalloc((float**) &d_B, nBytes);
```

2. host memoryì—ì„œ GPU global memoryë¡œ dataë¥¼ transferí•œë‹¤.

```c
cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);
```

3. kernel ìˆ˜í–‰.

GPU global memoryë¡œ transferí•˜ëŠ” ê³¼ì •ì´ ëë‚˜ë©´, ì´ì œë¶€í„°ëŠ” host sideì—ì„œ GPUì—ì„œ array summationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ kernel functionì„ invokeë  ìˆ˜ ìˆë‹¤. ë§Œì•½ kernelì´ callë˜ë©´ controlì€ ì¦‰ì‹œ hostë¡œ return backë˜ë©°, GPUê°€ kernelì„ ìˆ˜í–‰í•˜ëŠ” ì‚¬ì´ì— ë‹¤ë¥¸ functionì„ ìˆ˜í–‰í•œë‹¤.(asynchronous ë™ì‘)

4. resultë¥¼ host memoryë¡œ copyí•œë‹¤.

kernel ì‘ì—…ì´ ëª¨ë‘ ëë‚˜ë©´, result(array d_C)ëŠ” GPU global memoryì— ì €ì¥ëœë‹¤. ì´ì œ ì´ resultë¥¼ host array(gpuRef)ë¡œ copyí•´ì•¼ í•œë‹¤.

```c
cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);
```

> ë§Œì•½ ì´ë ‡ê²Œ copyë¥¼ ì§„í–‰í•˜ì§€ ì•Šê³  'gpuRef = d_C'ì™€ ê°™ì€ ì˜ëª»ëœ assignmentë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ê²Œ ë˜ë©´ applicationì€ runtimeì— crashëœë‹¤.

> ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ unified memoryê°€ CUDA 6ë¶€í„° ì œê³µëë‹¤. CPUì™€ GPU memory ëª¨ë‘ single pointerë¥¼ ì‚¬ìš©í•œë‹¤.

cudaMemcpyì— ì˜í•´ hostëŠ” copy ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ë©ˆì¶”ê²Œ ëœë‹¤. 

5. memoryë¥¼ releaseí•œë‹¤.

```c
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

---

## 2.6 organizing threads

> [thread block architecture](https://tododiary.tistory.com/57)

host sideì—ì„œ kernel functionì´ launchë˜ë©´, deviceë¡œ executionì´ ë„˜ì–´ê°€ë©° kernel functionì—ì„œ ì •ì˜í•œ threadë“¤ì—ì„œ ëª…ë ¹ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ì—ˆë‹¤. ì´ë•Œ 'threadë“¤ì„ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠ”ê°€'ë¼ëŠ” ë¬¸ì œê°€ CUDA programmingì— ìˆì–´ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì´ë‹¤.

threadëŠ” 2-level hierarcyë¡œ êµ¬ì„±ëœë‹¤. í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ blockìœ¼ë¡œ êµ¬ì„±ë˜ê³ , ê° blockì€ í•˜ë‚˜ ì´ìƒì˜ threadë¡œ êµ¬ì„±ëœë‹¤.

![thread hierarchy](images/thread_hierarchy.png)

- í•˜ë‚˜ì˜ kernel launchë¡œ ìƒì„±ëœ ëª¨ë“  threadë¥¼ **grid**ë¼ê³  í†µì¹­í•œë‹¤.

  - grid ë‚´ ëª¨ë“  threadëŠ” ê°™ì€ global memory spaceë¥¼ shareí•œë‹¤.

  - ì´ threadë“¤ì´ ëª¨ë‘ ë™ì¼í•œ kernel codeë¥¼ ì‹¤í–‰í•œë‹¤.

- í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ thread **block**ë“¤ë¡œ êµ¬ì„±ëœë‹¤. ê° thread blockëŠ” ë‹¤ìŒ íŠ¹ì§•ì„ ê°€ì§€ê³  cooperateí•  ìˆ˜ ìˆë‹¤.

  - block-local synchronization

  - block-local shared memory

  > <U>ë‹¤ë¥¸ blockì˜ threadë¼ë¦¬ëŠ” cooperateí•  ìˆ˜ ì—†ë‹¤.</U>

> blockì€ threadë¥¼ ìµœëŒ€ 512ê°œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ blockì´ ê°€ì§€ëŠ” thread ê°œìˆ˜ëŠ” 32(ë˜ëŠ” NVIDIAëŠ” 64ë¥¼ ê¶Œì¥) ë°°ìˆ˜ë¡œ ì§€ì •í•˜ëŠ” í¸ì´ ì¢‹ë‹¤.(SMì´ 32ë°°ìˆ˜ ë‹¨ìœ„ë¡œ ë™ì‘)

threadë“¤ì€ ë‹¤ìŒ indexë¥¼ í†µí•´ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° index(coordinate variables)ëŠ” CUDA runtimeì— ì˜í•´ ê° threadë³„ë¡œ í• ë‹¹ëœë‹¤.

![blockIdx, threadIdx](images/blockIdx_threadIdx.png)

- blockIdx: grid ë‚´ë¶€ì—ì„œì˜ block index

- threadIdx: block ë‚´ë¶€ì—ì„œì˜ thread index

> coordinate variableì€ uint3 typeì„ ê°–ëŠ”ë‹¤. ì´ structureì€ 3ê°œì˜ unsigned integerë¡œ êµ¬ì„±ë˜ë©°, ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ componentëŠ” x, y, zë¥¼ ë¶™ì—¬ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```c
blockIdx.x
blockIdx.y
blockIdx.z

// block ë‚´ ëª¨ë“  threadëŠ” ë™ì¼í•œ blockIdxë¥¼ ê³µìœ í•œë‹¤.
threadIdx.x
threadIdx.y
threadIdx.z
```

kernel launch êµ¬ë¬¸ì—ì„œ execution configutation parameters(<<<...>>>)ë¡œ gridì™€ ê° blockì˜ dimensionì„ ì§€ì •í–ˆë‹¤. gridëŠ” ì£¼ë¡œ blockì˜ 2D array, blockì€ ì£¼ë¡œ threadì˜ 3D arrayë¡œ êµ¬ì„±ëœë‹¤. ì´ëŸ° gridì™€ blockì˜ dimentionì€ ë‹¤ìŒ built-in variableë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- blockDim

- gridDim

> ì´ variableë“¤ì€ dim3 typeì´ë©°, uint3ì— ê¸°ë°˜í•œ dimensionì— íŠ¹í™”ëœ integer vector typeì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ blockDim.x, blockDim.y, blockDim.zë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

> ì‚¬ìš©í•˜ì§€ ì•Šì€ ì°¨ì›ì˜ í¬ê¸°ëŠ” 1ë¡œ ì§€ì •í•œë‹¤.(ê°’ì„ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ defaultë¡œ 1ì´ ë˜ì–´ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ëœë‹¤.)

![ì „ì—­ ì¸ë±ìŠ¤](images/CUDA_grid_execution.png)

ì´ëŸ° variableì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ gridì—ì„œ ìœ ì¼í•œ global indexë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ë‹¤ìŒì´ í•´ë‹¹ëœë‹¤. 

- i = blockIdx.x * blockDim.x + threadIdx.x

> blockì´ 1Dì´ë©°, threadê°€ 256ê°œë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ block 0ì˜ threadì—ì„œ iì˜ ë²”ìœ„ëŠ” 0\~255 / block 1ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 256\~511 / block 2ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 512\~767... ì‹ìœ¼ë¡œ ë°°ì •ëœë‹¤.

> ì´ëŸ° ë°©ë²•ìœ¼ë¡œ index ië¥¼ ì´ìš©í•˜ì—¬ vector A, B, C ê°’ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

ì´ëŸ° ë°©ì‹ ë•ë¶„ì— kernel functionì€ loopê°€ ì—†ë‹¤. ëŒ€ì‹  loopë¥¼ gridë¡œ ëŒ€ì²´í•˜ê³ , ê° threadê°€ iteration í•˜ë‚˜ì— ëŒ€ì‘ë˜ë©° ì—°ì‚°í•œë‹¤. ì´ëŸ° ì¢…ë¥˜ì˜ data parallelismì„ **loop parallelism**ì´ë¼ê³  ì§€ì¹­í•œë‹¤.

ì£¼ì˜í•  ì ì€ element ê°œìˆ˜(vector size)ê°€ block sizeì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ elementê°€ ì´ 100ê°œë¼ë©´, íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ê°€ì¥ ì‘ì€ thread ì°¨ì›ì€ 32ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ blockì€ ì´ 4ê°œê°€ ìƒê¸°ê³ , ì´ 128ê°œì˜ threadë¥¼ ê°€ì§€ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ë ‡ê²Œ êµ¬ì„±í•  ê²½ìš° 28ê°œì˜ threadëŠ” ë¹„í™œì„±í™”í•´ì•¼(ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ì•¼) í•œë‹¤. 

> block ê°œìˆ˜ë¥¼ êµ¬í•˜ëŠ” ì‹ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ vector sizeê°€ 1000ì´ê³  ê° blockì´ 256ê°œì˜ threadë¥¼ ê°€ì§„ë‹¤ë©´, (1000 + 256 - 1) / 256 = 4ë¡œ 4ê°œì˜ blockì„ ìƒì„±í•˜ê²Œ ëœë‹¤. ì´ ê²½ìš° ê²°ê³¼ì ìœ¼ë¡œëŠ” 256*4 = 1024 threadê°€ ì‹¤í–‰ë˜ê³ , ë‚˜ë¨¸ì§€ 24ê°œëŠ” ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•œë‹¤.

> ë¹„ìŠ·í•˜ê²Œ gridëŠ” (nElem + block.x - 1)/block.x) ê°œê°€ ëœë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: gridì™€ block dimension êµ¬í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

hostì™€ device ì–‘ìª½ì—ì„œ gridì™€ blockì˜ dimensionì„ ì²´í¬í•  ê²ƒì´ë‹¤.

deviceì—ì„œëŠ” kernel functionì„ ë§Œë“¤ì–´ì„œ, ê°ìì˜ thread index, block index, grid dimensionì„ ì¶œë ¥í•œë‹¤. íŒŒì¼ëª…ì€ checkDimension.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void checkIndex(void) {
    printf("threadIdx: (%d, %d, %d) blockIdx: (%d, %d, %d) blockDim: (%d, %d, %d) "
        "gridDim: (%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z,
        blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z,
        gridDim.x, gridDim.y, gridDim.z);
}

int main(int argc, char **argv) {
    // number of elements
    int nElem = 6;

    // grid, block structure ì •ì˜
    // ì§€ì •í•˜ì§€ ì•Šì€ ì°¨ì›ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒ(1)ìœ¼ë¡œ ì²˜ë¦¬

    // 3ê°œì˜ threadë¥¼ í¬í•¨í•˜ëŠ” 1D block
    dim3 block (3);
    // grid ê°œìˆ˜
    dim3 grid  ((nElem + block.x - 1)/block.x);

    // check grid and block dimension from host side
    printf("grid.x %d grid.y %d grid.z %d\n", grid.x, grid.y, grid.z);
    printf("block.x %d block.y %d block.z %d\n", block.x, block.y, block.z);

    // check grid and block dimension from device side
    checkIndex <<<grid, block>>> ();

    // reset device
    cudaDeviceReset();

    return(0);
}
```

CUDAì—ì„œ printf functionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” compileë˜ëŠ” í™˜ê²½ì˜ GPU architectureë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤.

```bash
nvcc -arch=sm_80 checkDimension.cu -o check
./check
```

> ì±…ì€ Fermi GPUì´ë¯€ë¡œ -arch=sm_20ì„ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤.([CUDA -arch í™•ì¸](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/))(FermiëŠ” CUDA ìƒìœ„ ë²„ì „ì—ì„œ deprecatedëë‹¤.) 

> í˜„ì¬ ì‹¤ìŠµ ì¤‘ì¸ í™˜ê²½ì€ RTX 3080 Tië¡œ Ampere architectureì´ë‹¤.(sm_86)($ nvidia-smi -q ëª…ë ¹ìœ¼ë¡œ í™•ì¸)([GPU í™•ì¸](https://kyumdoctor.tistory.com/72))

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![grid block indices and dimensions](images/grid_block_indice.png)

ì´ ì˜ˆì œì—ì„œ ëª…ì‹¬í•  ì ì€ gridì™€ block variableì˜ ì ‘ê·¼ì—ì„œ, hostì™€ deviceê°€ êµ¬ë¶„ëœë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤.

- host: hostì—ì„œ 'block'ìœ¼ë¡œ declarationí•œ ë’¤, block.x, block.y, block.zì„ ì‚¬ìš©.

- device: built-in block size variableì¸ blockDim.x, blockDim.y, blockDim.zë¥¼ ì‚¬ìš©.

ì´ëŠ” kernel launchë³´ë‹¤ ì•ì„œ hostì—ì„œ gridì™€ block variableì„ ì •ì˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì •ë¦¬í•˜ìë©´ data sizeê°€ ì£¼ì–´ì¡Œì„ ë•Œ, gridì™€ block dimensionì€ ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ ì •í•œë‹¤.

1. block sizeë¥¼ ê²°ì •

2. grid dimensionì„ data sizeì™€ block sizeë¥¼ ì´ìš©í•œ ê³„ì‚°ì„ í†µí•´ ê²°ì •

> block dimensionì„ ì •í•  ë•ŒëŠ” kernelì˜ performanceì ì¸ íŠ¹ì§•ê³¼, GPU resourceì˜ limitationì„ ì•Œì•„ì•¼ í•œë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: hostì—ì„œ grid, block dimension ì •ì˜í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

ì•„ë˜ ì˜ˆì œë¥¼ í†µí•´ 1D gridì™€ 1D blockì„ hostì—ì„œ ì •ì˜í•  ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ defineGridBlock.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

int main(int argc, char **argv) {
    // define total data elements
    int nElem = 1024;

    // define grid and block structure
    dim3 block (1024);
    dim3 grid  ((nElem + block.x - 1)/block.x);
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 512;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 256;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);
    
    // reset block
    block.x = 128;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset device
    cudaDeviceReset();
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œë‹¤.

```bash
nvcc defineGridBlock.cu -o block
./block 
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![defineGridBlock](images/defineGridBlock.png)

---

## 2.7 launching a CUDA kernel

ì•ì„œ CUDA kernel callì€ ë‹¤ìŒê³¼ ê°™ì´ ëª…ë ¹í–ˆë‹¤.

```c
kernel_name <<<grid, block>>>(argument list);
```

ë‹¤ìŒ ë‘ ì˜ˆì‹œë¥¼ ë³´ì.

- kernel_name<<<1, 32>>>(argument list): 32ê°œì˜ elementë¥¼ í•œ blockì— ë‹´ëŠ”ë‹¤.

- kernel_name<<<32, 1>>>(argument list): 32ê°œì˜ blockì€ ëª¨ë‘ element í•˜ë‚˜ë§Œì„ ê°–ëŠ”ë‹¤.

ë˜í•œ kernelì€ type qualifierë¡œ \_\_global\_\_ì„ ë¶™ì—¬ì„œ declarationí–ˆë‹¤.

```c
__global__ void kernel_name(argument list);
```

ì´ë•Œ <U>kernel functionì€ ê¼­ void return type</U>ì´ì–´ì•¼ í•œë‹¤. 

ì•„ë˜ëŠ” CUDA C programmingì—ì„œì˜ type qualifierë¥¼ ì •ë¦¬í•œ í‘œì´ë‹¤.

| qualifer | execution | callable | ì„¤ëª… |
| :---: | :---: | :---: | :---: |
| \_\_global\_\_ | device | hostì—ì„œ callable<br/>NVIDIAê°€ ì œì‹œí•˜ëŠ” compute capabilityê°€ 3 ì´ìƒì¸ device | void return typeì´ì–´ì•¼ í•œë‹¤. |
| \_\_device\_\_ | device | device only | |
| \_\_host\_\_ | host | host only | ìƒëµí•´ë„ ë¬´ë°©í•˜ë‹¤. |

> ì°¸ê³ ë¡œ functionì´ hostì™€ device ì–‘ìª½ì—ì„œ compileëœë‹¤ë©´, \_\_device\_\_ì™€ \_\_host\_\_ qualifierë¥¼ ê°™ì´ ì¨ë„ ëœë‹¤. 

> ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.<br/> \_\_host\_\_\_\_device\_\_const char* cudaGetErrorString(cudaError_t error)

ë˜í•œ CUDA kernelì˜ ì œì•½ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- device memoryë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤.

- void return typeë§Œ ê°€ëŠ¥í•˜ë‹¤.

- ìœ ë™ì ì¸ ìˆ«ìì˜ variableì„ argumentë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

- static variableì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- function pointerë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- asynchronousí•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.(ë”°ë¼ì„œ errorë¥¼ ì•Œê¸° í˜ë“¤ë‹¤.)

ë¬´ì—‡ë³´ë‹¤ë„ ë‘ ë²¡í„°ë¥¼ ë”í•˜ëŠ” ì—°ì‚°(A+B = C) functionì„ Cì™€ CUDAë¡œ êµ¬í˜„í–ˆì„ ë•Œì˜ ì°¨ì´ë¥¼ ë³´ë©´ ì œì¼ ê·¹ëª…í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.

ì•„ë˜ëŠ” C codeì´ë‹¤.

```c
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
  for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
  }
}
```

ë‹¤ìŒì€ CUDA codeì´ë‹¤. loopê°€ ì‚¬ë¼ì§€ê³  built-in thread coordinate variableì´ array indexë¥¼ ëŒ€ì‹ í–ˆë‹¤. ë˜í•œ Nê°œì˜ threadë¥¼ launchí•˜ë©´ì„œ, Nì„ referenceí•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤.

```c
__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}
```

---