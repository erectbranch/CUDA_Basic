# 2 CUDA Programming Model (Part I)

> [Managing Devices](https://github.com/erectbranch/CUDA_Basic/blob/master/ch02/management.md)

ì´ë²ˆ ì¥ì€ vector addition, matrix addition ì˜ˆì œë¥¼ CUDA programìœ¼ë¡œ ì‘ì„±í•˜ë©° ì‚´í´ë³¼ ê²ƒì´ë‹¤. 

---

## 2.1 Graphics card

> [A complete anatomy of a graphics card: Case study of the NVIDIA A100](https://blog.paperspace.com/a-complete-anatomy-of-a-graphics-card-case-study-of-the-nvidia-a100/)

ë“¤ì–´ê°€ê¸° ì•ì„œ ì ì‹œ GPUì˜ PCB ê¸°íŒ, ì¦‰ Graphics cardê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. í”íˆ GPUë¥¼ Graphics card ìì²´ë¡œ ì°©ê°í•˜ëŠ” ê²½ìš°ê°€ ë§ì§€ë§Œ, GPUëŠ” Graphics cardì˜ ì¼ë¶€ë¶„ì´ë‹¤.

![A100 PCB](images/A100_PCB.png)

- GPU chip ë‚´ë¶€ì—, CUDA coreì™€ SRAM ë“±ì´ ì¡´ì¬í•œë‹¤.

- ì™¸ë¶€ëŠ” GPUì—ì„œì˜ DRAMì— í•´ë‹¹ë˜ëŠ” Video RAM(VRAM)ìœ¼ë¡œ GDDR SGRAM memoryë¥¼ ê°–ëŠ”ë‹¤.

  > [DDRê³¼ GDDRì˜ ì°¨ì´](https://news.skhynix.co.kr/post/ddr-gddr)

ì—¬ê¸°ì„œ GDDR SGRAMì€ **Graphics Double Data Rate** SGRAM(Synchronous Graphics DRAM)ì˜ ì•½ìë‹¤. 3D graphic ì²˜ë¦¬ë¥¼ ë³´ë‹¤ ì›í™œí•˜ê²Œ í•  ìˆ˜ ìˆëŠ” (pixelì˜ ê¹Šì´ ì •ë³´ë¥¼ ë‹´ëŠ”) Z bufferë¥¼ ì¥ì°©í•˜ëŠ” ë“±, GPUëŠ” graphic ì²˜ë¦¬ì— íŠ¹í™”ëœ RAM(SGRAM)ì„ ê°–ê²Œ ë˜ì—ˆë‹¤.

- GDDRì€ dataë¥¼ ì½ê³  ì“¸ ìˆ˜ ìˆëŠ” í†µë¡œì¸ **strobe**(ìŠ¤íŠ¸ë¡œë¸Œ)ë¥¼ ì¼ë°˜ DRAMë³´ë‹¤ í›¨ì”¬ ë§ì´ ê°–ê³  ìˆë‹¤.

  > DDR(Double Data Rate)ë€, ê¸°ì¡´ SDR(Single Data Rate)ê°€ clock rising edgeì—ì„œë§Œ dataë¥¼ ì „ì†¡í•˜ëŠ” ë°©ì‹ì—ì„œ, clock rising, falling edgeë¥¼ ì´ìš©í•´ 2ë°°ì˜ dataë¥¼ ì „ì†¡í•  ìˆ˜ ìˆê²Œ ëœ ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤. ì„¸ëŒ€ê°€ ì§€ë‚ ìˆ˜ë¡ bus clock rateëŠ” í–¥ìƒì‹œí‚¤ê³ , ì†Œëª¨ voltageëŠ” ë‚®ì¶”ê³  ìˆë‹¤.

> ìµœì‹  ê¸°ìˆ ë¡œ ë” ë‚®ì€ latencyì™€ ë†’ì€ bandwidthë¥¼ ì–»ê¸° ìœ„í•´, GDDR ëŒ€ì‹  **HBM**(High Bandwidth Memory)ë¥¼ ì¥ì°©í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. í•˜ì§€ë§Œ ë¹„ìš©ì´ë‚˜ êµ¬í˜„ ë‚œì´ë„, ìš©ëŸ‰ì˜ í™•ì¥ ë¬¸ì œ ë“±ì„ ì•ˆê³  ìˆë‹¤.

---

### 2.1.1 spectrum of GPU

**GPU**(Graphics Processing Unit)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ graphic renderingì— í•„ìš”í•œ ë³µì¡í•œ mathematical, geometric calculationì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ processorì´ë‹¤.

- CPUì™€ ë¹„êµí–ˆì„ ë•Œ ë” ë§ì€ transistorì™€ ALUë¥¼ ê°€ì§„ë‹¤.

ì´ëŸ¬í•œ ì„¤ê³„ ëª©ì  ë•Œë¬¸ì— ì´ˆê¸° GPUëŠ” ì˜¤ë¡œì§€ graphics pipelineì˜ íŠ¹ì • ë¶€ë¶„ì„ accelerateí•˜ê¸° ìœ„í•œ deviceë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ GPUê°€ ë°œì „í•˜ë©´ì„œ ì¼ë°˜ì ìœ¼ë¡œ CPUê°€ ì²˜ë¦¬í–ˆë˜ ê³„ì‚°ì„ ì²˜ë¦¬í•˜ê¸° ì‹œì‘í•˜ê³ , general purpose computingì—ë„ ì‚¬ìš©ë˜ê¸° ì‹œì‘í–ˆë‹¤.

> ì´ˆê¸°ì—ëŠ” **GPGPU**(General Purpose GPU)ë¼ê³  êµ¬ë¶„í•´ì„œ ë¶€ë¥´ê¸°ë„ í–ˆë‹¤.

ë”°ë¼ì„œ í˜„ì¬ì˜ GPUëŠ”, 'íŠ¹ì • ëª©ì ì— **specialized**ëœ' **accelerator**(ê°€ì†ê¸°)ê°€ ì•„ë‹ˆë¼, general purpose computingì— ì‚¬ìš©ë˜ëŠ” (data parallelismì— íŠ¹í™”ëœ) **processor**ë¡œ ë³´ëŠ” í¸ì´ ë” ì •í™•í•˜ë‹¤.

![flexibillity, performance/power efficiency](images/devices_flexibillity_performance.png)

- processor: **flexibillity**ê°€ ë†’ë‹¤.

  - CPU, GPU

- accerelator: **performance**ì™€ **power efficiency**ê°€ ë†’ë‹¤.

  - FPGA(Field Programmable Gate Array), ASIC(Application Specific Integrated Circuit)

> í˜„ ì‹œì ì—ì„œëŠ” one-size-fits-all(ëª¨ë“  ì¼ì— ë§ŒëŠ¥ì¸) processorëŠ” ë‹¹ì¥ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²ƒìœ¼ë¡œ ë³´ë©°, ëŒ€í˜• chipì— ì„œë¡œ ë‹¤ë¥¸ deviceë¥¼ ê²°í•©í•˜ì—¬ íš¨ìœ¨ì„ ë†’ì´ëŠ” ë°©ì‹ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤.

---

## 2.2 GPU architecture overview

GPUëŠ” unitì„ ë„¤ ê°€ì§€ ë¶„ë¥˜ë¡œ ë‚˜ëˆˆë‹¤.

- **Streaming Multiprocessors**(SMs)

- **Load/Store**(LD/ST) units

  dataì˜ asynchronous copyê°€ ê°€ëŠ¥í•˜ë‹¤. ì¶”ê°€ thread resourceì˜ ì‚¬ìš© ì—†ì´, threadë“¤ì´ globalí•˜ê²Œ shareí•  ìˆ˜ ìˆë„ë¡ dataë¥¼ loadí•œë‹¤.

- **Special Function Units**(SFU)

  vectored dataì—ì„œ sine, cosine, reciprocal, square root ë“±ì˜ functionì„ ê³„ì‚°í•œë‹¤.

- **Texture Mapping Units**(TMU)

  image rotation, resizing, adding distortion, noise, moving 3D plane objects ë“±ì˜ taskë¥¼ ì²˜ë¦¬í•œë‹¤.

---

### 2.2.1 Streaming Multiprocessors

**SM**(Streaming Multiprocessor)ì€ ë‹¤ìŒ ìš”ì†Œë¡œ êµ¬ì„±ëœ execution entityì´ë‹¤.

- register spaceë¥¼ ê³µìœ í•˜ëŠ” core ë¬¶ìŒ.

  > NVIDIAì—ì„œëŠ” CUDA coresì™€ Tensor cores, AMDì—ì„œëŠ” Stream processorsë¼ê³  ë¶€ë¥¸ë‹¤.

  > Tensor coreëŠ” ML applicationì— íŠ¹í™”ëœ coreë¡œ, MLì—ì„œëŠ” í›¨ì”¬ ë¹ ë¥¸ ì—°ì‚° ì†ë„ë¥¼ ë³´ì´ì§€ë§Œ í‰ë²”í•œ ì—°ì‚°ì€ ì œëŒ€ë¡œ ìˆ˜í–‰í•˜ì§€ ëª»í•œë‹¤.(CUDA coreê°€ clock cycleë‹¹ í•˜ë‚˜ì˜ operationë§Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê²ƒì— ë¹„í•´, Tensor coreëŠ” cycleë‹¹ ì—¬ëŸ¬ ê°œì˜ operationì„ ìˆ˜í–‰í•œë‹¤.)

- shared memoryì™€ L1 cache

ì•„ë˜ëŠ” A100ì˜ cache hierarchyë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. SMì´ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆê³ , VRAMê³¼ ì–´ë–»ê²Œ data transferê°€ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì‚´í´ë³´ì.

![A100 cache hierarchy](images/A100_cache_hierarchy.png)

- ê° SMì€ shared memoryì™€ L1 cacheë¥¼ ê°–ëŠ”ë‹¤.

- L2 cacheëŠ” ëª¨ë“  SMì— unified, sharedë˜ì–´ ìˆë‹¤. HBM2(VRAM)ì˜ ê°€êµ ì—­í• ì„ í•œë‹¤.

---

### 2.2.2 benifits of using GPUs, latency hiding

CPUì˜ í•œ coreê°€ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ threadë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë“¯ì´, GPU ì—­ì‹œ SM ë‚´ë¶€ì˜ í•œ coreê°€ í•œ ë²ˆì— ì—¬ëŸ¬ ê°œì˜ threadë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì„¤ê³„ ì² í•™ì— ë”°ë¼ ì„¸ë¶€ì ì¸ ì°¨ì´ëŠ” í¬ë‹¤.

- CPU: ì†Œìˆ˜ì˜ coreë¡œ ìˆ˜ì‹­ ê°œì˜ threadë¥¼ parallelí•˜ê²Œ ìˆ˜í–‰.

- GPU: ìˆ˜ë°± ê°œì—ì„œ ìˆ˜ì²œ ê°œì— ë‹¬í•˜ëŠ” CUDA coreë¡œ, ìˆ˜ì²œ ê°œì˜ threadë¥¼ parallelí•˜ê²Œ ìˆ˜í–‰.

CPUì™€ GPUì˜ ì°¨ì´ë¥¼ cache hierarchyë¥¼ ë¹„êµí•˜ë©° ì‚´í´ë³´ì.

![CPU vs GPU](images/cpu_vs_gpu.png)

- CPU: chip ë‚´ë¶€ì˜ ì ˆë°˜ì„ cacheê°€ ì°¨ì§€í•˜ê³  ìˆë‹¤. 

  - **latency stall**ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì¤‘ì ìœ¼ë¡œ ì„¤ê³„í•˜ì˜€ë‹¤.

  - ë”°ë¼ì„œ ì¬ì‚¬ìš©ì„±ì´ ë†’ì€ dataë¥¼ cacheì— ì €ì¥í•´ë‘ê³ , cache hitê°€ ë°œìƒí•˜ë©´ ë¹ ë¥´ê²Œ dataë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. 

  - branch prediction, prefetching ë“± pipeline hazardë¥¼ ë§‰ê¸° ìœ„í•œ ì—¬ëŸ¬ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤.

- GPU: cacheê°€ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì´ ì ì€ ëŒ€ì‹ , ëŒ€ë¶€ë¶„ì„ core(ALU)ê°€ ì°¨ì§€í•˜ê³  ìˆë‹¤.

GPUëŠ” cacheê°€ ì ì–´ì„œ VRAMì— ì ‘ê·¼í•  ë•Œë§ˆë‹¤ penaltyë¥¼ ê²ªê²Œ ëœë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ê²ªì„ ë•Œ ë‹¤ë¥¸ warpê°€ ëŒ€ì‹  taskë¥¼ ì´ì–´ì„œ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ **latency hiding**ì„ í•  ìˆ˜ ìˆë‹¤.

GPUê°€ instructionì„ ì‹¤í–‰í•´ì•¼ í•œë‹¤ê³  í•˜ì. ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ idle(í˜¹ì€ wasted) timeì„ ë°œìƒì‹œí‚¤ì§€ ì•ŠëŠ” ê²ƒì´ë‹¤.

- ê¸°ì¡´ì˜ long-latency operation(memory access ë“±)ìœ¼ë¡œ ê¸¸ê²Œ ê¸°ë‹¤ë ¤ì•¼ í•˜ëŠ” warpëŠ” ì„ íƒí•˜ì§€ ì•ŠëŠ”ë‹¤.

- ì‹¤í–‰ ì¤€ë¹„ê°€ ëœ warpë¥¼ ì„ íƒí•´ì„œ ì‹¤í–‰í•œë‹¤.

  > (readyì¸ warpê°€ ì—¬ëŸ¬ ê°œ ìˆë‹¤ë©´, priority mechanismì— ë”°ë¼ ì„ íƒí•œë‹¤.) 

> [warp ì •ë¦¬](https://alpaka.readthedocs.io/en/0.5.0/usage/abstraction/warp.html): **warp**ë€ <U>ë™ì¼í•œ instructionì„ (parallelí•˜ê²Œ) ìˆ˜í–‰í•˜ëŠ” thread ë¬¶ìŒ</U>ì„ ì˜ë¯¸í•œë‹¤.(ì´ thread 32ê°œë¡œ êµ¬ì„±ëœ single execution unit)

> ë‹¤ì‹œ ë§í•´ warp ë‚´ ì–´ë–¤ í•œ threadê°€ ì–´ë–¤ instructionì„ ì‹¤í–‰í•˜ë©´, ë‚˜ë¨¸ì§€ ë‚´ë¶€ì˜ threadë“¤ë„ ë™ì¼í•œ instructionì„ ì‹¤í–‰í•´ì•¼ í•œë‹¤. **SIMT**(Single Instruction Multiple Thread)ë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ .

```
// ë§Œì•½ 128ê°œì˜ ê°€ëŠ¥í•œ threadê°€ ìˆë‹¤ë©´, 4ê°œì˜ warpë¡œ partitionëœë‹¤.
Warp 0: thread  0, thread  1, thread  2, ... thread 31
Warp 1: thread 32, thread 33, thread 34, ... thread 63
Warp 2: thread 64, thread 65, thread 66, ... thread 95
Warp 3: thread 96, thread 97, thread 98, ... thread 127
```

> processê°€ ì€í–‰ì´ë¼ë©´, warpëŠ” ì€í–‰ ì°½êµ¬ì˜ ì€í–‰ì›ì— í•´ë‹¹ëœë‹¤.

![latency hiding](images/latency_hiding.png)

> ë‹¨, warpê°€ ë¶€ì¡±í•˜ë©´ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ latency hidingì— ì‹¤íŒ¨í•  ìˆ˜ë„ ìˆë‹¤.

---

## 2.3 CUDA programming model

programming modelì´ë€, hardwareìƒì—ì„œ ë™ì‘í•˜ëŠ” applicationì„ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ computer architectureì„ abstractioní•œ ê²ƒì„ ì˜ë¯¸í•œë‹¤.

> programming languageë‚˜ programming environment í˜•íƒœë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ programê³¼ programming model êµ¬í˜„ì— ìˆì–´ì„œì˜ abstractionì„ ê³„ì¸µ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

> CUDA programming modelì„ ë³´ë©´ GPU architectureê°€ ê°–ëŠ” memory hierarchyì˜ abstractionì„ ì•Œ ìˆ˜ ìˆë‹¤.

![abstraction layer](images/programming_model_layer.png)

í”„ë¡œê·¸ë˜ë¨¸ ê´€ì ì—ì„œëŠ” parallel computationì„ ë‹¤ìŒ ì„¸ ê°€ì§€ levelì—ì„œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

- domain level

    programê³¼ algorithm. ì–´ë–»ê²Œ dataì™€ functionì„ **decompose**(ë¶„í•´)í•´ì•¼ íš¨ìœ¨ì ìœ¼ë¡œ parallelí•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•œë‹¤.

- logic level

    programê³¼ algorithm ë””ìì¸ì´ ëë‚˜ë©´ programming ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤. ì–´ë–»ê²Œ programmingí•´ì•¼ logicì„ concurrent threadë“¤ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•œë‹¤. 

- hardware level

    hardware ìì²´ì ìœ¼ë¡œ threadë¥¼ íš¨ìœ¨ ì¢‹ê²Œ coreë¡œ mappingí•˜ëŠ” ë°©ë²• ë“±ì„ ê³ ë ¤í•œë‹¤.

---

## 2.4 CUDA Programming Structure

> [CUDA ê¸°ì´ˆ ì •ë¦¬](https://velog.io/@lunarainproject/CUDA-%EA%B8%B0%EC%B4%88)

- ì±…(2014 ë°œê°„)ì—ì„œëŠ” CUDA 6ìœ¼ë¡œ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

- host(CPU) memory: variable ì´ë¦„ ì•ì— `h_`ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

- device(GPU) memory: variable ì´ë¦„ ì•ì— `d_`ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œ CPUì™€ GPU ì‚¬ì´ì—ì„œ ê³µìœ &ê´€ë¦¬ë˜ëŠ” memory poolì¸ **unified memory**ë¥¼ ë¨¼ì € ì‚´í´ë³´ì. 

- CPU, GPU memory ëª¨ë‘ ë‹¨ì¼ pointerë¡œ dataì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

  > unified memoryì— allocateëœ dataëŠ” hostì™€ device ì‚¬ì´ì—ì„œ ìë™ìœ¼ë¡œ **migrate**í•œë‹¤.

CUDAì˜ í•µì‹¬ì€ kernelì´ë‹¤. CUDAë¥¼ ì´ìš©í•˜ë©´ GPU threadë¥¼ í†µí•´ ì‹¤í–‰ë˜ëŠ” kernelì„ schedulingí•  ìˆ˜ ìˆë‹¤. 

![serial code execute](images/execute_on_CPU_GPU.png)

- hostëŠ” ëŒ€ë¶€ë¶„ì˜ operationì—ì„œ deviceì™€ independentí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤.

- kernelì´ **launch**(êµ¬ë™)ë˜ë©´, hostëŠ” additional taskì—ì„œ ë²—ì–´ë‚˜ ì¦‰ì‹œ ë³¸ë˜ì˜ ì‘ì—…ìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ë‹¤ì‹œ ë§í•´ kernelì€ **asynchronous**(ë¹„ë™ê¸°ì )ìœ¼ë¡œ launchëœë‹¤. hostëŠ” kernel launchê°€ ì™„ë£Œë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

> ì´í›„ ì„¤ëª…í•˜ê² ì§€ë§Œ, CUDA runtimeì—ì„œ ì œê³µí•˜ëŠ” `cudaDeviceSynchronize`ë¥¼ ì´ìš©í•´ì„œ CPUê°€ device codeì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.

---

## 2.5 managing memory

![GPU memory hierarchy](images/GPU_memory_hierarchy.png)

CUDA runtimeì€ device memoryë¥¼ allocateí•˜ëŠ” functionë“¤ì„ ì œê³µí•œë‹¤.

| í‘œì¤€ C function | CUDA C function |
| --- | --- |
| malloc | cudaMalloc |
| memcpy | cudaMemcpy |
| memset | cudaMemset |
| free | cudaFree |

---

### 2.5.1 cudaMalloc

ìš°ì„  GPU memory allocationì„ ìœ„í•œ functionìœ¼ë¡œ `cudaMalloc`ì„ ì‚¬ìš©í•œë‹¤. ë‘ ê°€ì§€ parameterê°€ í•„ìš”í•˜ë‹¤.

```c
cudaError_t cudaMalloc ( void** devPtr, size_t size )
```

- `cudaError_t`: error ë°œìƒ ì‹œ error ì •ë³´ë¥¼ ë‹´ëŠ” enumerated type

- í• ë‹¹ í›„ pointer 'devPtr'ë¡œ device memory addressê°€ ë°˜í™˜ëœë‹¤.

---

### 2.5.2 cudaMemcpy

hostì™€ device ì‚¬ì´ data transferì„ ìœ„í•´ functionìœ¼ë¡œ `cudaMemcpy`ë¥¼ ì‚¬ìš©í•œë‹¤. 

> unified memoryì— ì¡´ì¬í•˜ëŠ” dataê°€ ì•„ë‹ˆë¼ë©´, `cudaMemcpy`ë¥¼ ì´ìš©í•´ dataë¥¼ ì‚¬ì „ì— device memoryë¡œ ì „ë‹¬í•´ì•¼ í•œë‹¤.

```c
cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )
```

- dst: destination memory address pointer

- src: source memory address pointer

- count: copyí•  byte size

- kind: data transfer type

  - `cudaMemcpyHostToHost`

  - `cudaMemcpyHostToDevice`

  - `cudaMemcpyDeviceToHost`

  - `cudaMemcpyDeviceToDevice`

<U>cudaMemcpyëŠ” synchronous behavior</U>ì´ë‹¤. ë”°ë¼ì„œ host applicationì€ cudaMemcpyì˜ return/transferê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ë©ˆì¶”ê²Œ ëœë‹¤.

ì°¸ê³ ë¡œ kernel launchë¥¼ ì œì™¸í•œ ëª¨ë“  CUDA callì€, enumerated type `cudaError_t`ìœ¼ë¡œ error codeë¥¼ returní•œë‹¤. 

- ë§Œì•½ GPU memoryì— ì„±ê³µì ìœ¼ë¡œ allocateí–ˆë‹¤ë©´, `cudaSuccess`ë¥¼ returní•œë‹¤.

- ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ `cudaErrorMemoryAllocation`ì„ returní•œë‹¤.

ë‹¤ìŒ functionì„ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ error messageë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.(Cì˜ strerror functionê³¼ ë¹„ìŠ·í•˜ë‹¤.)

```c
char* cudaGetErrorString(cudaError_t error)
```

---

### 2.5.3 array summation example

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: array summation&nbsp;&nbsp;&nbsp;</span>

![array summation](images/array_summation.png)

array summation ì˜ˆì œë¥¼ ë³´ë©° hostì™€ deviceê°„ì˜ data movementê°€ ì–´ë–»ê²Œ ì¼ì–´ë‚˜ëŠ”ì§€ ì‚´í´ë³´ì. 

ìš°ì„  ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ array ì—°ì‚°(host-based array summation)ì„ ì˜¤ì§ Cë§Œ ì‚¬ìš©í•´ì„œ êµ¬í˜„í•  ê²ƒì´ë‹¤. ì´ë¥¼ GPU codeë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.

> íŒŒì¼ëª…ì€ sumArraysOnHost.cì´ë‹¤. 

> [int main(int argc, char **argv)ë€?](https://iamaman.tistory.com/364)

```c
#include <stdlib.h>
#include <string.h>
#include <time.h>

// Hostì—ì„œ array sum ìˆ˜í–‰
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    for (int idx=0; idx<N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

// arrayì— random numberë¡œ ì´ˆê¸°ê°’ì„ ì„¤ì •
void initialData(float *ip, int size) {
    // random number ìƒì„±
    time_t t;
    srand((unsigned int) time(&t));

    for (int i=0; i<size; i++) {
        ip[i] = (float) ( rand() & 0xFF )/10.0f;
    }
}

int main(int argc, char **argv) {
    int nElem = 1024;
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *h_C;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    h_C = (float *)malloc(nBytes);

    initialData(h_A, nElem);
    initialData(h_B, nElem);

    sumArraysOnHost(h_A, h_B, h_C, nElem);

    free(h_A);
    free(h_B);
    free(h_C);

    return(0);
}
```

pure C programì´ë¯€ë¡œ C compilerë¥¼ ì‚¬ìš©í•´ë„ ê´œì°®ê³ , nvcc compilerë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ compileí•´ë„ ëœë‹¤.

```bash
$ nvcc -Xcompiler -std=c99 sumArraysOnHost.c -o sum
$ ./sum
```

ì°¸ê³ ë¡œ ìœ„ compile ëª…ë ¹ì˜ **flag**(ì˜µì…˜)ëŠ” ë‹¤ìŒ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.

- -Xcompiler: ì¶”ê°€ ì¸ìë¥¼ compilerì—ê²Œ ì „ë‹¬í•˜ë¼ëŠ” ì˜ë¯¸.

- -std=c99: code styleì´ c99 standardì„ì„ ì•Œë¦°ë‹¤.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

pure C codeë¥¼ GPU ë²„ì „ìœ¼ë¡œ ë°”ê¿”ë³´ì.

1. GPU memory allocation(`cudaMalloc`)

    ```c
    /* CPU
    float *h_A, *h_B, *h_C;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    h_C = (float *)malloc(nBytes);
    */

    float *d_A, *d_B, *d_C;
    cudaMalloc((float**) &d_A, nBytes);
    cudaMalloc((float**) &d_B, nBytes);
    cudaMalloc((float**) &d_B, nBytes);
    ```

2. `cudaMemcpy`ë¥¼ í†µí•´ GPU global memoryë¡œ dataë¥¼ transferí•œë‹¤.

    ```c
    cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);
    ```

3. kernel launch

    host sideì—ì„œ GPUê°€ array summationì„ ìˆ˜í–‰í•˜ë„ë¡ kernel functionì„ launchí•œë‹¤.
    
    kernel launch ì‹œ controlì€ ì¦‰ì‹œ hostë¡œ return backë˜ë©°, GPUê°€ kernelì„ ìˆ˜í–‰í•˜ëŠ” ì‚¬ì´ì— ë‹¤ë¥¸ functionì„ ìˆ˜í–‰í•œë‹¤.(asynchronous)

4. `cudaMemcpy`ë¥¼ í†µí•´ GPUê°€ ê³„ì‚°í•œ resultë¥¼ host memoryë¡œ copyí•œë‹¤.

    kernel ì‘ì—…ì´ ëª¨ë‘ ëë‚˜ë©´, 'result(array d_C)'ëŠ” GPU global memoryì— ì €ì¥ë  ê²ƒì´ë‹¤. 
    
    ì´ resultë¥¼ host array(gpuRef)ë¡œ copyí•´ì•¼ í•œë‹¤.

    ```c
    cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);
    ```

    > ë§Œì•½ ì´ë ‡ê²Œ copyë¥¼ ì§„í–‰í•˜ì§€ ì•Šê³  'gpuRef = d_C'ì™€ ê°™ì€ ì˜ëª»ëœ assignmentë¬¸ìœ¼ë¡œ ì‘ì„±í•œë‹¤ë©´ runtime crashê°€ ë°œìƒí•œë‹¤.

    > ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ CUDA 6ë¶€í„° unified memoryê°€ ì œê³µëë‹¤.

5. memoryë¥¼ releaseí•œë‹¤.

    ```c
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
    ```

---

## 2.6 organizing threads

> [thread block architecture](https://tododiary.tistory.com/57)

'threadë“¤ì„ ì–´ë–»ê²Œ êµ¬ì„±í•  ê²ƒì¸ê°€'ë¼ëŠ” ë¬¸ì œê°€ CUDA programmingì— ìˆì–´ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì´ë‹¤. **thread**ëŠ” **grid**ì™€ **block**ì˜ 2-level hierarcyë¡œ êµ¬ì„±ëœë‹¤. 

- grid: ì—¬ëŸ¬ blockìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

- block: í•˜ë‚˜ ì´ìƒì˜ threadë¡œ êµ¬ì„±ëœë‹¤.

  > blockì€ threadë¥¼ ìµœëŒ€ 512ê°œê¹Œì§€ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. 
  
  > blockì´ ê°€ì§€ëŠ” thread ê°œìˆ˜ëŠ” 32(NVIDIAëŠ” 64ë¥¼ ê¶Œì¥) ë°°ìˆ˜ë¡œ ì§€ì •í•˜ëŠ” í¸ì´ ì¢‹ë‹¤.(warp íŠ¹ì„±ìƒ)

ê·¸ë¦¼ì„ ë³´ë©° ìì„¸íˆ ì‚´í´ë³´ì. hostì—ì„œ kernel launchë¥¼ ìˆ˜í–‰í•˜ë©´, deviceì— ë‹¤ìŒê³¼ ê°™ì€ thread hierarchyê°€ ìƒì„±ëœë‹¤.

![thread hierarchy](images/thread_hierarchy.png)

- **grid**: í•œ ë²ˆì— kernel launchì—ì„œ ìƒì„±ëœ thread ì „ì²´.

  - ê°™ì€ global memory spaceë¥¼ ê³µìœ í•œë‹¤.

  - ë™ì¼í•œ kernel codeë¥¼ ì‹¤í–‰í•œë‹¤.

- gridëŠ” ì—¬ëŸ¬ thread **block**ë“¤ë¡œ êµ¬ì„±ëœë‹¤. block ë‹¨ìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì„±ì„ ê°–ëŠ”ë‹¤.

  - block-local synchronization

  - block-local shared memory

  > <U>ë‹¤ë¥¸ blockì˜ threadë¼ë¦¬ëŠ” cooperateí•  ìˆ˜ ì—†ë‹¤.</U>

---

### 2.6.1 blockIdx, threadIdx

CUDA runtimeì— ì˜í•´ ê° threadë³„ë¡œ index(coordinate variable)ê°€ í• ë‹¹ëœë‹¤.

![blockIdx, threadIdx](images/blockIdx_threadIdx.png)

- `blockIdx`: block index

- `threadIdx`: block ë‚´ë¶€ì—ì„œ ê°–ëŠ” thread index

> coordinate variableì€ `uint3 type`ì´ë‹¤. 3ê°œì˜ unsigned integerë¡œ êµ¬ì„±ë˜ë©°, `.x`, `.y`, `.z`ë¥¼ ë¶™ì—¬ì„œ componentì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```c
blockIdx.x
blockIdx.y
blockIdx.z

// block ë‚´ ëª¨ë“  threadëŠ” ë™ì¼í•œ blockIdxë¥¼ ê³µìœ í•œë‹¤.
threadIdx.x
threadIdx.y
threadIdx.z
```

---

### 2.6.2 blockDim, gridDim

kernel launch êµ¬ë¬¸ì€ execution configutation parameters(`<<<...>>>`)ë¡œ gridì™€ blockì˜ dimensionì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤. 

> gridëŠ” ì£¼ë¡œ 2D array block, blockì€ ì£¼ë¡œ 3D array threadë¡œ êµ¬ì„±ëœë‹¤.

> ì§€ì •í•˜ì§€ ì•Šì€ ì°¨ì›ì˜ í¬ê¸°ëŠ” 1ë¡œ ì§€ì •ëœë‹¤.(default: 1)

 gridì™€ block dimensionì€ ë‹¤ìŒ built-in variableë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- `blockDim`

- `gridDim`

ìœ„ variableë“¤ì€ `dim3` typeì´ë©°, `uint3`ì— ê¸°ë°˜í•´ dimensionì— íŠ¹í™”ëœ integer vector typeì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ `.x`, `.y`, `.z`ë¥¼ ë¶™ì—¬ì„œ componentì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

ì´ë¥¼ í†µí•´ threadë§ˆë‹¤ ìœ ì¼í•œ global index `i`ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

- `i = blockIdx.x * blockDim.x + threadIdx.x`

ë§Œì•½ blockì´ 1ì°¨ì›ì´ë©° threadë¥¼ 256ê°œ ê°–ëŠ”ë‹¤ê³  í•˜ì. ì´ ê²½ìš° global indexëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- block 0: thread index `i`ì˜ ë²”ìœ„ëŠ” 0~255

- block 1: thread index `i`ì˜ ë²”ìœ„ëŠ” 256~511

- block 2: thread index `i`ì˜ ë²”ìœ„ëŠ” 512~767

ë¶ˆí¸í•´ ë³´ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, ì´ëŸ¬í•œ ë°©ì‹ ë•ë¶„ì— kernel functionì€ loopê°€ ì—†ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ C codeê°€ ìˆë‹¤ë©´, ê° threadê°€ iteration í•˜ë‚˜ì”©ì„ ë‹´ë‹¹(M*Nê°œ thread)í•˜ì—¬ parallelí•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.(ì‹¤ì œë¡œëŠ” ê³ ë ¤í•´ì•¼ í•˜ëŠ” ìš”ì†Œê°€ ë” ë§ë‹¤.)

```c
for(i = 0, i < N, i++)
    for (j = 0, j < M, j++)
        convolution(i, j);
```

ì´ëŸ° ì¢…ë¥˜ì˜ data parallelismì„ **loop-level parallelism**ì´ë¼ê³  ì§€ì¹­í•œë‹¤.

ë‹¨, element(vector size)ë¥¼ threadë¡œ mappingí•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‚¬í•­ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì´ 100ê°œì˜ elementë¥¼, (íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ê°€ì¥ ì‘ì€ thread ê°œìˆ˜ì¸) 32ë¡œ ë‚˜ëˆ´ë‹¤ê³  í•˜ì. 

ê·¸ë ‡ë‹¤ë©´ blockì€ ì´ 4ê°œê°€ ìƒê¸°ê³ , ì´ 128ê°œì˜ threadë¥¼ ê°€ì§€ê²Œ ëœë‹¤. ê·¸ëŸ°ë° ì´ ê²½ìš° 28ê°œì˜ threadëŠ” ë¹„í™œì„±í™”í•´ì•¼(ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ì•¼) í•œë‹¤. 

---

### 2.6.3 grid, block dimension example

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: gridì™€ block dimension êµ¬í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

host, device ì–‘ìª½ì—ì„œ gridì™€ block dimensionì„ ì²´í¬í•´ ë³´ì.

- 'checkIndex'ëŠ” thread index, block index, grid dimensionì„ ì¶œë ¥í•˜ëŠ” kernel functionì´ë‹¤. 

> íŒŒì¼ëª…ì€ checkDimension.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void checkIndex(void) {
    printf("threadIdx: (%d, %d, %d) blockIdx: (%d, %d, %d) blockDim: (%d, %d, %d) "
        "gridDim: (%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z,
        blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z,
        gridDim.x, gridDim.y, gridDim.z);
}

int main(int argc, char **argv) {
    // number of elements
    int nElem = 6;

    // grid, block structure ì •ì˜
    // ì§€ì •í•˜ì§€ ì•Šì€ ì°¨ì›ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒ(1)ìœ¼ë¡œ ì²˜ë¦¬

    // 3ê°œì˜ threadë¥¼ í¬í•¨í•˜ëŠ” 1ì°¨ì› block (3, 1, 1)
    dim3 block (3);
    // í•„ìš”í•œ grid ê°œìˆ˜. (6 + 2)/3 = 2
    // -> 2ê°œ blockì„ í¬í•¨í•˜ëŠ” 1ì°¨ì› grid (2, 1, 1)
    dim3 grid  ((nElem + block.x - 1)/block.x);

    // CPUì—ì„œ grid, block dimension ì²´í¬
    printf("grid.x %d grid.y %d grid.z %d\n", grid.x, grid.y, grid.z);
    printf("block.x %d block.y %d block.z %d\n", block.x, block.y, block.z);

    // GPUì—ì„œ grid, block dimension ì²´í¬
    checkIndex <<<grid, block>>> ();

    // reset device
    cudaDeviceReset();

    return(0);
}
```

ì°¸ê³ ë¡œ CUDAë¡œ `printf` functionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ”, compile ë•Œ GPU architectureë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤.

```bash
nvcc -arch=sm_80 checkDimension.cu -o check
./check
```

> ì±…ì€ Fermi GPUì´ë¯€ë¡œ -arch=sm_20ì„ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤.([CUDA -arch í™•ì¸](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/))

> í˜„ì¬ ì‹¤ìŠµ ì¤‘ì¸ í™˜ê²½ì€ RTX 3080Tië¡œ Ampere architectureì´ë‹¤.(sm_86)($ nvidia-smi -q ëª…ë ¹ìœ¼ë¡œ í™•ì¸)([GPU í™•ì¸](https://kyumdoctor.tistory.com/72))

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![grid block indices and dimensions](images/grid_block_indice.png)

ì´ ì˜ˆì œì—ì„œ ëª…ì‹¬í•  ì ì€ grid, block variableì„, hostì™€ deviceê°€ ì„œë¡œ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•œë‹¤ëŠ” ì ì´ë‹¤.

- host: 'block', 'grid'ë¥¼ declarationí•œ ë’¤, 'block.x', 'block.y', 'block.z'ì„ ì‚¬ìš©.

- device: built-in variableì¸ `blockDim.x`, `blockDim.y`, `blockDim.z`ë¥¼ ì‚¬ìš©.

ì •ë¦¬í•˜ìë©´ data sizeê°€ ì£¼ì–´ì¡Œì„ ë•Œ, gridì™€ block dimensionì€ ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ ì •í•œë‹¤.

1. block sizeë¥¼ ë¨¼ì € ê²°ì •

2. grid dimensionì„ 'data size'ì™€ 'block size'ë¥¼ ì´ìš©í•´ ê²°ì •

> ë‹¨, block dimensionì„ ì •í•  ë•ŒëŠ” GPU resourceì˜ limitationì„ ìˆ™ì§€í•´ì•¼ í•œë‹¤.

---

## 2.7 launching a CUDA kernel

ì•ì„œ CUDA kernel callì€ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ë²•ì„ ì‚¬ìš©í–ˆë‹¤.

```c
kernel_name <<<grid, block>>>(argument list);
```

ë‹¤ìŒ ë‘ ì˜ˆì‹œë¥¼ ë³´ì.

- `kernel_name<<<1, 32>>>(argument list)`: í•œ blockì— 32ê°œì˜ elementë¥¼ ë‹´ëŠ”ë‹¤.

- `kernel_name<<<32, 1>>>(argument list)`: element í•˜ë‚˜ë§Œì„ ê°–ëŠ” 32ê°œì˜ blockì„ ìƒì„±í•œë‹¤.

ë˜í•œ kernelì€ ë‹¤ìŒê³¼ ê°™ì´ type qualifierë¡œ `__global__`ì„ ë¶™ì—¬ì„œ declarationí•´ì•¼ í•œë‹¤. ì´ë•Œ <U>kernel functionì€ ê¼­ void return type</U>ì„ ê°€ì ¸ì•¼ í•œë‹¤.

```c
__global__ void kernel_name(argument list);
```

ì•„ë˜ëŠ” CUDA C programmingì—ì„œì˜ type qualifierë¥¼ ì •ë¦¬í•œ í‘œì´ë‹¤.

| qualifer | execution | callable | ì„¤ëª… |
| :---: | :---: | :---: | :---: |
| \_\_global\_\_ | device | hostì—ì„œ callable<br/>NVIDIAê°€ ì œì‹œí•˜ëŠ” compute capabilityê°€ 3 ì´ìƒì¸ device | void return typeì´ì–´ì•¼ í•œë‹¤. |
| \_\_device\_\_ | device | device only | |
| \_\_host\_\_ | host | host only | ìƒëµ ê°€ëŠ¥. |

ì°¸ê³ ë¡œ functionì´ hostì™€ device ì–‘ìª½ì—ì„œ compileëœë‹¤ë©´, \_\_device\_\_ì™€ \_\_host\_\_ qualifierë¥¼ í•¨ê»˜ ì¨ì„œ ì„ ì–¸í•´ë„ ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
__host__ __device__ const char* cudaGetErrorString(cudaError_t error)
```

host functionê³¼ GPU kernelì˜ ì°¨ì´ë¥¼ vector addition(A+B = C)ìœ¼ë¡œ ì‚´í´ë³´ì.

- host code

    ```c
    void sumArraysOnHost(float *A, float *B, float *C, const int N) {
      for (int i = 0; i < N; i++) {
        C[i] = A[i] + B[i];
      }
    }
    ```

- CUDA

  loopê°€ ì‚¬ë¼ì§€ê³  built-in thread coordinate variableì´ array indexë¥¼ ëŒ€ì‹ í–ˆë‹¤. ë˜í•œ Nê°œì˜ threadë¥¼ launchí•˜ë©´ì„œ, Nì„ referenceí•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤.

    ```c
    __global__ void sumArraysOnGPU(float *A, float *B, float *C) {
      int i = threadIdx.x;
      C[i] = A[i] + B[i];
    }
    ```

---

### 2.7.1 kernel limitation

CUDA kernelì´ ê°–ëŠ” ì œì•½ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- device memoryë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤.

- void return typeë§Œ ê°€ëŠ¥í•˜ë‹¤.

- ìœ ë™ì ì¸ ìˆ«ìì˜ variableì„ argumentë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

- static variableì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- function pointerë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- asynchronousí•œ íŠ¹ì„± ë•Œë¬¸ì— debuggingì´ í˜ë“¤ë‹¤.

---