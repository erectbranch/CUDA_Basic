# 2 CUDA Programming Model (Part II)

## 2.8 Handling Errors

> [error handling functions](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html)

CUDA kernel launchë¡œ ë°œìƒí•˜ëŠ” errorëŠ” synchronous errorì™€ asynchronos error ë‘ ê°€ì§€ typeìœ¼ë¡œ ë‚˜ë‰œë‹¤.

- synchronous error: hostì—ì„œ kernelì´ illegalí•˜ê±°ë‚˜ invalidí•œ ê²ƒì„ ì•Œê²Œ ë˜ë©´ ë°œìƒí•œë‹¤. 

  - ì˜ˆë¥¼ ë“¤ì–´ thread block sizeë‚˜ grid sizeë¥¼ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í–ˆë‹¤ë©´, kernel launch callì´ ì‹¤í–‰ë˜ëŠ” ë™ì‹œì— ë°”ë¡œ synchronous errorë¥¼ ë°œìƒì‹œí‚¨ë‹¤.

- asynchronous error: kernel execution, í˜¹ì€ CUDA runtime asynchronous API execution ì¤‘ ë°œìƒí•œë‹¤.

  - ì˜ˆë¥¼ ë“¤ì–´ kernel execution ì¤‘ ì˜ëª»ëœ memory addressì— ì ‘ê·¼í•˜ë©´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. (cudaMemcpyAsyncì™€ ê°™ì€ CUDA runtime asynchronous API executionì—ì„œ ë°œìƒí•  ìˆ˜ ìˆë‹¤.)

> kernel launch call ë°”ë¡œ ë‹¤ìŒì— cudaGetLastError APIë¥¼ ì‚¬ìš©í•´ì„œ error capturingë„ ê°€ëŠ¥í•˜ë‹¤.

> ì°¸ê³ ë¡œ í•´ê²°í•˜ê¸° ì–´ë ¤ìš´, not-recoverable errorë¥¼ **sticky error**, recoverableí•œ errorë¥¼ **non-sticky error**ë¼ê³  ì§€ì¹­í•˜ê¸°ë„ í•œë‹¤.

> cudaMallocì—ì„œ GPU memory ë¶€ì¡±ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” errorëŠ” non-sticky errorì— í•´ë‹¹í•œë‹¤. ë°˜ë©´ host processê°€ terminateë˜ê¸° ì „ê¹Œì§€ CUDA contextê°€ corruptë˜ëŠ” errorëŠ” sticky errorì— í•´ë‹¹í•œë‹¤.

ëŒ€ì²´ë¡œ kernelì´ asynchronousí•˜ê¸° ë•Œë¬¸ì—, errorê°€ ì–´ë””ì„œ ë°œìƒí–ˆëŠ”ì§€ ì•Œê¸° í˜ë“¤ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ CUDA API callë“¤ì„ ê²€ì¦í•˜ëŠ” macroë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•˜ë©´ ë¶ˆí¸í•¨ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

```c
#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}
```

ì´ë ‡ê²Œ ë§Œë“¤ì—ˆë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ kernelì„ ê°ì‹¸ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```c
CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));
```

debugging ëª©ì ìœ¼ë¡œ í•œì •ì§€ìœ¼ë©´, kernel errorë¥¼ checkí•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í…Œí¬ë‹‰ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ preceding requested taskê°€ ëë‚  ë•Œê¹Œì§€ host applicationì„ ë§‰ëŠ” ê²ƒì´ë‹¤.

```c
kernel_function<<<grid, block>>>(argument list);
CHECK(cudaDeviceSynchronize());    // cudaError_t cudaDeviceSynchronize(void);ëŠ” ë°”ë¡œ ì „ asynchronous CUDA operationsì˜ errorë¥¼ returní•œë‹¤.
```

---

## 2.9 compiling and Executing

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: vector addition&nbsp;&nbsp;&nbsp;</span>

1ì°¨ì› array A, B(ì¦‰, vector)ì˜ vector additionì„ ìˆ˜í–‰í•œ ë’¤ ì—°ì‚° ê²°ê³¼ë¥¼ array Cì— ì €ì¥í•  ê²ƒì´ë‹¤. ì—°ì‚°ì€ host ë²„ì „ vector addition(sumArraysOnHost)ê³¼ GPU ë²„ì „ vector addition(sumArraysOnGPU)ì„ ëª¨ë‘ ìˆ˜í–‰í•œ ë’¤ ì„œë¡œì˜ ì—°ì‚° ê²°ê³¼ë¥¼ ë¹„êµ(checkResult)í•´ ë³¼ ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ sumArraysOnGPU-small-case.cuì´ë‹¤.

> ê³„ì‚° ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ê²€í† í•˜ë ¤ë©´ double typeì˜ ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ë‚´ì—ì„œ ë¹„êµë¥¼ í•˜ë©´ ëœë‹¤. í˜„ì¬ ì˜ˆì œì—ì„œëŠ” ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ì„ 1.0e-8 ì´í•˜ê¹Œì§€ í—ˆìš©í•˜ê²Œ êµ¬ì„±í–ˆë‹¤.

- vector size: 32

- ë‹¤ìŒê³¼ ê°™ì´ í•œ blockì´ 32ê°œì˜ elementìœ¼ë¡œ êµ¬ì„±ë˜ë„ë¡ í–ˆë‹¤.

  ```c
  dim3 block (nElem);    // nElem = 32
  dim3 grid  (nElem/block.x); 
  ```

  - ì•„ë˜ì™€ ê°™ì´ blockë‹¹ 1ê°œì˜ elementë¡œ êµ¬ì„±í•˜ëŠ” ëŒ€ì‹ , blockì„ 32ê°œë¡œ ì„¤ì •í•´ë„ ë¬´ë°©í•˜ë‹¤. ë‹¤ë§Œ kernelì´ ì‚¬ìš©í•˜ëŠ” indexë„ ìˆ˜ì •í•´ì•¼ í•œë‹¤.

    ```c
    dim3 block (1);
    dim3 grid  (nElem);

    //...
    int i = blockIdx.x;
    ```

```c
#include <cuda_runtime.h>
#include <stdio.h>

#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}

void checkResult(float *hostRef, float *gpuRef, const int N) {
    double epsilon = 1.0E-8;
    bool match = 1;
    for (int i = 0; i < N; i++) {
        if (abs(hostRef[i] - gpuRef[i]) > epsilon) {
            match = 0;
            printf("Arrays do not match!\n");
            printf("host %5.2f gpu %5.2f at current %d\n", hostRef[i], gpuRef[i], i);
            break;
        }
    }
    
    if (match) printf("Arrays match.\n\n");
}

void initialData(float *ip, int size) {
    // generate different seed for random number
    // Arrayì— random elementë¥¼ ì±„ìš´ë‹¤.
    time_t t;
    srand((unsigned) time(&t));

    for (int i = 0; i < size; i++) {
        ip[i] = (float)( rand() & 0xFF )/10.0f;
    }
}

void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    // Array A, Bì˜ ê° elementë¥¼ í•©ì‚° = Array C
    for (int idx = 0; idx < N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaSetDevice(dev);

    // set up data size of vectors
    int nElem = 32;
    printf("Vector size: %d\n", nElem);

    // malloc host memory
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A     = (float *)malloc(nBytes);
    h_B     = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef  = (float *)malloc(nBytes);

    // initialize data at host side
    initialData(h_A, nElem);
    initialData(h_B, nElem);

    memset(hostRef, 0, nBytes);
    memset(gpuRef, 0, nBytes);

    // malloc device global memory
    float *d_A, *d_B, *d_C;
    cudaMalloc((float**)&d_A, nBytes);
    cudaMalloc((float**)&d_B, nBytes);
    cudaMalloc((float**)&d_C, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    dim3 block (nElem);
    dim3 grid  (nElem/block.x);

    sumArraysOnGPU<<< grid, block >>>(d_A, d_B, d_C);
    printf("Execution configuration <<<%d, %d>>>\n", grid.x, block.x);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);

    // add vector at host side for result checks
    sumArraysOnHost(h_A, h_B, hostRef, nElem);

    // check device results
    checkResult(hostRef, gpuRef, nElem);

    // free device global memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•´ì„œ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
$ nvcc sumArraysOnGPU-small-case.cu -o addvector
$ ./addvector
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![addvector result](images/addvector.png)

---

## 2.10 timing kernel

kernelì´ ì–¼ë§ˆë‚˜ ì‹œê°„ì„ ì†Œëª¨í•˜ëŠ”ì§€, ì–´ëŠ ì •ë„ê°€ ì ì •í•œ ì†Œëª¨ ì‹œê°„ì¸ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. host sideì—ì„œ CPU timerë‚˜ GPU timerë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ì†Œëª¨í•œ ì‹œê°„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

---

### 2.10.1 timing with CPU timer

sys/time.h ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ gettimeofday()ë¥¼ ì´ìš©í•´ì„œ CPU timerë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

```c
double cpuSecond() {
    struct timeval tp;
    gettimeofday(&tp, NULL);
    return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);
}
```

ì´ë ‡ê²Œ timerë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´ kernelì„ ì‹œì‘í•˜ëŠ” ì‹œê°„ì„ ê¸°ë¡í•œ ë’¤, ëë‚œ ì‹œì ì—ì„œ ë‘ ì‹œê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ë©´ kernel ìˆ˜í–‰ ì‹œê°„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

```c
double iStart = cpuSecond();
kernel_name<<<grid, block>>>(argument list);
cudaDeviceSynchronize();
double iElaps = cpuSecond() - iStart;
```

í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì‹œê°„ì„ ì¸¡ì •í•  ê²½ìš°, CPUê°€ ëª¨ë“  GPU threadê°€ ì‘ì—…ì„ ì™„ë£Œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë„ë¡ cudaDeviceSynchronize()ë¥¼ ê¼­ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: vector addition ì‹œê°„ ì¸¡ì •í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

- ë¹„íŠ¸ ì—°ì‚°ìë¥¼ ì´ìš©í•´ì„œ elementë¥¼ ì•½ 1600ë§Œ ê°œë¡œ ì„¤ì •í•œë‹¤.(16,777,216ê°œ)

    ```c
    int nElem = 1<<24;
    ```

- kernelì˜ vector addition ë•Œ, array boundë¥¼ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ ê¼­ indexë¥¼ ì ê²€í•´ì•¼ í•œë‹¤.(total thread ê°œìˆ˜ê°€ vector element ê°œìˆ˜ë³´ë‹¤ ë§ê¸° ë•Œë¬¸)

   ```c
   __global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];    // 
    }
   }
   ```

   ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´ê°€ ì‰½ë‹¤.

   ![vector elements < total threads](images/threads_and_vector_elements.png)

ë‹¤ìŒì€ sumArraysOnGPU-timer.cu ì½”ë“œì´ë‹¤. ì•ì„œ ë³¸ ëª‡ ê°€ì§€ functionì€ ìƒëµí•´ì„œ ê¸°ë¡í–ˆë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>
#include <sys/time.h>

// error handlingì„ ìœ„í•œ CHECK macro ìƒëµ

// CPU timer ì—­í–˜ì„ í•˜ëŠ” cpuSecond() ìƒëµ

// random elementë¥¼ ìƒì„±í•˜ëŠ” initialData() ìƒëµ

// host sideì—ì„œ vector additionì„ ìˆ˜í–‰í•˜ëŠ” sumArraysOnHost() ìƒëµ

// device sideì—ì„œ vector additionì„ ìˆ˜í–‰í•˜ëŠ” sumArraysOnGPU() ìƒëµ

// host/deviceì˜ vector addition ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” checkResult() ìƒëµ

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set up data size of vectors
    int nElem = 1<<24;
    printf("Vector size %d\n", nElem);

    // malloc host memory
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A     = (float *)malloc(nBytes);
    h_B     = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef  = (float *)malloc(nBytes);

    double iStart, iElaps;

    // initialize data at host side
    iStart = cpuSecond();
    initialData(h_A, nElem);
    initialData(h_B, nElem);
    iElaps = cpuSecond() - iStart;

    memset(hostRef, 0, nBytes);
    memset(gpuRef,  0, nBytes);

    // add vector at host side for result checks
    iStart = cpuSecond();
    sumArraysOnHost(h_A, h_B, hostRef, nElem);
    iElaps = cpuSecond() - iStart;

    // malloc device global memory
    float *d_A, *d_B, *d_C;
    cudaMalloc((float**)&d_A, nBytes);
    cudaMalloc((float**)&d_B, nBytes);
    cudaMalloc((float**)&d_C, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    int iLen = 1024;
    dim3 block(iLen);
    dim3 grid ((nElem + block.x - 1)/block.x);

    iStart = cpuSecond();
    sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);
    cudaDeviceSynchronize();
    iElaps = cpuSecond() - iStart;
    printf("sumArraysOnGPU<<<%d, %d>>> Time elapsed %f" \
        "sec\n", grid.x, block.x, iElaps);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);

    // check device results
    checkResult(hostRef, gpuRef, nElem);

    // free device global memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    return(0);  
}
```

ë‹¤ìŒê³¼ ê°™ì´ compile í›„ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc sumArraysOnGPU-timer.cu -o sumArraysOnGPU-timer
$ ./sumArraysOnGPU-timer
```

![sumArraysOnGPU-timer 1024 thread](images/sumArraysOnGPU-timer_1.png)

ì˜ˆì œëŒ€ë¡œ êµ¬ì„±í•˜ë©´ 1D gridê°€ 16,384ê°œì˜ blockì„ ê°–ëŠ”ë‹¤. ê·¸ë¦¬ê³  ê° blockì€ thread 1,024ê°œë¥¼ ê°€ì§„ë‹¤.)

ì—¬ê¸°ì„œ 1 blockì´ ê°–ëŠ” thread ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” ëŒ€ì‹ , block ìˆ˜ë¥¼ 2ë°°ë¡œ ëŠ˜ë¦¬ë©´ ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ë‹¬ë¼ì§„ë‹¤.(iLen = 512)

![sumArraysOnGPU-timer 512 thread](images/sumArraysOnGPU-timer_2.png)

> Tesla device ê¸°ì¤€ìœ¼ë¡œëŠ” 0.002058 secì—ì„œ 0.000183 secë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤.

ì´ë³´ë‹¤ ë” thread ìˆ˜(block dimension)ë¥¼ ì¤„ì´ê³ , block ìˆ˜(grid dimension)ë¥¼ ëŠ˜ë¦¬ê²Œ ë˜ë©´ deviceì— ë”°ë¼ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. deviceì— ë”°ë¼ ê° thread hierarchy levelì˜ maximum sizeê°€ ë‹¤ë¥´ë¯€ë¡œ ìœ ì˜í•´ì•¼ í•œë‹¤.

![fermi device error](images/fermi_architecture_grid_dimension_limit.png)

> ì˜ˆë¥¼ ë“¤ì–´ Tesla deviceëŠ” blockì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” thread ìˆ˜ì˜ í•œê³„ê°€ 1,024ê°œì´ë©°, grid dimension ìˆ˜ì¹˜ëŠ” ê° x,y,z ì°¨ì›ì—ì„œ ìµœëŒ€ 65,535ê¹Œì§€ë§Œ ê°€ëŠ¥í•˜ë‹¤.

---

### 2.10.2 timing with nvprof

CUDA 5.0ë¶€í„° nvprofë¥¼ ì´ìš©í•´ì„œ applicationì˜ CPUì™€ GPU activity timelineì„ ì•Œ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```bash
$ nvprof [nvprof_args] <application> [application_args]
```

> \$ nvprof --help ëª…ë ¹ìœ¼ë¡œ ë” ìì„¸í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.

ì•ì„  ì˜ˆì œë¥¼ nvprofë¡œ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.

```bash
$ nvprof ./sumArraysOnGPU-timer
```

ìœ„ ëª…ë ¹ì„ ì…ë ¥í•´ì„œ Tesla GPUì—ì„œ ì‚´í´ë³¸ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì ˆë°˜ì€ program outputì„ ë‹´ì€ messageì´ê³ , ë‚˜ë¨¸ì§€ê°€ nvprofì˜ outputì— í•´ë‹¹í•œë‹¤.

![nvprof report 1](images/nvprof_report_1.png)

![nvprof report 2](images/nvprof_report_2.png)

- 2.9024ms: nvprofê°€ ì¸¡ì •í•œ kernel time

nvprofì˜ ì¸¡ì • ê°’ì¸ 2.9024msëŠ” CPU timerë¡œ ì¸¡ì •í•œ kernel timeì¸ 3.26msì™€ ë‹¤ë¥´ë‹¤. ì´ëŠ” CPU timer outputì´ nvprof ì²˜ë¦¬ë¥¼ ìœ„í•´ ë•Œë¬¸ì— ìƒê¸°ëŠ” overheadë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì´ë‹¤.(ë”°ë¼ì„œ nvprofì˜ ê²°ê³¼ê°€ ë” ì •í™•í•˜ë‹¤.)

![nvprof report diagram](images/nvprof_report_diagram.png)

> kernel ìˆ˜í–‰ë³´ë‹¤ë„ hostì™€ device ì‚¬ì´ì—ì„œ ì¼ì–´ë‚˜ëŠ” data transferê°€ í›¨ì”¬ ì‹œê°„ì„ ì¡ì•„ë¨¹ì—ˆë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ:nvprofë¥¼ ì´ìš©í•˜ì—¬ theoretical limitê³¼ ë¹„êµí•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

nvprofë¥¼ ì´ìš©í•´ì„œ applicationì˜ instructionê³¼ memory throughputì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ ê³„ì‚°ì„ theoretical peak value(ì´ë¡ ì ì¸ í•œê³„ê°’)ì™€ ë¹„êµí•˜ë©´, applicationì´ ì–´ë–¤ ì œì•½ì„ ë°›ê³  ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

ê³„ì‚°ì€ Tesla K10ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

- Tesla K10 Peak Single Precision FLOPS

    = 745 MHz core clock * 2 GPUs/board * (8 multiprocessors * 192 fp32 cores/multiprocessor) * 2ops/cycle = 4.58TFLOPS

- Tesla K10 Peak Memory Bandwidth

    = 2 GPUs/board * 256 bit * 2500 MHz mem-clock * 2 DDR / 8bits/byte = 320 GB/s

- Ratio of instruction:bytes

    = 4.58 TFLOPS / 320 GB/s 

    $\rightarrow$ 13.6 instructions:1 byte

ì¦‰, Tesla K10ì€ byte accessë§ˆë‹¤ instruction 13.6ê°œ ì´ìƒì„ **issue**(íˆ¬ì…) ê°€ëŠ¥í•˜ë‹¤.

---

## 2.11 organizing parallel threads

ì•ì„œ vector additionì—ì„œëŠ” kernelì— ì•Œë§ëŠ” grid, block sizeë¥¼ ì§€ì •í•´ì„œ ê³„ì‚°í–ˆë‹¤. ì´ë•Œ grid sizeëŠ” block sizeì™€ vector sizeì— ì˜í•´ ê²°ì •ë˜ì—ˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ matrix additionì€ ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ? ì¼ë°˜ì ìœ¼ë¡œ 2D gridë‚˜ 2D blockì„ ì´ìš©í•´ì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

- 2D grid with 2D blocks

- 1D grid with 1D blocks

- 2D grid with 1D blocks

ì¼ë°˜ì ìœ¼ë¡œ 2D array í˜•íƒœì˜ data(image pixel ë“±)ë¼ë©´, 2D blockìœ¼ë¡œ êµ¬ì„±ëœ 2D gridë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ í¸ë¦¬í•˜ë‹¤. ê°€ë ¹ 76x62 pixelì´ ìˆë‹¤ê³  í•˜ì. ì´ë¥¼ 16x16 threadë¡œ êµ¬ì„±ëœ blockìœ¼ë¡œ ë‚˜ëˆ„ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

![76x62 pixel](images/76_62_pixel.png)

- íšŒìƒ‰ìœ¼ë¡œ ì¹ í•´ì§„ ë¶€ë¶„ì´ pixel data(76x62)ë¥¼ ì˜ë¯¸í•œë‹¤.

- xì¶•ìœ¼ë¡œëŠ” 5ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

- yì¶•ìœ¼ë¡œëŠ” 4ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

- ì¦‰, ì´ pixel dataë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” 5*4 = 20ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

> ì—¬ì „íˆ data ìˆ˜ë³´ë‹¤ thread ìˆ˜ê°€ ë” ë§ë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì.

pixel dataë¥¼ Pinì´ë¼ê³  í•œë‹¤ë©´, block(1,0)ì— ìˆëŠ” thread(0,0)ì˜ Pin elementëŠ” ë‹¤ìŒê³¼ ê°™ì´ indexí•  ìˆ˜ ìˆë‹¤.

- blockIdx.y * blockDim.y + threadIdx.y = 1 * 16 + 0 = 16

- blockIdx.x * blockDim.x + threadIdx.x = 0 * 16 + 0 = 0

$$ P_{blockIdx.y * blockDim.y + threadIdx.y, \, blockIdx.x * blockDim.x + threadIdx.x} = P_{16,0} $$

---

### 2.11.1 indexing matrices with blocks and threads

![matrix ì˜ˆì‹œ](images/matrix_ex_1.png)

$8 \times 6$ matrixê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì. matrix addition kernelì—ì„œë„ threadëŠ” ì£¼ë¡œ í•˜ë‚˜ì˜ data elementë¥¼ ì²˜ë¦¬í•œë‹¤. matrix ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ ì •í•´ì•¼ í•œë‹¤.

- thread and block index

- coordinate of a given point in the matrix

- offset in linear global memory

ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ì„œ global memoryì— matrixë¥¼ mappingí•  ìˆ˜ ìˆë‹¤.( $8 \times 6$ ì´ë¯€ë¡œ nx = 8, ny = 6ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. )

1. matrix coordinateì— ë§ì¶°ì„œ threadì™€ block indexë¥¼ ì§€ì •í•œë‹¤.

    - ix = threadIdx.x + blockIdx.x * blockDim.x

    - iy = threadIdx.y + blockIdx.y * blockDim.y

2. global matrixì— ì´ë¥¼ mappingí•œë‹¤.

    - idx = iy * nx + ix

![mapping matrix 1](images/mapping_matrix_1.png)

ì°¸ê³ ë¡œ printThreadInfo() functionì„ ì‚¬ìš©í•˜ë©´ ê´€ë ¨ëœ ì—¬ëŸ¬ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.

- thread index

- block index

- matrix coordinate

- global linear memory offset

- value of corresponding elements

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix element printing&nbsp;&nbsp;&nbsp;</span>

ì´ì œ ë‹¤ìŒê³¼ ê°™ì€ $8 \times 6$ matrixì˜ elementë¥¼ ì¶œë ¥í•´ ë³´ë©° indexë¥¼ í™•ì¸í•  ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ 'checkThreadIndex.cu'ì´ë‹¤.

![8x6 matrix indices](images/matrix_ex_indices.png)

- 1 blockì€ $4 \times 2$ threadë¡œ êµ¬ì„±ëœë‹¤.

- gridëŠ” $(8 + 4 - 1)/4 \times (6 + 2 - 1)/2$ blockìœ¼ë¡œ êµ¬ì„±ëœë‹¤.( ì¦‰, $3 \times 2$ )

> ë„ì¤‘ì— cudaDeviceProp()ì„ ì´ìš©í•´ device ì •ë³´ë¥¼ queryí•  ê²ƒì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}

// hostì—ì„œ matrix elementë¥¼ ì±„ìš¸ function
void initialInt(int *ip, int size) {
    for (int i = 0; i < size; i++) {
        ip[i] = i;
    }
}

// hostì—ì„œ matrix elementë¥¼ printí•  function 
void printMatrix(int *C, const int nx, const int ny) {
    int *ic = C;
    printf("\nMatrix: (%d.%d)\n", nx, ny);
    for (int iy = 0; iy < ny; iy++) {
        for (int ix = 0; ix < nx; ix++) {
            printf("%3d", ic[ix]);
        }
        ic += nx;
        printf("\n");
    }
    printf("\n");
}

// deviceì—ì„œ matrix elementë¥¼ printí•  kernel
__global__ void printThreadIndex(int *A, const int nx, const int ny) {
    int ix = threadIdx.x + blockIdx.x * blockDim.x;
    int iy = threadIdx.y + blockIdx.y * blockDim.y;
    unsigned int idx = iy*nx + ix;

    // printf function ì‚¬ìš©ì— ìœ ì˜
    printf("thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) "
        "global index %2d ival %2d\n", threadIdx.x, threadIdx.y, blockIdx.x,
        blockIdx.y, ix, iy, idx, A[idx]);
}

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // get device information
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set matrix dimension
    int nx = 8;
    int ny = 6;
    int nxy = nx*ny;
    int nBytes = nxy * sizeof(float);

    // malloc host memory
    int *h_A;
    h_A = (int *)malloc(nBytes);

    // initialize host matrix with integer
    initialInt(h_A, nxy);
    printMatrix(h_A, nx, ny);

    // malloc device memory
    int *d_MatA;
    cudaMalloc((void **)&d_MatA, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);

    // set up execution configuration
    dim3 block(4, 2);
    dim3 grid((nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y);

    // invoke the kernel
    printThreadIndex<<<grid, block>>>(d_MatA, nx, ny);
    cudaDeviceSynchronize();

    // free host and device memory
    cudaFree(d_MatA);
    free(h_A);

    // reset device
    cudaDeviceReset();

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œ ë’¤ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 checkThreadIndex.cu -o checkIndex
$ ./checkIndex
```

![checkThreadIndex](images/checkindex.png)

---

### 2.11.2 summing matrices with 2D grid and 2d blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(2D grid, 2D blocks)&nbsp;&nbsp;&nbsp;</span>

2D gridì™€ 2D blockìœ¼ë¡œ êµ¬ì„±í•œ kernelë¡œ matrix additionì„ ìˆ˜í–‰í•œë‹¤. host sideì—ì„œë„ ë™ì¼í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•œ ë’¤, kernel ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì œëŒ€ë¡œ ìˆ˜í–‰ì´ ëëŠ”ì§€ í™•ì¸í•  ê²ƒì´ë‹¤.

- matrix sizeëŠ” 16,384 elementë¥¼ ê°–ëŠ”ë‹¤.

    ```c
    int nx = 1<<14;
    int ny = 1<<14;
    ```

- 1 blockì€ (32, 32) threadë¥¼ ê°–ëŠ”ë‹¤.

- 1 gridëŠ” blockì„ (nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y ë§Œí¼ ê°–ëŠ”ë‹¤.(512, 512)

ìš°ì„  hostì˜ matrix addition functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
// host sideì—ì„œ matrix addition ìˆ˜í–‰
void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {
    float *ia = A;
    float *ib = B;
    float *ic = C;

    for (int iy = 0; iy < ny; iy++){
        for (int ix = 0; ix < nx; ix++){
            ic[ix] = ia[ix] + ib[ix];
        }
        ia += nx;
        ib += nx;
        ic += nx;
    }
}
```

deviceì˜ matrix addtion kernelì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
__global__ void sumMatrixOnGPU2D(float *MatA, float *MatB, float *MatC, int nx, int ny){
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
    unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;
    unsigned int idx = iy*nx + ix;

    if (ix < nx && iy < ny){
        MatC[idx] = MatA[idx] + MatB[idx];
    }
}
```

ì•„ë˜ëŠ” matrix additionì„ ìˆ˜í–‰í•˜ëŠ” sumMatrixOnGPU-2D-grid-2D-block.cu codeì´ë‹¤.

```c
// include ìƒëµ
// error handlingì„ ìœ„í•œ CHECK macro ìƒëµ
// hostì™€ deviceì˜ matrix addition function ìƒëµ(sumMatrixOnHost, sumMatrixOnGPU2D)
// CPU timerì¸ CPUsecond() function ìƒëµ
// matrixì— elementë¥¼ ìƒì„±í•˜ëŠ” initialData() function ìƒëµ
// hostì™€ deviceì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” checkResult() function ìƒëµ

int main(int argc, char** argv){
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set up data size of matrix
    int nx = 1<<14;
    int ny = 1<<14;

    int nxy = nx*ny;
    int nBytes = nxy * sizeof(float);
    printf("Matrix size: nx %d ny %d\n", nx, ny);

    // malloc host memory
    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef = (float *)malloc(nBytes);

    // initialize data at host side
    double iStart = cpuSecond();
    initialData(h_A, nxy);
    initialData(h_B, nxy);
    double iElaps = cpuSecond() - iStart;

    memset(hostRef, 0, nBytes);
    memset(gpuRef, 0, nBytes);

    // add matrix at host side for result checks
    iStart = cpuSecond();
    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);
    iElaps = cpuSecond() - iStart;

    // malloc device global memory
    float *d_MatA, *d_MatB, *d_MatC;
    cudaMalloc((void **)&d_MatA, nBytes);
    cudaMalloc((void **)&d_MatB, nBytes);
    cudaMalloc((void **)&d_MatC, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    int dimx = 32;
    int dimy = 32;
    dim3 block(dimx, dimy);
    dim3 grid ((nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y);

    iStart = cpuSecond();
    sumMatrixOnGPU2D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
    cudaDeviceSynchronize();
    iElaps = cpuSecond() - iStart;
    printf("sumMatrixOnGPU<<<(%d,%d), (%d,%d)>>> elapsed %f sec\n",
        grid.x, grid.y, block.x, block.y, iElaps);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost);

    // check device results
    checkResult(hostRef, gpuRef, nxy);

    // free device global memory
    cudaFree(d_MatA);
    cudaFree(d_MatB);
    cudaFree(d_MatC);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    // reset device
    cudaDeviceReset();

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œ ë’¤ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-2D-grid-2D-block.cu -o matrix2D
$ ./matrix2D
```

![matrix2D 1](images/matrix2D.png)

block dimensionì„ 32x16ìœ¼ë¡œ í•œ ë’¤(blockì€ 512x1024ê°€ ëœë‹¤.). recompileí•´ì„œ ì‹¤í–‰í•˜ë©´ ì‹œê°„ì€ ì•½ 1/2ë°° ì •ë„ë¡œ ì¤„ì–´ë“ ë‹¤. ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë„ parallelismì´ ë‘ ë°° ëŠ˜ì—ˆê¸° ë–„ë¬¸ì— ì‹œê°„ì´ ì¤„ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. 

> ìœ„ ì„œìˆ ì€ Tesla device ê¸°ì¤€ìœ¼ë¡œ ì¤„ì–´ë“  ì‹œê°„ì´ë‹¤. 0.060323 secì—ì„œ 0.038041 secë¡œ ì¤„ì–´ë“ ë‹¤.

![matrix2D 2](images/matrix2D_2.png)

ê·¸ëŸ¬ë‚˜ 16x16ìœ¼ë¡œ block dimensionì„ ì§€ì •í•œ ë’¤(blockì€ 1024x1024ê°€ ëœë‹¤.), recompileí•˜ê³  ì‹¤í–‰í•˜ë©´ ì˜¤íˆë ¤ ì‹œê°„ì´ ë” ëŠ˜ì–´ë‚œë‹¤. ì²˜ìŒê³¼ ë¹„êµí•˜ë©´ blockì´ 4ë°°ê°€ ë˜ë©° parallelismì´ ëŠ˜ì—ˆëŠ”ë° ì–´ì§¸ì„œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì¼ê¹Œ? (ch03 ì°¸ì¡°)

![matrix2D 3](images/matrix2D_3.png)

> Fermi deviceì—ì„œ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

| kernel configuration | kernel elapsed time | block number |
| --- | --- | --- |
| (32,32) | 0.060323 sec | 512x512 |
| (32,16) | 0.038041 sec | 512x1024 |
| (16,16) | 0.045535 sec | 1024x1024 |

---

### 2.11.3 summing matrices with a 1D grid and 1D blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(1D grid, 1D blocks)&nbsp;&nbsp;&nbsp;</span>

ì•ì„  2D grid, 2D blocks ì˜ˆì œë¥¼ 1D grid, 1D blocksë¡œ ë³€í˜•í•´ì„œ matrix additionì„ ìˆ˜í–‰í•œë‹¤.(íŒŒì¼ëª…ì€ sumMatrixOnGPU-1D-grid-1D-block.cu)

![mapping matrix 1D grid, 1D blocks](images/mapping_matrix_2.png)

ì´ì™€ ê°™ì´ 1D grid, 1D blockìœ¼ë¡œ êµ¬ì„±í•œë‹¤ë©´, kernelì˜ indexë¥¼ ë‹¤ë¥´ê²Œ êµ¬ì„±í•´ì•¼ í•œë‹¤.(1D blockì€ ì˜¤ì§ threadIdx.xë§Œ index ê³„ì‚°ì— ì“¸ ìˆ˜ ìˆê²Œ ëœë‹¤.)

```c
__global__ void sumMatrixOnGPU1D(float *MatA, float *MatB, float *MatC,
    int nx, int ny) {
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
        
    if (ix < nx) {
        for (int iy  = 0; iy < ny; iy++) {
            int idx = iy*nx + ix;
            MatC[idx] = MatA[idx] + MatB[idx];
        }
    }
}
```

ë‹¤ìŒê³¼ ê°™ì´ block configurationì„ ì„¤ì •í•œë‹¤.

```c
dim3 block(32,1);
dim3 grid ((nx + block.x - 1)/block.x,1);
```

- 1 blockì€ 32ê°œ threadë¥¼ ê°–ëŠ”ë‹¤.

- 1 gridëŠ” (nx + block.x - 1)/block.xê°œì˜ blockì„ ê°–ëŠ”ë‹¤.

kernel ëª…ì¹­ì´ ë°”ë€ ê²ƒì— ì£¼ì˜í•œë‹¤.

```c
sumMatrixOnGPU1D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
```

ì‘ì„±ì„ ì™„ë£Œí•˜ë©´ compileí•´ì„œ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-1D-grid-1D-block.cu -o matrix1D
$ ./matrix1D
```

![matrix1D](images/matrix1D.png)

ì‚¬ì‹¤ ê²°ê³¼ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” 2D grid, 2D block (32x32)ê³¼ êµ¬ì¡°ì ìœ¼ë¡œ ì°¨ì´ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ ì—°ì‚° ì‹œê°„ì—ì„œ ì°¨ì´ë¥¼ ë³´ì¸ë‹¤.

block(128,1)ë¡œ ë³€ê²½í•´ì„œ ìˆ˜í–‰ì„ í•˜ë©´ ì¡°ê¸ˆ ë” ë¹¨ë¼ì§€ëŠ” ëª¨ìŠµì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤.

```
dim3 block(128,1);
dim3 grid ((nx + block.x - 1)/block.x,1);
```

![matrix1D 2](images/matrix1D_2.png)

> Tesla device ê¸°ì¤€ìœ¼ë¡œ 0.061352 secì—ì„œ 0.044701 secë¡œ ë¹¨ë¼ì§„ë‹¤.

---

### 2.11.4 summing matrices with a 2D grid and 1D blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(2D grid, 1D blocks)&nbsp;&nbsp;&nbsp;</span>

ì•ì„  2D grid, 2D blocks ì˜ˆì œë¥¼ 2D grid, 1D blocksë¡œ ë³€í˜•í•´ì„œ matrix additionì„ ìˆ˜í–‰í•œë‹¤.(íŒŒì¼ëª…ì€ sumMatrixOnGPU-2D-grid-1D-block.cu)

![mapping matrix 2D grid, 1D blocks](images/mapping_matrix_3.png)

ì´ ê²½ìš° matrix coordinate mapping(ix, iy)ì´ ë‹¤ìŒê³¼ ê°™ì´ ë°”ë€ë‹¤.

```c
ix = threadIdx.x + blockIdx.x * blockDim.x;
iy = blockIdx.y;
```

ë‹¤ìŒê³¼ ê°™ì´ blockê³¼ grid sizeë¥¼ ì§€ì •í•œë‹¤.

```c
dim3 block(32);
dim3 grid((nx + block.x - 1)/block.x,ny);
```

global linear memory offsetì€ ë§ˆì°¬ê°€ì§€ë‹¤. ë³€ê²½ ì‚¬í•­ì„ ë°˜ì˜í•œ kernelì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

> ì‚¬ì‹¤ ì•„ë˜ì™€ ê°™ì´ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ sumMatrixOnGPU2Dë¡œ ë™ì‘ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ëŒ€ì‹  ë°”ê¾¼ kernelì´ threadë§ˆë‹¤ integer multiplication ì—°ì‚° 1ê°œ, integer addition ì—°ì‚° 1ê°œë¥¼ ëœ ìˆ˜í–‰í•˜ëŠ” ì´ì ì„ ê°–ëŠ”ë‹¤.

```c
__global__ void sumMatrixOnGPUMix(float *MatA, float *MatB, float *MatC,
    int nx, int ny) {
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
    unsigned int iy = blockIdx.y;
    unsigned int idx = iy*nx + ix;

    if (ix < nx && iy < ny){
        MatC[idx] = MatA[idx] + MatB[idx];
    }
}
```

kernel invokeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
sumMatrixInGPUMix<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
```

compileí•˜ê³  ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-2D-grid-1D-block.cu -o mat2D1D
$ ./mat2D1D
```

![mat2D1D](images/mat2D1D.png)

ì°¸ê³ ë¡œ block sizeë¥¼ 256ìœ¼ë¡œ ëŠ˜ë¦¬ë©´ ì‹œê°„ì´ ë” ê°ì†Œí•œë‹¤. 

![mat2D1D 2](images/mat2D1D_2.png)

> Tesla ê¸°ì¤€ìœ¼ë¡œ 0.073727 secì—ì„œ 0.030765 secê°€ ëœë‹¤.

<br/>

ì•„ë˜ëŠ” Fermi deviceì—ì„œ grid, block dimensionì„ ë°”ê¾¸ë©° ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ì •ë¦¬í•œ í‘œë‹¤.

| kernel | execution comfigure | time elapsed |
| --- | --- | --- |
| sumMatrixOnGPU2D | (512, 1024), (32,16) | 0.038041 |
| sumMatrixOnGPU1D | (128, 1), (128,1) | 0.044701 |
| sumMatrixOnGPUMix | (64, 16384), (256,1) | 0.030765 |

ì´ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. execution configurationì„ ë°”ê¾¸ë©´ performanceì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

2. ì¼ë°˜ì ìœ¼ë¡œ ìµœì ì˜ configurationì„ ì°¾ì§€ ì•Šìœ¼ë©´ best performanceì„ ì–»ì„ ìˆ˜ ì—†ë‹¤.

3. gridì™€ block dimensionì„ ë°”ê¾¸ëŠ” ê²ƒìœ¼ë¡œë„ performanceë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

---