# 2 CUDA Programming Model

ì´ë²ˆ ì¥ì€ vector addition, matrix addition ì˜ˆì œë¥¼ CUDA programìœ¼ë¡œ ì‘ì„±í•˜ë©° ì‚´í´ë³¼ ê²ƒì´ë‹¤.

---

## 2.1 CUDA programming modelì´ë€?

programming modelì€ applicationì´ hardwareì—ì„œ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” computer architectureì„ abstractioní•œ í˜•íƒœì— í•´ë‹¹ëœë‹¤.

> programming languageë‚˜ programming environmentë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ programê³¼ programming model êµ¬í˜„ì— ìˆì–´ì„œì˜ abstractionì„ ê³„ì¸µ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

> CUDA programming modelì€ GPU architectureì˜ memory hierarchyì˜ abstractionì„ ë“œëŸ¬ë‚¸ë‹¤.

![abstraction layer](images/programming_model_layer.png)

ë˜í•œ ë‹¤ë¥¸ parallel programming modelì—ì„œ abstractionë“¤ì„ shareí•˜ê¸° ìœ„í•´ì„œ, CUDA programming modelì€ ë‹¤ìŒê³¼ ê°™ì´ GPUë¥¼ ì œì–´í•˜ëŠ” íŠ¹ì§•ë“¤ì„ ê°–ëŠ”ë‹¤.

- organize threads on the GPU through a hierarchy structure

- access memory on the GPU through a hierarchy structure

í”„ë¡œê·¸ë˜ë¨¸ ê´€ì ì—ì„œëŠ” parallel computationì„ ë‹¤ë¥¸ levelì—ì„œ ë³¸ë‹¤. 

- domain level

programê³¼ algorithmì„ ë””ìì¸í•  ë•Œ í•´ë‹¹í•˜ëŠ” levelì´ë‹¤. ì–´ë–»ê²Œ dataì™€ functionì„ **decompose**(ë¶„í•´)í•´ì•¼ ì˜¬ë°”ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ programì„ parallelí•  ìˆ˜ ìˆì„ì§€ ê³ ë¯¼í•œë‹¤.

- logic level

programê³¼ algorithm ë””ìì¸ì´ ëë‚˜ë©´ programming ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤. ì–´ë–»ê²Œ êµ¬ì„±í•´ì•¼ logicì„ concurrent threadë“¤ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•œë‹¤. 

- hardware level

threadë“¤ì„ íš¨ìœ¨ ì¢‹ê²Œ coreë¡œ mappingí•˜ëŠ” ì—¬ë¶€ ë“±ì„ ê³ ë ¤í•œë‹¤.

---

## 2.2 CUDA Programming Structure

> [CUDA ê¸°ì´ˆ](https://velog.io/@lunarainproject/CUDA-%EA%B8%B0%EC%B4%88)

- ì±…(2014 ë°œê°„)ì—ì„œëŠ” CUDA 6ìœ¼ë¡œ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

- host(CPU) memoryëŠ” variable ì´ë¦„ ì•ì— h_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

- device(GPU) memoryëŠ” variable ì´ë¦„ ì•ì— d_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œ CPUì™€ GPU ì‚¬ì´ì— ê³µìœ ë˜ëŠ” managed memory poolì„ í˜•ì„±í•˜ëŠ” **unified memory**ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ë•ë¶„ì— CPUì™€ GPU memory ëª¨ë‘ ë‹¨ì¼ pointerë¥¼ ì‚¬ìš©í•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. unified memoryì— allocateëœ dataë¥¼ hostì™€ device ì‚¬ì´ì— ìë™ìœ¼ë¡œ **migrate**í•œë‹¤.

CUDAì˜ í•µì‹¬ì€ kernelì´ë‹¤. CUDAëŠ” GPU threadì—ì„œ ë™ì‘í•˜ëŠ” kernelë“¤ì„ schedulingí•œë‹¤. 

![serial code execute](images/execute_on_CPU_GPU.png)

hostëŠ” ëŒ€ë¶€ë¶„ì˜ operationì—ì„œ deviceì™€ independentí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤. kernelì´ **launch**(êµ¬ë™)ì„ ì‹œì‘í•˜ë©´, hostëŠ” data parallel codeë¥¼ GPUì—ì„œ ì‘ë™í•˜ê²Œ ë§Œë“œëŠ” additional taskì—ì„œ ë²—ì–´ë‚˜ ì¦‰ì‹œ control ì‘ì—…ìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ë‹¤ì‹œ ë§í•´ kernelì€ **asynchronous**(ë¹„ë™ê¸°ì )ìœ¼ë¡œ launchëœë‹¤. hostëŠ” kernel launchê°€ ì™„ë£Œë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

> CUDA runtimeì—ì„œ ì œê³µí•˜ëŠ” cudaDeviceSynchronizeë¥¼ ì´ìš©í•´ì„œ CPUê°€ device codeì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.

---

## 2.3 managing memory

CUDA runtimeì€ device memoryë¥¼ allocateí•˜ëŠ” functionë“¤ì„ ì œê³µí•œë‹¤.

| í‘œì¤€ C function | CUDA C function |
| --- | --- |
| malloc | cudaMalloc |
| memcpy | cudaMemcpy |
| memset | cudaMemset |
| free | cudaFree |

ìš°ì„  GPU memory allocationì„ ìœ„í•œ cudaMallocì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤. ë‘ ê°€ì§€ parameterê°€ í•„ìš”í•˜ë‹¤.

```c
cudaError_t cudaMalloc ( void** devPtr, size_t size )
```

- cudaError_të¥¼ ì…ë ¥í•˜ë©´ error ë°œìƒ ì‹œ ì´ìœ ë¥¼ ì¶œë ¥í•´ ì¤€ë‹¤.

- allocateí•œ memoryëŠ” í•´ë‹¹ device memory addressë¥¼ ê°€ë¦¬í‚¤ëŠ” pointerì¸ devPtrì„ í†µí•´ returnëœë‹¤.

hostì™€ device ì‚¬ì´ì— dataë¥¼ transferí•˜ê¸° ìœ„í•œ functionìœ¼ë¡œëŠ” cudaMemcpyë¥¼ ì‚¬ìš©í•œë‹¤. unified memoryì— ì¡´ì¬í•˜ëŠ” dataê°€ ì•„ë‹ˆë¼ë©´, dataë¥¼ ì‚¬ì „ì— device memoryë¡œ transferí•´ì•¼ í•œë‹¤.

```c
cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )
```

- dst: destination memory address pointer

- src: source memory address pointer

- count: copyí•  byte size

- kind: data transfer type

  - cudaMemcpyHostToHost

  - cudaMemcpyHostToDevice

  - cudaMemcpyDeviceToHost

  - cudaMemcpyDeviceToDevice

<U>cudaMemcpyëŠ” synchronous behavior</U>ì´ë‹¤. host applicationì€ cudaMemcpyì˜ return/transferê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ë©ˆì¶˜ë‹¤.

ì°¸ê³ ë¡œ kernel launchë¥¼ ì œì™¸í•œ ëª¨ë“  CUDA callì€, enumerated type cudaError_tìœ¼ë¡œ error codeë¥¼ returní•œë‹¤. 

- ë§Œì•½ GPU memoryì— ì„±ê³µì ìœ¼ë¡œ allocateí–ˆë‹¤ë©´, 'cudaSuccess'ë¥¼ returní•œë‹¤.

- ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ cudaErrorMemoryAllocationì„ returní•œë‹¤.

ë‹¤ìŒ functionì„ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ error messageë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.(Cì˜ strerror functionê³¼ ë¹„ìŠ·í•˜ë‹¤.)

```c
char* cudaGetErrorString(cudaError_t error)
```

![GPU memory hierarchy](images/GPU_memory_hierarchy.png)

ì´ì œ ì˜ˆì œë¥¼ ë³´ë©° hostì™€ deviceê°„ì˜ data movement ê´€ë¦¬ë¥¼ ì‚´í´ë³´ì. 

![array summation](images/array_summation.png)

ìš°ì„  ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ array ì—°ì‚°(host-based array summation)ì„ Cë¥¼ ì‚¬ìš©í•´ì„œ êµ¬í˜„í•œë‹¤. íŒŒì¼ëª…ì€ sumArraysOnHost.cì´ë‹¤.

```c
#include <stdlib.h>
#include <string.h>
#include <time.h>

// Hostì—ì„œ array sum ìˆ˜í–‰
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    for (int idx=0; idx<N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

// arrayì— random numberë¡œ ì´ˆê¸°ê°’ì„ ì„¤ì •
void initialData(float *ip, int size) {
    // random number ìƒì„±
    time_t t;
    srand((unsigned int) time(&t));

    for (int i=0; i<size; i++) {
        ip[i] = (float) ( rand() & 0xFF )/10.0f;
    }
}

int main(int argc, char **argv) {
    int nElem = 1024;
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *h_C;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    h_C = (float *)malloc(nBytes);

    initialData(h_A, nElem);
    initialData(h_B, nElem);

    sumArraysOnHost(h_A, h_B, h_C, nElem);

    free(h_A);
    free(h_B);
    free(h_C);

    return(0);
}
```

pure C programì´ë¯€ë¡œ C compilerë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ nvcc compilerë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
$ nvcc -Xcompiler -std=c99 sumArraysOnHost.c -o sum
$ ./sum
```

ì°¸ê³ ë¡œ ìœ„ compile ëª…ë ¹ì˜ **flag**(ì˜µì…˜)ì€ ë‹¤ìŒ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.(ìì„¸íˆëŠ” CUDA compiler documentë¥¼ ì‚´í”¼ë©´ ì•Œ ìˆ˜ ìˆë‹¤.)

- -Xcompiler: ë°”ë¡œ C compiler ë˜ëŠ” preprocessorë¡œ ì²˜ë¦¬í•œë‹¤.

- -std=c99: code styleì´ c99 standardì„ì„ ì•Œë¦°ë‹¤.

ì´ ì—°ì‚°ì„ GPUì—ì„œ êµ¬ë™í•˜ê²Œë” ë°”ê¾¸ëŠ” ê²ƒì€ ì‰½ë‹¤. 

1. GPU memory allocation

```c
float *d_A, *d_B, *d_C;
cudaMalloc((float**) &d_A, nBytes);
cudaMalloc((float**) &d_B, nBytes);
cudaMalloc((float**) &d_B, nBytes);
```

2. host memoryì—ì„œ GPU global memoryë¡œ dataë¥¼ transferí•œë‹¤.

```c
cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);
```

3. kernel ìˆ˜í–‰.

GPU global memoryë¡œ transferí•˜ëŠ” ê³¼ì •ì´ ëë‚˜ë©´, ì´ì œë¶€í„°ëŠ” host sideì—ì„œ GPUì—ì„œ array summationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ kernel functionì„ invokeë  ìˆ˜ ìˆë‹¤. ë§Œì•½ kernelì´ callë˜ë©´ controlì€ ì¦‰ì‹œ hostë¡œ return backë˜ë©°, GPUê°€ kernelì„ ìˆ˜í–‰í•˜ëŠ” ì‚¬ì´ì— ë‹¤ë¥¸ functionì„ ìˆ˜í–‰í•œë‹¤.(asynchronous ë™ì‘)

4. resultë¥¼ host memoryë¡œ copyí•œë‹¤.

kernel ì‘ì—…ì´ ëª¨ë‘ ëë‚˜ë©´, result(array d_C)ëŠ” GPU global memoryì— ì €ì¥ëœë‹¤. ì´ì œ ì´ resultë¥¼ host array(gpuRef)ë¡œ copyí•´ì•¼ í•œë‹¤.

```c
cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);
```

> ë§Œì•½ ì´ë ‡ê²Œ copyë¥¼ ì§„í–‰í•˜ì§€ ì•Šê³  'gpuRef = d_C'ì™€ ê°™ì€ ì˜ëª»ëœ assignmentë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ê²Œ ë˜ë©´ applicationì€ runtimeì— crashëœë‹¤.

> ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ unified memoryê°€ CUDA 6ë¶€í„° ì œê³µëë‹¤. CPUì™€ GPU memory ëª¨ë‘ single pointerë¥¼ ì‚¬ìš©í•œë‹¤.

cudaMemcpyì— ì˜í•´ hostëŠ” copy ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ë©ˆì¶”ê²Œ ëœë‹¤. 

5. memoryë¥¼ releaseí•œë‹¤.

```c
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

---

## 2.4 organizing threads

> [thread block architecture](https://tododiary.tistory.com/57)

host sideì—ì„œ kernel functionì´ launchë˜ë©´, deviceë¡œ executionì´ ë„˜ì–´ê°€ë©° kernel functionì—ì„œ ì •ì˜í•œ threadë“¤ì—ì„œ ëª…ë ¹ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ì—ˆë‹¤. ì´ë•Œ 'threadë“¤ì„ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠ”ê°€'ë¼ëŠ” ë¬¸ì œê°€ CUDA programmingì— ìˆì–´ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì´ë‹¤.

threadëŠ” 2-level hierarcyë¡œ êµ¬ì„±ëœë‹¤. í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ blockìœ¼ë¡œ êµ¬ì„±ë˜ê³ , ê° blockì€ í•˜ë‚˜ ì´ìƒì˜ threadë¡œ êµ¬ì„±ëœë‹¤.

![thread hierarchy](images/thread_hierarchy.png)

- í•˜ë‚˜ì˜ kernel launchë¡œ ìƒì„±ëœ ëª¨ë“  threadë¥¼ **grid**ë¼ê³  í†µì¹­í•œë‹¤.

  - grid ë‚´ ëª¨ë“  threadëŠ” ê°™ì€ global memory spaceë¥¼ shareí•œë‹¤.

  - ì´ threadë“¤ì´ ëª¨ë‘ ë™ì¼í•œ kernel codeë¥¼ ì‹¤í–‰í•œë‹¤.

- í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ thread **block**ë“¤ë¡œ êµ¬ì„±ëœë‹¤. ê° thread blockëŠ” ë‹¤ìŒ íŠ¹ì§•ì„ ê°€ì§€ê³  cooperateí•  ìˆ˜ ìˆë‹¤.

  - block-local synchronization

  - block-local shared memory

  > <U>ë‹¤ë¥¸ blockì˜ threadë¼ë¦¬ëŠ” cooperateí•  ìˆ˜ ì—†ë‹¤.</U>

> blockì€ threadë¥¼ ìµœëŒ€ 512ê°œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ blockì´ ê°€ì§€ëŠ” thread ê°œìˆ˜ëŠ” 32(ë˜ëŠ” NVIDIAëŠ” 64ë¥¼ ê¶Œì¥) ë°°ìˆ˜ë¡œ ì§€ì •í•˜ëŠ” í¸ì´ ì¢‹ë‹¤.(SMì´ 32ë°°ìˆ˜ ë‹¨ìœ„ë¡œ ë™ì‘)

threadë“¤ì€ ë‹¤ìŒ indexë¥¼ í†µí•´ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° index(coordinate variables)ëŠ” CUDA runtimeì— ì˜í•´ ê° threadë³„ë¡œ í• ë‹¹ëœë‹¤.

![blockIdx, threadIdx](images/blockIdx_threadIdx.png)

- blockIdx: grid ë‚´ë¶€ì—ì„œì˜ block index

- threadIdx: block ë‚´ë¶€ì—ì„œì˜ thread index

> coordinate variableì€ uint3 typeì„ ê°–ëŠ”ë‹¤. ì´ structureì€ 3ê°œì˜ unsigned integerë¡œ êµ¬ì„±ë˜ë©°, ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ componentëŠ” x, y, zë¥¼ ë¶™ì—¬ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```c
blockIdx.x
blockIdx.y
blockIdx.z

// block ë‚´ ëª¨ë“  threadëŠ” ë™ì¼í•œ blockIdxë¥¼ ê³µìœ í•œë‹¤.
threadIdx.x
threadIdx.y
threadIdx.z
```

kernel launch êµ¬ë¬¸ì—ì„œ execution configutation parameters(<<<...>>>)ë¡œ gridì™€ ê° blockì˜ dimensionì„ ì§€ì •í–ˆë‹¤. gridëŠ” ì£¼ë¡œ blockì˜ 2D array, blockì€ ì£¼ë¡œ threadì˜ 3D arrayë¡œ êµ¬ì„±ëœë‹¤. ì´ëŸ° gridì™€ blockì˜ dimentionì€ ë‹¤ìŒ built-in variableë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- blockDim

- gridDim

> ì´ variableë“¤ì€ dim3 typeì´ë©°, uint3ì— ê¸°ë°˜í•œ dimensionì— íŠ¹í™”ëœ integer vector typeì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ blockDim.x, blockDim.y, blockDim.zë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

> ì‚¬ìš©í•˜ì§€ ì•Šì€ ì°¨ì›ì˜ í¬ê¸°ëŠ” 1ë¡œ ì§€ì •í•œë‹¤.(ê°’ì„ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ defaultë¡œ 1ì´ ë˜ì–´ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ëœë‹¤.)

![ì „ì—­ ì¸ë±ìŠ¤](images/CUDA_grid_execution.png)

ì´ëŸ° variableì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ gridì—ì„œ ìœ ì¼í•œ global indexë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ë‹¤ìŒì´ í•´ë‹¹ëœë‹¤. 

- i = blockIdx.x * blockDim.x + threadIdx.x

> blockì´ 1Dì´ë©°, threadê°€ 256ê°œë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ block 0ì˜ threadì—ì„œ iì˜ ë²”ìœ„ëŠ” 0~255 / block 1ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 256~511 / block 2ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 512~767... ì‹ìœ¼ë¡œ ë°°ì •ëœë‹¤.

> ì´ëŸ° ë°©ë²•ìœ¼ë¡œ index ië¥¼ ì´ìš©í•˜ì—¬ vector A, B, C ê°’ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

ì´ëŸ° ë°©ì‹ ë•ë¶„ì— kernel functionì€ loopê°€ ì—†ë‹¤. ëŒ€ì‹  loopë¥¼ gridë¡œ ëŒ€ì²´í•˜ê³ , ê° threadê°€ iteration í•˜ë‚˜ì— ëŒ€ì‘ë˜ë©° ì—°ì‚°í•œë‹¤. ì´ëŸ° ì¢…ë¥˜ì˜ data parallelismì„ **loop parallelism**ì´ë¼ê³  ì§€ì¹­í•œë‹¤.

ì£¼ì˜í•  ì ì€ element ê°œìˆ˜(vector size)ê°€ block sizeì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ elementê°€ ì´ 100ê°œë¼ë©´, íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ê°€ì¥ ì‘ì€ thread ì°¨ì›ì€ 32ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ blockì€ ì´ 4ê°œê°€ ìƒê¸°ê³ , ì´ 128ê°œì˜ threadë¥¼ ê°€ì§€ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ë ‡ê²Œ êµ¬ì„±í•  ê²½ìš° 28ê°œì˜ threadëŠ” ë¹„í™œì„±í™”í•´ì•¼(ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ì•¼) í•œë‹¤. 

> block ê°œìˆ˜ë¥¼ êµ¬í•˜ëŠ” ì‹ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ vector sizeê°€ 1000ì´ê³  ê° blockì´ 256ê°œì˜ threadë¥¼ ê°€ì§„ë‹¤ë©´, (1000 + 256 - 1) / 256 = 4ë¡œ 4ê°œì˜ blockì„ ìƒì„±í•˜ê²Œ ëœë‹¤. ì´ ê²½ìš° ê²°ê³¼ì ìœ¼ë¡œëŠ” 256*4 = 1024 threadê°€ ì‹¤í–‰ë˜ê³ , ë‚˜ë¨¸ì§€ 24ê°œëŠ” ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•œë‹¤.

> ë¹„ìŠ·í•˜ê²Œ(1D block) gridëŠ” (nElem + block.x - 1)/block.x) ê°œê°€ ëœë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: gridì™€ block dimension êµ¬í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

hostì™€ device ì–‘ìª½ì—ì„œ gridì™€ blockì˜ dimensionì„ ì²´í¬í•  ê²ƒì´ë‹¤.

deviceì—ì„œëŠ” kernel functionì„ ë§Œë“¤ì–´ì„œ, ê°ìì˜ thread index, block index, grid dimensionì„ ì¶œë ¥í•œë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void checkIndex(void) {
    printf("threadIdx: (%d, %d, %d) blockIdx: (%d, %d, %d) blockDim: (%d, %d, %d) "
        "gridDim: (%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z,
        blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z,
        gridDim.x, gridDim.y, gridDim.z);
}

int main(int argc, char **argv) {
    // number of elements
    int nElem = 6;

    // grid, block structure ì •ì˜
    // ì§€ì •í•˜ì§€ ì•Šì€ ì°¨ì›ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒ(1)ìœ¼ë¡œ ì²˜ë¦¬

    // 3ê°œì˜ threadë¥¼ í¬í•¨í•˜ëŠ” 1D block
    dim3 block (3);
    // grid ê°œìˆ˜
    dim3 grid  ((nElem + block.x - 1)/block.x);

    // check grid and block dimension from host side
    printf("grid.x %d grid.y %d grid.z %d\n", grid.x, grid.y, grid.z);
    printf("block.x %d block.y %d block.z %d\n", block.x, block.y, block.z);

    // check grid and block dimension from device side
    checkIndex <<<grid, block>>> ();

    // reset device
    cudaDeviceReset();

    return(0);
}
```

CUDAì—ì„œ printf functionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” compileë˜ëŠ” í™˜ê²½ì˜ GPU architectureë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤.

```bash
nvcc -arch=sm_80 checkDimension.cu -o check
./check
```

> ì±…ì€ Fermi GPUì´ë¯€ë¡œ -arch=sm_20ì„ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤.([CUDA -arch í™•ì¸](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/))(FermiëŠ” CUDA ìƒìœ„ ë²„ì „ì—ì„œ deprecatedëë‹¤.) 

> í˜„ì¬ ì‹¤ìŠµ ì¤‘ì¸ í™˜ê²½ì€ RTX 3080 Tië¡œ Ampere architectureì´ë‹¤.(sm_86)($ nvidia-smi -q ëª…ë ¹ìœ¼ë¡œ í™•ì¸)([GPU í™•ì¸](https://kyumdoctor.tistory.com/72))

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![grid block indices and dimensions](images/grid_block_indice.png)

ì´ ì˜ˆì œì—ì„œ ëª…ì‹¬í•  ì ì€ gridì™€ block variableì˜ ì ‘ê·¼ì—ì„œ, hostì™€ deviceê°€ êµ¬ë¶„ëœë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤.

- host: hostì—ì„œ 'block'ìœ¼ë¡œ declarationí•œ ë’¤, block.x, block.y, block.zì„ ì‚¬ìš©.

- device: built-in block size variableì¸ blockDim.x, blockDim.y, blockDim.zë¥¼ ì‚¬ìš©.

ì´ëŠ” kernel launchë³´ë‹¤ ì•ì„œ hostì—ì„œ gridì™€ block variableì„ ì •ì˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì •ë¦¬í•˜ìë©´ data sizeê°€ ì£¼ì–´ì¡Œì„ ë•Œ, gridì™€ block dimensionì€ ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ ì •í•œë‹¤.

1. block sizeë¥¼ ê²°ì •

2. grid dimensionì„ data sizeì™€ block sizeë¥¼ ì´ìš©í•œ ê³„ì‚°ì„ í†µí•´ ê²°ì •

> block dimensionì„ ì •í•  ë•ŒëŠ” kernelì˜ performanceì ì¸ íŠ¹ì§•ê³¼, GPU resourceì˜ limitationì„ ì•Œì•„ì•¼ í•œë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: hostì—ì„œ grid, block dimension ì •ì˜í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

ì•„ë˜ ì˜ˆì œë¥¼ í†µí•´ 1D gridì™€ 1D blockì„ hostì—ì„œ ì •ì˜í•  ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ defineGridBlock.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

int main(int argc, char **argv) {
    // define total data elements
    int nElem = 1024;

    // define grid and block structure
    dim3 block (1024);
    dim3 grid  ((nElem + block.x - 1)/block.x);
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 512;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 256;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);
    
    // reset block
    block.x = 128;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset device
    cudaDeviceReset();
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œë‹¤.

```bash
nvcc defineGridBlock.cu -o block
./block 
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![defineGridBlock](images/defineGridBlock.png)

---

## 2.5 launching a CUDA kernel

ì•ì„œ CUDA kernel callì€ ë‹¤ìŒê³¼ ê°™ì´ ëª…ë ¹í–ˆë‹¤.

```c
kernel_name <<<grid, block>>>(argument list);
```

ë‹¤ìŒ ë‘ ì˜ˆì‹œë¥¼ ë³´ì.

- kernel_name<<<1, 32>>>(argument list): 32ê°œì˜ elementë¥¼ í•œ blockì— ë‹´ëŠ”ë‹¤.

- kernel_name<<<32, 1>>>(argument list): 32ê°œì˜ blockì€ ëª¨ë‘ element í•˜ë‚˜ë§Œì„ ê°–ëŠ”ë‹¤.

ë˜í•œ kernelì€ type qualifierë¡œ \_\_global\_\_ì„ ë¶™ì—¬ì„œ declarationí–ˆë‹¤.

```c
__global__ void kernel_name(argument list);
```

ì´ë•Œ <U>kernel functionì€ ê¼­ void return type</U>ì´ì–´ì•¼ í•œë‹¤. 

ì•„ë˜ëŠ” CUDA C programmingì—ì„œì˜ type qualifierë¥¼ ì •ë¦¬í•œ í‘œì´ë‹¤.

| qualifer | execution | callable | ì„¤ëª… |
| :---: | :---: | :---: | :---: |
| \_\_global\_\_ | device | hostì—ì„œ callable<br/>NVIDIAê°€ ì œì‹œí•˜ëŠ” compute capabilityê°€ 3 ì´ìƒì¸ device | void return typeì´ì–´ì•¼ í•œë‹¤. |
| \_\_device\_\_ | device | device only | |
| \_\_host\_\_ | host | host only | ìƒëµí•´ë„ ë¬´ë°©í•˜ë‹¤. |

> ì°¸ê³ ë¡œ functionì´ hostì™€ device ì–‘ìª½ì—ì„œ compileëœë‹¤ë©´, \_\_device\_\_ì™€ \_\_host\_\_ qualifierë¥¼ ê°™ì´ ì¨ë„ ëœë‹¤.

ë˜í•œ CUDA kernelì˜ ì œì•½ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- device memoryë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤.

- void return typeë§Œ ê°€ëŠ¥í•˜ë‹¤.

- ìœ ë™ì ì¸ ìˆ«ìì˜ variableì„ argumentë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

- static variableì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- function pointerë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- asynchronousí•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.(ë”°ë¼ì„œ errorë¥¼ ì•Œê¸° í˜ë“¤ë‹¤.)

ë¬´ì—‡ë³´ë‹¤ë„ ë‘ ë²¡í„°ë¥¼ ë”í•˜ëŠ” ì—°ì‚°(A+B = C) functionì„ Cì™€ CUDAë¡œ êµ¬í˜„í–ˆì„ ë•Œì˜ ì°¨ì´ë¥¼ ë³´ë©´ ì œì¼ ê·¹ëª…í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.

ì•„ë˜ëŠ” C codeì´ë‹¤.

```c
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
  for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
  }
}
```

ë‹¤ìŒì€ CUDA codeì´ë‹¤. loopê°€ ì‚¬ë¼ì§€ê³  built-in thread coordinate variableì´ array indexë¥¼ ëŒ€ì‹ í–ˆë‹¤. ë˜í•œ Nê°œì˜ threadë¥¼ launchí•˜ë©´ì„œ, Nì„ referenceí•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤.

```c
__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}
```

---

