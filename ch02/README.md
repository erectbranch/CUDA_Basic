# 2 CUDA Programming Model

ì´ë²ˆ ì¥ì€ vector addition, matrix addition ì˜ˆì œë¥¼ CUDA programìœ¼ë¡œ ì‘ì„±í•˜ë©° ì‚´í´ë³¼ ê²ƒì´ë‹¤.

---

## 2.1 CUDA programming modelì´ë€?

programming modelì€ applicationì´ hardwareì—ì„œ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë„ë¡ í•˜ëŠ” computer architectureì„ abstractioní•œ í˜•íƒœì— í•´ë‹¹ëœë‹¤.

> programming languageë‚˜ programming environmentë¡œ ë‚˜íƒ€ë‚œë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ programê³¼ programming model êµ¬í˜„ì— ìˆì–´ì„œì˜ abstractionì„ ê³„ì¸µ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

> CUDA programming modelì€ GPU architectureì˜ memory hierarchyì˜ abstractionì„ ë“œëŸ¬ë‚¸ë‹¤.

![abstraction layer](images/programming_model_layer.png)

ë˜í•œ ë‹¤ë¥¸ parallel programming modelì—ì„œ abstractionë“¤ì„ shareí•˜ê¸° ìœ„í•´ì„œ, CUDA programming modelì€ ë‹¤ìŒê³¼ ê°™ì´ GPUë¥¼ ì œì–´í•˜ëŠ” íŠ¹ì§•ë“¤ì„ ê°–ëŠ”ë‹¤.

- organize threads on the GPU through a hierarchy structure

- access memory on the GPU through a hierarchy structure

<br/>

í”„ë¡œê·¸ë˜ë¨¸ ê´€ì ì—ì„œëŠ” parallel computationì„ ë‹¤ìŒê³¼ ê°™ì€ levelë¡œ ë³¸ë‹¤. 

- domain level

    programê³¼ algorithmì„ ë””ìì¸í•  ë•Œ í•´ë‹¹í•˜ëŠ” levelì´ë‹¤. ì–´ë–»ê²Œ dataì™€ functionì„ **decompose**(ë¶„í•´)í•´ì•¼ íš¨ìœ¨ì ìœ¼ë¡œ programì´ parallelí•˜ê²Œ ìˆ˜í–‰ë ì§€ ê³ ë¯¼í•œë‹¤.

- logic level

    programê³¼ algorithm ë””ìì¸ì´ ëë‚˜ë©´ programming ë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤. ì–´ë–»ê²Œ êµ¬ì„±í•´ì•¼ logicì„ concurrent threadë“¤ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ì§€ ê³ ë¯¼í•œë‹¤. 

- hardware level

    threadë¥¼ íš¨ìœ¨ ì¢‹ê²Œ coreì— mappingí•˜ëŠ” ë°©ë²• ë“±ì„ ê³ ë ¤í•œë‹¤.

---

## 2.2 CUDA Programming Structure

> [CUDA ê¸°ì´ˆ](https://velog.io/@lunarainproject/CUDA-%EA%B8%B0%EC%B4%88)

- ì±…(2014 ë°œê°„)ì—ì„œëŠ” CUDA 6ìœ¼ë¡œ ì‹¤ìŠµì„ ì§„í–‰í•œë‹¤.

- host(CPU) memoryëŠ” variable ì´ë¦„ ì•ì— h_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

- device(GPU) memoryëŠ” variable ì´ë¦„ ì•ì— d_ë¥¼ ë¶™ì—¬ì„œ êµ¬ë¶„í•  ê²ƒì´ë‹¤.

ì—¬ê¸°ì„œ CPUì™€ GPU ì‚¬ì´ì— ê³µìœ ë˜ëŠ” managed memory poolì„ í˜•ì„±í•˜ëŠ” **unified memory**ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ë•ë¶„ì— CPUì™€ GPU memory ëª¨ë‘ ë‹¨ì¼ pointerë¥¼ ì‚¬ìš©í•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. unified memoryì— allocateëœ dataë¥¼ hostì™€ device ì‚¬ì´ì— ìë™ìœ¼ë¡œ **migrate**í•œë‹¤.

CUDAì˜ í•µì‹¬ì€ kernelì´ë‹¤. CUDAëŠ” GPU threadì—ì„œ ë™ì‘í•˜ëŠ” kernelë“¤ì„ schedulingí•œë‹¤. 

![serial code execute](images/execute_on_CPU_GPU.png)

hostëŠ” ëŒ€ë¶€ë¶„ì˜ operationì—ì„œ deviceì™€ independentí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë‹¤. kernelì´ **launch**(êµ¬ë™)ì„ ì‹œì‘í•˜ë©´, hostëŠ” data parallel codeë¥¼ GPUì—ì„œ ì‘ë™í•˜ê²Œ ë§Œë“œëŠ” additional taskì—ì„œ ë²—ì–´ë‚˜ ì¦‰ì‹œ control ì‘ì—…ìœ¼ë¡œ ëŒì•„ê°„ë‹¤.

ë‹¤ì‹œ ë§í•´ kernelì€ **asynchronous**(ë¹„ë™ê¸°ì )ìœ¼ë¡œ launchëœë‹¤. hostëŠ” kernel launchê°€ ì™„ë£Œë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.

> CUDA runtimeì—ì„œ ì œê³µí•˜ëŠ” cudaDeviceSynchronizeë¥¼ ì´ìš©í•´ì„œ CPUê°€ device codeì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë§Œë“¤ ìˆ˜ë„ ìˆë‹¤.

---

## 2.3 managing memory

CUDA runtimeì€ device memoryë¥¼ allocateí•˜ëŠ” functionë“¤ì„ ì œê³µí•œë‹¤.

| í‘œì¤€ C function | CUDA C function |
| --- | --- |
| malloc | cudaMalloc |
| memcpy | cudaMemcpy |
| memset | cudaMemset |
| free | cudaFree |

ìš°ì„  GPU memory allocationì„ ìœ„í•œ cudaMallocì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤. ë‘ ê°€ì§€ parameterê°€ í•„ìš”í•˜ë‹¤.

```c
cudaError_t cudaMalloc ( void** devPtr, size_t size )
```

- cudaError_të¥¼ ì…ë ¥í•˜ë©´ error ë°œìƒ ì‹œ ì´ìœ ë¥¼ ì¶œë ¥í•´ ì¤€ë‹¤.

- allocateí•œ memoryëŠ” í•´ë‹¹ device memory addressë¥¼ ê°€ë¦¬í‚¤ëŠ” pointerì¸ devPtrì„ í†µí•´ returnëœë‹¤.

hostì™€ device ì‚¬ì´ì— dataë¥¼ transferí•˜ê¸° ìœ„í•œ functionìœ¼ë¡œëŠ” cudaMemcpyë¥¼ ì‚¬ìš©í•œë‹¤. unified memoryì— ì¡´ì¬í•˜ëŠ” dataê°€ ì•„ë‹ˆë¼ë©´, dataë¥¼ ì‚¬ì „ì— device memoryë¡œ transferí•´ì•¼ í•œë‹¤.

```c
cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )
```

- dst: destination memory address pointer

- src: source memory address pointer

- count: copyí•  byte size

- kind: data transfer type

  - cudaMemcpyHostToHost

  - cudaMemcpyHostToDevice

  - cudaMemcpyDeviceToHost

  - cudaMemcpyDeviceToDevice

<U>cudaMemcpyëŠ” synchronous behavior</U>ì´ë‹¤. host applicationì€ cudaMemcpyì˜ return/transferê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ë©ˆì¶˜ë‹¤.

ì°¸ê³ ë¡œ kernel launchë¥¼ ì œì™¸í•œ ëª¨ë“  CUDA callì€, enumerated type cudaError_tìœ¼ë¡œ error codeë¥¼ returní•œë‹¤. 

- ë§Œì•½ GPU memoryì— ì„±ê³µì ìœ¼ë¡œ allocateí–ˆë‹¤ë©´, 'cudaSuccess'ë¥¼ returní•œë‹¤.

- ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ cudaErrorMemoryAllocationì„ returní•œë‹¤.

ë‹¤ìŒ functionì„ ì‚¬ìš©í•˜ë©´ ì´ë¥¼ error messageë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.(Cì˜ strerror functionê³¼ ë¹„ìŠ·í•˜ë‹¤.)

```c
char* cudaGetErrorString(cudaError_t error)
```

![GPU memory hierarchy](images/GPU_memory_hierarchy.png)

ì´ì œ ì˜ˆì œë¥¼ ë³´ë©° hostì™€ deviceê°„ì˜ data movement ê´€ë¦¬ë¥¼ ì‚´í´ë³´ì. 

![array summation](images/array_summation.png)

ìš°ì„  ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ array ì—°ì‚°(host-based array summation)ì„ ì˜¤ì§ Cë§Œ ì‚¬ìš©í•´ì„œ êµ¬í˜„í•œë‹¤. íŒŒì¼ëª…ì€ sumArraysOnHost.cì´ë‹¤.

> [int main(int argc, char **argv)ë€?](https://iamaman.tistory.com/364)

```c
#include <stdlib.h>
#include <string.h>
#include <time.h>

// Hostì—ì„œ array sum ìˆ˜í–‰
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    for (int idx=0; idx<N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

// arrayì— random numberë¡œ ì´ˆê¸°ê°’ì„ ì„¤ì •
void initialData(float *ip, int size) {
    // random number ìƒì„±
    time_t t;
    srand((unsigned int) time(&t));

    for (int i=0; i<size; i++) {
        ip[i] = (float) ( rand() & 0xFF )/10.0f;
    }
}

int main(int argc, char **argv) {
    int nElem = 1024;
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *h_C;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    h_C = (float *)malloc(nBytes);

    initialData(h_A, nElem);
    initialData(h_B, nElem);

    sumArraysOnHost(h_A, h_B, h_C, nElem);

    free(h_A);
    free(h_B);
    free(h_C);

    return(0);
}
```

pure C programì´ë¯€ë¡œ C compilerë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ nvcc compilerë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
$ nvcc -Xcompiler -std=c99 sumArraysOnHost.c -o sum
$ ./sum
```

ì°¸ê³ ë¡œ ìœ„ compile ëª…ë ¹ì˜ **flag**(ì˜µì…˜)ì€ ë‹¤ìŒ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤.(ìì„¸íˆëŠ” CUDA compiler documentë¥¼ ì‚´í”¼ë©´ ì•Œ ìˆ˜ ìˆë‹¤.)

- -Xcompiler: ë°”ë¡œ C compiler ë˜ëŠ” preprocessorë¡œ ì²˜ë¦¬í•œë‹¤.

- -std=c99: code styleì´ c99 standardì„ì„ ì•Œë¦°ë‹¤.

ì´ ì—°ì‚°ì„ GPUì—ì„œ êµ¬ë™í•˜ê²Œë” ë°”ê¾¸ëŠ” ê²ƒì€ ì‰½ë‹¤. 

1. GPU memory allocation

```c
float *d_A, *d_B, *d_C;
cudaMalloc((float**) &d_A, nBytes);
cudaMalloc((float**) &d_B, nBytes);
cudaMalloc((float**) &d_B, nBytes);
```

2. host memoryì—ì„œ GPU global memoryë¡œ dataë¥¼ transferí•œë‹¤.

```c
cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);
```

3. kernel ìˆ˜í–‰.

GPU global memoryë¡œ transferí•˜ëŠ” ê³¼ì •ì´ ëë‚˜ë©´, ì´ì œë¶€í„°ëŠ” host sideì—ì„œ GPUì—ì„œ array summationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ kernel functionì„ invokeë  ìˆ˜ ìˆë‹¤. ë§Œì•½ kernelì´ callë˜ë©´ controlì€ ì¦‰ì‹œ hostë¡œ return backë˜ë©°, GPUê°€ kernelì„ ìˆ˜í–‰í•˜ëŠ” ì‚¬ì´ì— ë‹¤ë¥¸ functionì„ ìˆ˜í–‰í•œë‹¤.(asynchronous ë™ì‘)

4. resultë¥¼ host memoryë¡œ copyí•œë‹¤.

kernel ì‘ì—…ì´ ëª¨ë‘ ëë‚˜ë©´, result(array d_C)ëŠ” GPU global memoryì— ì €ì¥ëœë‹¤. ì´ì œ ì´ resultë¥¼ host array(gpuRef)ë¡œ copyí•´ì•¼ í•œë‹¤.

```c
cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);
```

> ë§Œì•½ ì´ë ‡ê²Œ copyë¥¼ ì§„í–‰í•˜ì§€ ì•Šê³  'gpuRef = d_C'ì™€ ê°™ì€ ì˜ëª»ëœ assignmentë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ê²Œ ë˜ë©´ applicationì€ runtimeì— crashëœë‹¤.

> ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ unified memoryê°€ CUDA 6ë¶€í„° ì œê³µëë‹¤. CPUì™€ GPU memory ëª¨ë‘ single pointerë¥¼ ì‚¬ìš©í•œë‹¤.

cudaMemcpyì— ì˜í•´ hostëŠ” copy ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ë©ˆì¶”ê²Œ ëœë‹¤. 

5. memoryë¥¼ releaseí•œë‹¤.

```c
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
```

---

## 2.4 organizing threads

> [thread block architecture](https://tododiary.tistory.com/57)

host sideì—ì„œ kernel functionì´ launchë˜ë©´, deviceë¡œ executionì´ ë„˜ì–´ê°€ë©° kernel functionì—ì„œ ì •ì˜í•œ threadë“¤ì—ì„œ ëª…ë ¹ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ì—ˆë‹¤. ì´ë•Œ 'threadë“¤ì„ ì–´ë–»ê²Œ êµ¬ì„±í•˜ëŠ”ê°€'ë¼ëŠ” ë¬¸ì œê°€ CUDA programmingì— ìˆì–´ì„œ í•µì‹¬ì ì¸ ë¶€ë¶„ì´ë‹¤.

threadëŠ” 2-level hierarcyë¡œ êµ¬ì„±ëœë‹¤. í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ blockìœ¼ë¡œ êµ¬ì„±ë˜ê³ , ê° blockì€ í•˜ë‚˜ ì´ìƒì˜ threadë¡œ êµ¬ì„±ëœë‹¤.

![thread hierarchy](images/thread_hierarchy.png)

- í•˜ë‚˜ì˜ kernel launchë¡œ ìƒì„±ëœ ëª¨ë“  threadë¥¼ **grid**ë¼ê³  í†µì¹­í•œë‹¤.

  - grid ë‚´ ëª¨ë“  threadëŠ” ê°™ì€ global memory spaceë¥¼ shareí•œë‹¤.

  - ì´ threadë“¤ì´ ëª¨ë‘ ë™ì¼í•œ kernel codeë¥¼ ì‹¤í–‰í•œë‹¤.

- í•˜ë‚˜ì˜ gridëŠ” ì—¬ëŸ¬ thread **block**ë“¤ë¡œ êµ¬ì„±ëœë‹¤. ê° thread blockëŠ” ë‹¤ìŒ íŠ¹ì§•ì„ ê°€ì§€ê³  cooperateí•  ìˆ˜ ìˆë‹¤.

  - block-local synchronization

  - block-local shared memory

  > <U>ë‹¤ë¥¸ blockì˜ threadë¼ë¦¬ëŠ” cooperateí•  ìˆ˜ ì—†ë‹¤.</U>

> blockì€ threadë¥¼ ìµœëŒ€ 512ê°œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ blockì´ ê°€ì§€ëŠ” thread ê°œìˆ˜ëŠ” 32(ë˜ëŠ” NVIDIAëŠ” 64ë¥¼ ê¶Œì¥) ë°°ìˆ˜ë¡œ ì§€ì •í•˜ëŠ” í¸ì´ ì¢‹ë‹¤.(SMì´ 32ë°°ìˆ˜ ë‹¨ìœ„ë¡œ ë™ì‘)

threadë“¤ì€ ë‹¤ìŒ indexë¥¼ í†µí•´ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° index(coordinate variables)ëŠ” CUDA runtimeì— ì˜í•´ ê° threadë³„ë¡œ í• ë‹¹ëœë‹¤.

![blockIdx, threadIdx](images/blockIdx_threadIdx.png)

- blockIdx: grid ë‚´ë¶€ì—ì„œì˜ block index

- threadIdx: block ë‚´ë¶€ì—ì„œì˜ thread index

> coordinate variableì€ uint3 typeì„ ê°–ëŠ”ë‹¤. ì´ structureì€ 3ê°œì˜ unsigned integerë¡œ êµ¬ì„±ë˜ë©°, ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸, ì„¸ ë²ˆì§¸ componentëŠ” x, y, zë¥¼ ë¶™ì—¬ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```c
blockIdx.x
blockIdx.y
blockIdx.z

// block ë‚´ ëª¨ë“  threadëŠ” ë™ì¼í•œ blockIdxë¥¼ ê³µìœ í•œë‹¤.
threadIdx.x
threadIdx.y
threadIdx.z
```

kernel launch êµ¬ë¬¸ì—ì„œ execution configutation parameters(<<<...>>>)ë¡œ gridì™€ ê° blockì˜ dimensionì„ ì§€ì •í–ˆë‹¤. gridëŠ” ì£¼ë¡œ blockì˜ 2D array, blockì€ ì£¼ë¡œ threadì˜ 3D arrayë¡œ êµ¬ì„±ëœë‹¤. ì´ëŸ° gridì™€ blockì˜ dimentionì€ ë‹¤ìŒ built-in variableë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- blockDim

- gridDim

> ì´ variableë“¤ì€ dim3 typeì´ë©°, uint3ì— ê¸°ë°˜í•œ dimensionì— íŠ¹í™”ëœ integer vector typeì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ blockDim.x, blockDim.y, blockDim.zë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

> ì‚¬ìš©í•˜ì§€ ì•Šì€ ì°¨ì›ì˜ í¬ê¸°ëŠ” 1ë¡œ ì§€ì •í•œë‹¤.(ê°’ì„ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ defaultë¡œ 1ì´ ë˜ì–´ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ëœë‹¤.)

![ì „ì—­ ì¸ë±ìŠ¤](images/CUDA_grid_execution.png)

ì´ëŸ° variableì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ gridì—ì„œ ìœ ì¼í•œ global indexë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ë‹¤ìŒì´ í•´ë‹¹ëœë‹¤. 

- i = blockIdx.x * blockDim.x + threadIdx.x

> blockì´ 1Dì´ë©°, threadê°€ 256ê°œë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ block 0ì˜ threadì—ì„œ iì˜ ë²”ìœ„ëŠ” 0\~255 / block 1ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 256\~511 / block 2ì˜ threadì—ì„œëŠ” iì˜ ë²”ìœ„ê°€ 512\~767... ì‹ìœ¼ë¡œ ë°°ì •ëœë‹¤.

> ì´ëŸ° ë°©ë²•ìœ¼ë¡œ index ië¥¼ ì´ìš©í•˜ì—¬ vector A, B, C ê°’ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

ì´ëŸ° ë°©ì‹ ë•ë¶„ì— kernel functionì€ loopê°€ ì—†ë‹¤. ëŒ€ì‹  loopë¥¼ gridë¡œ ëŒ€ì²´í•˜ê³ , ê° threadê°€ iteration í•˜ë‚˜ì— ëŒ€ì‘ë˜ë©° ì—°ì‚°í•œë‹¤. ì´ëŸ° ì¢…ë¥˜ì˜ data parallelismì„ **loop parallelism**ì´ë¼ê³  ì§€ì¹­í•œë‹¤.

ì£¼ì˜í•  ì ì€ element ê°œìˆ˜(vector size)ê°€ block sizeì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ elementê°€ ì´ 100ê°œë¼ë©´, íš¨ìœ¨ì„±ì„ ê³ ë ¤í•œ ê°€ì¥ ì‘ì€ thread ì°¨ì›ì€ 32ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ blockì€ ì´ 4ê°œê°€ ìƒê¸°ê³ , ì´ 128ê°œì˜ threadë¥¼ ê°€ì§€ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ì´ë ‡ê²Œ êµ¬ì„±í•  ê²½ìš° 28ê°œì˜ threadëŠ” ë¹„í™œì„±í™”í•´ì•¼(ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ì•¼) í•œë‹¤. 

> block ê°œìˆ˜ë¥¼ êµ¬í•˜ëŠ” ì‹ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ vector sizeê°€ 1000ì´ê³  ê° blockì´ 256ê°œì˜ threadë¥¼ ê°€ì§„ë‹¤ë©´, (1000 + 256 - 1) / 256 = 4ë¡œ 4ê°œì˜ blockì„ ìƒì„±í•˜ê²Œ ëœë‹¤. ì´ ê²½ìš° ê²°ê³¼ì ìœ¼ë¡œëŠ” 256*4 = 1024 threadê°€ ì‹¤í–‰ë˜ê³ , ë‚˜ë¨¸ì§€ 24ê°œëŠ” ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•œë‹¤.

> ë¹„ìŠ·í•˜ê²Œ gridëŠ” (nElem + block.x - 1)/block.x) ê°œê°€ ëœë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: gridì™€ block dimension êµ¬í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

hostì™€ device ì–‘ìª½ì—ì„œ gridì™€ blockì˜ dimensionì„ ì²´í¬í•  ê²ƒì´ë‹¤.

deviceì—ì„œëŠ” kernel functionì„ ë§Œë“¤ì–´ì„œ, ê°ìì˜ thread index, block index, grid dimensionì„ ì¶œë ¥í•œë‹¤. íŒŒì¼ëª…ì€ checkDimension.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

__global__ void checkIndex(void) {
    printf("threadIdx: (%d, %d, %d) blockIdx: (%d, %d, %d) blockDim: (%d, %d, %d) "
        "gridDim: (%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z,
        blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z,
        gridDim.x, gridDim.y, gridDim.z);
}

int main(int argc, char **argv) {
    // number of elements
    int nElem = 6;

    // grid, block structure ì •ì˜
    // ì§€ì •í•˜ì§€ ì•Šì€ ì°¨ì›ì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒ(1)ìœ¼ë¡œ ì²˜ë¦¬

    // 3ê°œì˜ threadë¥¼ í¬í•¨í•˜ëŠ” 1D block
    dim3 block (3);
    // grid ê°œìˆ˜
    dim3 grid  ((nElem + block.x - 1)/block.x);

    // check grid and block dimension from host side
    printf("grid.x %d grid.y %d grid.z %d\n", grid.x, grid.y, grid.z);
    printf("block.x %d block.y %d block.z %d\n", block.x, block.y, block.z);

    // check grid and block dimension from device side
    checkIndex <<<grid, block>>> ();

    // reset device
    cudaDeviceReset();

    return(0);
}
```

CUDAì—ì„œ printf functionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” compileë˜ëŠ” í™˜ê²½ì˜ GPU architectureë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤.

```bash
nvcc -arch=sm_80 checkDimension.cu -o check
./check
```

> ì±…ì€ Fermi GPUì´ë¯€ë¡œ -arch=sm_20ì„ ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤.([CUDA -arch í™•ì¸](https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/))(FermiëŠ” CUDA ìƒìœ„ ë²„ì „ì—ì„œ deprecatedëë‹¤.) 

> í˜„ì¬ ì‹¤ìŠµ ì¤‘ì¸ í™˜ê²½ì€ RTX 3080 Tië¡œ Ampere architectureì´ë‹¤.(sm_86)($ nvidia-smi -q ëª…ë ¹ìœ¼ë¡œ í™•ì¸)([GPU í™•ì¸](https://kyumdoctor.tistory.com/72))

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![grid block indices and dimensions](images/grid_block_indice.png)

ì´ ì˜ˆì œì—ì„œ ëª…ì‹¬í•  ì ì€ gridì™€ block variableì˜ ì ‘ê·¼ì—ì„œ, hostì™€ deviceê°€ êµ¬ë¶„ëœë‹¤ëŠ” ì‚¬ì‹¤ì´ë‹¤.

- host: hostì—ì„œ 'block'ìœ¼ë¡œ declarationí•œ ë’¤, block.x, block.y, block.zì„ ì‚¬ìš©.

- device: built-in block size variableì¸ blockDim.x, blockDim.y, blockDim.zë¥¼ ì‚¬ìš©.

ì´ëŠ” kernel launchë³´ë‹¤ ì•ì„œ hostì—ì„œ gridì™€ block variableì„ ì •ì˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ì •ë¦¬í•˜ìë©´ data sizeê°€ ì£¼ì–´ì¡Œì„ ë•Œ, gridì™€ block dimensionì€ ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ ì •í•œë‹¤.

1. block sizeë¥¼ ê²°ì •

2. grid dimensionì„ data sizeì™€ block sizeë¥¼ ì´ìš©í•œ ê³„ì‚°ì„ í†µí•´ ê²°ì •

> block dimensionì„ ì •í•  ë•ŒëŠ” kernelì˜ performanceì ì¸ íŠ¹ì§•ê³¼, GPU resourceì˜ limitationì„ ì•Œì•„ì•¼ í•œë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: hostì—ì„œ grid, block dimension ì •ì˜í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

ì•„ë˜ ì˜ˆì œë¥¼ í†µí•´ 1D gridì™€ 1D blockì„ hostì—ì„œ ì •ì˜í•  ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ defineGridBlock.cuì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

int main(int argc, char **argv) {
    // define total data elements
    int nElem = 1024;

    // define grid and block structure
    dim3 block (1024);
    dim3 grid  ((nElem + block.x - 1)/block.x);
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 512;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset block
    block.x = 256;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);
    
    // reset block
    block.x = 128;
    grid.x = (nElem + block.x - 1)/block.x;
    printf("grid.x %d block.x %d \n", grid.x, block.x);

    // reset device
    cudaDeviceReset();
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œë‹¤.

```bash
nvcc defineGridBlock.cu -o block
./block 
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![defineGridBlock](images/defineGridBlock.png)

---

## 2.5 launching a CUDA kernel

ì•ì„œ CUDA kernel callì€ ë‹¤ìŒê³¼ ê°™ì´ ëª…ë ¹í–ˆë‹¤.

```c
kernel_name <<<grid, block>>>(argument list);
```

ë‹¤ìŒ ë‘ ì˜ˆì‹œë¥¼ ë³´ì.

- kernel_name<<<1, 32>>>(argument list): 32ê°œì˜ elementë¥¼ í•œ blockì— ë‹´ëŠ”ë‹¤.

- kernel_name<<<32, 1>>>(argument list): 32ê°œì˜ blockì€ ëª¨ë‘ element í•˜ë‚˜ë§Œì„ ê°–ëŠ”ë‹¤.

ë˜í•œ kernelì€ type qualifierë¡œ \_\_global\_\_ì„ ë¶™ì—¬ì„œ declarationí–ˆë‹¤.

```c
__global__ void kernel_name(argument list);
```

ì´ë•Œ <U>kernel functionì€ ê¼­ void return type</U>ì´ì–´ì•¼ í•œë‹¤. 

ì•„ë˜ëŠ” CUDA C programmingì—ì„œì˜ type qualifierë¥¼ ì •ë¦¬í•œ í‘œì´ë‹¤.

| qualifer | execution | callable | ì„¤ëª… |
| :---: | :---: | :---: | :---: |
| \_\_global\_\_ | device | hostì—ì„œ callable<br/>NVIDIAê°€ ì œì‹œí•˜ëŠ” compute capabilityê°€ 3 ì´ìƒì¸ device | void return typeì´ì–´ì•¼ í•œë‹¤. |
| \_\_device\_\_ | device | device only | |
| \_\_host\_\_ | host | host only | ìƒëµí•´ë„ ë¬´ë°©í•˜ë‹¤. |

> ì°¸ê³ ë¡œ functionì´ hostì™€ device ì–‘ìª½ì—ì„œ compileëœë‹¤ë©´, \_\_device\_\_ì™€ \_\_host\_\_ qualifierë¥¼ ê°™ì´ ì¨ë„ ëœë‹¤. 

> ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.<br/> \_\_host\_\_\_\_device\_\_const char* cudaGetErrorString(cudaError_t error)

ë˜í•œ CUDA kernelì˜ ì œì•½ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

- device memoryë§Œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤.

- void return typeë§Œ ê°€ëŠ¥í•˜ë‹¤.

- ìœ ë™ì ì¸ ìˆ«ìì˜ variableì„ argumentë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

- static variableì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- function pointerë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤.

- asynchronousí•œ íŠ¹ì„±ì„ ë³´ì¸ë‹¤.(ë”°ë¼ì„œ errorë¥¼ ì•Œê¸° í˜ë“¤ë‹¤.)

ë¬´ì—‡ë³´ë‹¤ë„ ë‘ ë²¡í„°ë¥¼ ë”í•˜ëŠ” ì—°ì‚°(A+B = C) functionì„ Cì™€ CUDAë¡œ êµ¬í˜„í–ˆì„ ë•Œì˜ ì°¨ì´ë¥¼ ë³´ë©´ ì œì¼ ê·¹ëª…í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.

ì•„ë˜ëŠ” C codeì´ë‹¤.

```c
void sumArraysOnHost(float *A, float *B, float *C, const int N) {
  for (int i = 0; i < N; i++) {
    C[i] = A[i] + B[i];
  }
}
```

ë‹¤ìŒì€ CUDA codeì´ë‹¤. loopê°€ ì‚¬ë¼ì§€ê³  built-in thread coordinate variableì´ array indexë¥¼ ëŒ€ì‹ í–ˆë‹¤. ë˜í•œ Nê°œì˜ threadë¥¼ launchí•˜ë©´ì„œ, Nì„ referenceí•  í•„ìš”ê°€ ì—†ì–´ì¡Œë‹¤.

```c
__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
  int i = threadIdx.x;
  C[i] = A[i] + B[i];
}
```

---

## 2.6 Handling Errors

> [error handling functions](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html)

CUDA kernel launchë¡œ ë°œìƒí•˜ëŠ” errorëŠ” synchronous errorì™€ asynchronos error ë‘ ê°€ì§€ typeìœ¼ë¡œ ë‚˜ë‰œë‹¤.

- synchronous error: hostì—ì„œ kernelì´ illegalí•˜ê±°ë‚˜ invalidí•œ ê²ƒì„ ì•Œê²Œ ë˜ë©´ ë°œìƒí•œë‹¤. 

  - ì˜ˆë¥¼ ë“¤ì–´ thread block sizeë‚˜ grid sizeë¥¼ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í–ˆë‹¤ë©´, kernel launch callì´ ì‹¤í–‰ë˜ëŠ” ë™ì‹œì— ë°”ë¡œ synchronous errorë¥¼ ë°œìƒì‹œí‚¨ë‹¤.

- asynchronous error: kernel execution, í˜¹ì€ CUDA runtime asynchronous API execution ì¤‘ ë°œìƒí•œë‹¤.

  - ì˜ˆë¥¼ ë“¤ì–´ kernel execution ì¤‘ ì˜ëª»ëœ memory addressì— ì ‘ê·¼í•˜ë©´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. (cudaMemcpyAsyncì™€ ê°™ì€ CUDA runtime asynchronous API executionì—ì„œ ë°œìƒí•  ìˆ˜ ìˆë‹¤.)

> kernel launch call ë°”ë¡œ ë‹¤ìŒì— cudaGetLastError APIë¥¼ ì‚¬ìš©í•´ì„œ error capturingë„ ê°€ëŠ¥í•˜ë‹¤.

> ì°¸ê³ ë¡œ í•´ê²°í•˜ê¸° ì–´ë ¤ìš´, not-recoverable errorë¥¼ **sticky error**, recoverableí•œ errorë¥¼ **non-sticky error**ë¼ê³  ì§€ì¹­í•˜ê¸°ë„ í•œë‹¤.

> cudaMallocì—ì„œ GPU memory ë¶€ì¡±ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” errorëŠ” non-sticky errorì— í•´ë‹¹í•œë‹¤. ë°˜ë©´ host processê°€ terminateë˜ê¸° ì „ê¹Œì§€ CUDA contextê°€ corruptë˜ëŠ” errorëŠ” sticky errorì— í•´ë‹¹í•œë‹¤.

ëŒ€ì²´ë¡œ kernelì´ asynchronousí•˜ê¸° ë•Œë¬¸ì—, errorê°€ ì–´ë””ì„œ ë°œìƒí–ˆëŠ”ì§€ ì•Œê¸° í˜ë“¤ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ CUDA API callë“¤ì„ ê²€ì¦í•˜ëŠ” macroë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•˜ë©´ ë¶ˆí¸í•¨ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

```c
#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}
```

ì´ë ‡ê²Œ ë§Œë“¤ì—ˆë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ kernelì„ ê°ì‹¸ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```c
CHECK(cudaMemcpy(d_C, gpuRef, nBytes, cudaMemcpyHostToDevice));
```

debugging ëª©ì ìœ¼ë¡œ í•œì •ì§€ìœ¼ë©´, kernel errorë¥¼ checkí•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í…Œí¬ë‹‰ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ preceding requested taskê°€ ëë‚  ë•Œê¹Œì§€ host applicationì„ ë§‰ëŠ” ê²ƒì´ë‹¤.

```c
kernel_function<<<grid, block>>>(argument list);
CHECK(cudaDeviceSynchronize());    // cudaError_t cudaDeviceSynchronize(void);ëŠ” ë°”ë¡œ ì „ asynchronous CUDA operationsì˜ errorë¥¼ returní•œë‹¤.
```

---

### 2.7 compiling and Executing

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: vector addition&nbsp;&nbsp;&nbsp;</span>

1ì°¨ì› array A, B(ì¦‰, vector)ì˜ vector additionì„ ìˆ˜í–‰í•œ ë’¤ ì—°ì‚° ê²°ê³¼ë¥¼ array Cì— ì €ì¥í•  ê²ƒì´ë‹¤. ì—°ì‚°ì€ host ë²„ì „ vector addition(sumArraysOnHost)ê³¼ GPU ë²„ì „ vector addition(sumArraysOnGPU)ì„ ëª¨ë‘ ìˆ˜í–‰í•œ ë’¤ ì„œë¡œì˜ ì—°ì‚° ê²°ê³¼ë¥¼ ë¹„êµ(checkResult)í•´ ë³¼ ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ sumArraysOnGPU-small-case.cuì´ë‹¤.

> ê³„ì‚° ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ê²€í† í•˜ë ¤ë©´ double typeì˜ ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ë‚´ì—ì„œ ë¹„êµë¥¼ í•˜ë©´ ëœë‹¤. í˜„ì¬ ì˜ˆì œì—ì„œëŠ” ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ì„ 1.0e-8 ì´í•˜ê¹Œì§€ í—ˆìš©í•˜ê²Œ êµ¬ì„±í–ˆë‹¤.

- vector size: 32

- ë‹¤ìŒê³¼ ê°™ì´ í•œ blockì´ 32ê°œì˜ elementìœ¼ë¡œ êµ¬ì„±ë˜ë„ë¡ í–ˆë‹¤.

  ```c
  dim3 block (nElem);    // nElem = 32
  dim3 grid  (nElem/block.x); 
  ```

  - ì•„ë˜ì™€ ê°™ì´ blockë‹¹ 1ê°œì˜ elementë¡œ êµ¬ì„±í•˜ëŠ” ëŒ€ì‹ , blockì„ 32ê°œë¡œ ì„¤ì •í•´ë„ ë¬´ë°©í•˜ë‹¤. ë‹¤ë§Œ kernelì´ ì‚¬ìš©í•˜ëŠ” indexë„ ìˆ˜ì •í•´ì•¼ í•œë‹¤.

    ```c
    dim3 block (1);
    dim3 grid  (nElem);

    //...
    int i = blockIdx.x;
    ```

```c
#include <cuda_runtime.h>
#include <stdio.h>

#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}

void checkResult(float *hostRef, float *gpuRef, const int N) {
    double epsilon = 1.0E-8;
    bool match = 1;
    for (int i = 0; i < N; i++) {
        if (abs(hostRef[i] - gpuRef[i]) > epsilon) {
            match = 0;
            printf("Arrays do not match!\n");
            printf("host %5.2f gpu %5.2f at current %d\n", hostRef[i], gpuRef[i], i);
            break;
        }
    }
    
    if (match) printf("Arrays match.\n\n");
}

void initialData(float *ip, int size) {
    // generate different seed for random number
    // Arrayì— random elementë¥¼ ì±„ìš´ë‹¤.
    time_t t;
    srand((unsigned) time(&t));

    for (int i = 0; i < size; i++) {
        ip[i] = (float)( rand() & 0xFF )/10.0f;
    }
}

void sumArraysOnHost(float *A, float *B, float *C, const int N) {
    // Array A, Bì˜ ê° elementë¥¼ í•©ì‚° = Array C
    for (int idx = 0; idx < N; idx++) {
        C[idx] = A[idx] + B[idx];
    }
}

__global__ void sumArraysOnGPU(float *A, float *B, float *C) {
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaSetDevice(dev);

    // set up data size of vectors
    int nElem = 32;
    printf("Vector size: %d\n", nElem);

    // malloc host memory
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A     = (float *)malloc(nBytes);
    h_B     = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef  = (float *)malloc(nBytes);

    // initialize data at host side
    initialData(h_A, nElem);
    initialData(h_B, nElem);

    memset(hostRef, 0, nBytes);
    memset(gpuRef, 0, nBytes);

    // malloc device global memory
    float *d_A, *d_B, *d_C;
    cudaMalloc((float**)&d_A, nBytes);
    cudaMalloc((float**)&d_B, nBytes);
    cudaMalloc((float**)&d_C, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    dim3 block (nElem);
    dim3 grid  (nElem/block.x);

    sumArraysOnGPU<<< grid, block >>>(d_A, d_B, d_C);
    printf("Execution configuration <<<%d, %d>>>\n", grid.x, block.x);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);

    // add vector at host side for result checks
    sumArraysOnHost(h_A, h_B, hostRef, nElem);

    // check device results
    checkResult(hostRef, gpuRef, nElem);

    // free device global memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•´ì„œ ì‹¤í–‰í•˜ë©´ ëœë‹¤.

```bash
$ nvcc sumArraysOnGPU-small-case.cu -o addvector
$ ./addvector
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![addvector result](images/addvector.png)

---

## 2.8 timing kernel

kernelì´ ì–¼ë§ˆë‚˜ ì‹œê°„ì„ ì†Œëª¨í•˜ëŠ”ì§€, ì–´ëŠ ì •ë„ê°€ ì ì •í•œ ì†Œëª¨ ì‹œê°„ì¸ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. host sideì—ì„œ CPU timerë‚˜ GPU timerë¥¼ ì‚¬ìš©í•˜ë©´ ë§¤ìš° ì‰½ê²Œ ì†Œëª¨í•œ ì‹œê°„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

---

### 2.8.1 timing with CPU timer

sys/time.h ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ gettimeofday()ë¥¼ ì´ìš©í•´ì„œ CPU timerë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

```c
double cpuSecond() {
    struct timeval tp;
    gettimeofday(&tp, NULL);
    return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);
}
```

ì´ë ‡ê²Œ timerë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´ kernelì„ ì‹œì‘í•˜ëŠ” ì‹œê°„ì„ ê¸°ë¡í•œ ë’¤, ëë‚œ ì‹œì ì—ì„œ ë‘ ì‹œê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ë©´ kernel ìˆ˜í–‰ ì‹œê°„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

```c
double iStart = cpuSecond();
kernel_name<<<grid, block>>>(argument list);
cudaDeviceSynchronize();
double iElaps = cpuSecond() - iStart;
```

í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì‹œê°„ì„ ì¸¡ì •í•  ê²½ìš°, CPUê°€ ëª¨ë“  GPU threadê°€ ì‘ì—…ì„ ì™„ë£Œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ë„ë¡ cudaDeviceSynchronize()ë¥¼ ê¼­ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: vector addition ì‹œê°„ ì¸¡ì •í•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

- ë¹„íŠ¸ ì—°ì‚°ìë¥¼ ì´ìš©í•´ì„œ elementë¥¼ ì•½ 1600ë§Œ ê°œë¡œ ì„¤ì •í•œë‹¤.(16,777,216ê°œ)

    ```c
    int nElem = 1<<24;
    ```

- kernelì˜ vector addition ë•Œ, array boundë¥¼ ë„˜ì–´ê°€ì§€ ì•Šë„ë¡ ê¼­ indexë¥¼ ì ê²€í•´ì•¼ í•œë‹¤.(total thread ê°œìˆ˜ê°€ vector element ê°œìˆ˜ë³´ë‹¤ ë§ê¸° ë•Œë¬¸)

   ```c
   __global__ void sumArraysOnGPU(float *A, float *B, float *C, const int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];    // 
    }
   }
   ```

   ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´ê°€ ì‰½ë‹¤.

   ![vector elements < total threads](images/threads_and_vector_elements.png)

ë‹¤ìŒì€ sumArraysOnGPU-timer.cu ì½”ë“œì´ë‹¤. ì•ì„œ ë³¸ ëª‡ ê°€ì§€ functionì€ ìƒëµí•´ì„œ ê¸°ë¡í–ˆë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>
#include <sys/time.h>

// error handlingì„ ìœ„í•œ CHECK macro ìƒëµ

// CPU timer ì—­í–˜ì„ í•˜ëŠ” cpuSecond() ìƒëµ

// random elementë¥¼ ìƒì„±í•˜ëŠ” initialData() ìƒëµ

// host sideì—ì„œ vector additionì„ ìˆ˜í–‰í•˜ëŠ” sumArraysOnHost() ìƒëµ

// device sideì—ì„œ vector additionì„ ìˆ˜í–‰í•˜ëŠ” sumArraysOnGPU() ìƒëµ

// host/deviceì˜ vector addition ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” checkResult() ìƒëµ

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set up data size of vectors
    int nElem = 1<<24;
    printf("Vector size %d\n", nElem);

    // malloc host memory
    size_t nBytes = nElem * sizeof(float);

    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A     = (float *)malloc(nBytes);
    h_B     = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef  = (float *)malloc(nBytes);

    double iStart, iElaps;

    // initialize data at host side
    iStart = cpuSecond();
    initialData(h_A, nElem);
    initialData(h_B, nElem);
    iElaps = cpuSecond() - iStart;

    memset(hostRef, 0, nBytes);
    memset(gpuRef,  0, nBytes);

    // add vector at host side for result checks
    iStart = cpuSecond();
    sumArraysOnHost(h_A, h_B, hostRef, nElem);
    iElaps = cpuSecond() - iStart;

    // malloc device global memory
    float *d_A, *d_B, *d_C;
    cudaMalloc((float**)&d_A, nBytes);
    cudaMalloc((float**)&d_B, nBytes);
    cudaMalloc((float**)&d_C, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_A, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    int iLen = 1024;
    dim3 block(iLen);
    dim3 grid ((nElem + block.x - 1)/block.x);

    iStart = cpuSecond();
    sumArraysOnGPU<<<grid, block>>>(d_A, d_B, d_C, nElem);
    cudaDeviceSynchronize();
    iElaps = cpuSecond() - iStart;
    printf("sumArraysOnGPU<<<%d, %d>>> Time elapsed %f" \
        "sec\n", grid.x, block.x, iElaps);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_C, nBytes, cudaMemcpyDeviceToHost);

    // check device results
    checkResult(hostRef, gpuRef, nElem);

    // free device global memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    return(0);  
}
```

ë‹¤ìŒê³¼ ê°™ì´ compile í›„ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc sumArraysOnGPU-timer.cu -o sumArraysOnGPU-timer
$ ./sumArraysOnGPU-timer
```

![sumArraysOnGPU-timer 1024 thread](images/sumArraysOnGPU-timer_1.png)

ì˜ˆì œëŒ€ë¡œ êµ¬ì„±í•˜ë©´ 1D gridê°€ 16,384ê°œì˜ blockì„ ê°–ëŠ”ë‹¤. ê·¸ë¦¬ê³  ê° blockì€ thread 1,024ê°œë¥¼ ê°€ì§„ë‹¤.)

ì—¬ê¸°ì„œ 1 blockì´ ê°–ëŠ” thread ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ëŠ” ëŒ€ì‹ , block ìˆ˜ë¥¼ 2ë°°ë¡œ ëŠ˜ë¦¬ë©´ ì†Œìš”ë˜ëŠ” ì‹œê°„ì´ ë‹¬ë¼ì§„ë‹¤.(iLen = 512)

![sumArraysOnGPU-timer 512 thread](images/sumArraysOnGPU-timer_2.png)

> Tesla device ê¸°ì¤€ìœ¼ë¡œëŠ” 0.002058 secì—ì„œ 0.000183 secë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤.

ì´ë³´ë‹¤ ë” thread ìˆ˜(block dimension)ë¥¼ ì¤„ì´ê³ , block ìˆ˜(grid dimension)ë¥¼ ëŠ˜ë¦¬ê²Œ ë˜ë©´ deviceì— ë”°ë¼ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. deviceì— ë”°ë¼ ê° thread hierarchy levelì˜ maximum sizeê°€ ë‹¤ë¥´ë¯€ë¡œ ìœ ì˜í•´ì•¼ í•œë‹¤.

![fermi device error](images/fermi_architecture_grid_dimension_limit.png)

> ì˜ˆë¥¼ ë“¤ì–´ Tesla deviceëŠ” blockì´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” thread ìˆ˜ì˜ í•œê³„ê°€ 1,024ê°œì´ë©°, grid dimension ìˆ˜ì¹˜ëŠ” ê° x,y,z ì°¨ì›ì—ì„œ ìµœëŒ€ 65,535ê¹Œì§€ë§Œ ê°€ëŠ¥í•˜ë‹¤.

---

### 2.8.2 timing with nvprof

CUDA 5.0ë¶€í„° nvprofë¥¼ ì´ìš©í•´ì„œ applicationì˜ CPUì™€ GPU activity timelineì„ ì•Œ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```bash
$ nvprof [nvprof_args] <application> [application_args]
```

> \$ nvprof --help ëª…ë ¹ìœ¼ë¡œ ë” ìì„¸í•˜ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.

ì•ì„  ì˜ˆì œë¥¼ nvprofë¡œ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.

```bash
$ nvprof ./sumArraysOnGPU-timer
```

ìœ„ ëª…ë ¹ì„ ì…ë ¥í•´ì„œ Tesla GPUì—ì„œ ì‚´í´ë³¸ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì ˆë°˜ì€ program outputì„ ë‹´ì€ messageì´ê³ , ë‚˜ë¨¸ì§€ê°€ nvprofì˜ outputì— í•´ë‹¹í•œë‹¤.

![nvprof report 1](images/nvprof_report_1.png)

![nvprof report 2](images/nvprof_report_2.png)

- 2.9024ms: nvprofê°€ ì¸¡ì •í•œ kernel time

nvprofì˜ ì¸¡ì • ê°’ì¸ 2.9024msëŠ” CPU timerë¡œ ì¸¡ì •í•œ kernel timeì¸ 3.26msì™€ ë‹¤ë¥´ë‹¤. ì´ëŠ” CPU timer outputì´ nvprof ì²˜ë¦¬ë¥¼ ìœ„í•´ ë•Œë¬¸ì— ìƒê¸°ëŠ” overheadë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì´ë‹¤.(ë”°ë¼ì„œ nvprofì˜ ê²°ê³¼ê°€ ë” ì •í™•í•˜ë‹¤.)

![nvprof report diagram](images/nvprof_report_diagram.png)

> kernel ìˆ˜í–‰ë³´ë‹¤ë„ hostì™€ device ì‚¬ì´ì—ì„œ ì¼ì–´ë‚˜ëŠ” data transferê°€ í›¨ì”¬ ì‹œê°„ì„ ì¡ì•„ë¨¹ì—ˆë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ:nvprofë¥¼ ì´ìš©í•˜ì—¬ theoretical limitê³¼ ë¹„êµí•˜ê¸°&nbsp;&nbsp;&nbsp;</span>

nvprofë¥¼ ì´ìš©í•´ì„œ applicationì˜ instructionê³¼ memory throughputì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ ê³„ì‚°ì„ theoretical peak value(ì´ë¡ ì ì¸ í•œê³„ê°’)ì™€ ë¹„êµí•˜ë©´, applicationì´ ì–´ë–¤ ì œì•½ì„ ë°›ê³  ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

ê³„ì‚°ì€ Tesla K10ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

- Tesla K10 Peak Single Precision FLOPS

    = 745 MHz core clock * 2 GPUs/board * (8 multiprocessors * 192 fp32 cores/multiprocessor) * 2ops/cycle = 4.58TFLOPS

- Tesla K10 Peak Memory Bandwidth

    = 2 GPUs/board * 256 bit * 2500 MHz mem-clock * 2 DDR / 8bits/byte = 320 GB/s

- Ratio of instruction:bytes

    = 4.58 TFLOPS / 320 GB/s 

    $\rightarrow$ 13.6 instructions:1 byte

ì¦‰, Tesla K10ì€ byte accessë§ˆë‹¤ instruction 13.6ê°œ ì´ìƒì„ **issue**(íˆ¬ì…) ê°€ëŠ¥í•˜ë‹¤.

---

## 2.9 organizing parallel threads

ì•ì„œ vector additionì—ì„œëŠ” kernelì— ì•Œë§ëŠ” grid, block sizeë¥¼ ì§€ì •í•´ì„œ ê³„ì‚°í–ˆë‹¤. ì´ë•Œ grid sizeëŠ” block sizeì™€ vector sizeì— ì˜í•´ ê²°ì •ë˜ì—ˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ matrix additionì€ ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ? ì¼ë°˜ì ìœ¼ë¡œ 2D gridë‚˜ 2D blockì„ ì´ìš©í•´ì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

- 2D grid with 2D blocks

- 1D grid with 1D blocks

- 2D grid with 1D blocks

ì¼ë°˜ì ìœ¼ë¡œ 2D array í˜•íƒœì˜ data(image pixel ë“±)ë¼ë©´, 2D blockìœ¼ë¡œ êµ¬ì„±ëœ 2D gridë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ í¸ë¦¬í•˜ë‹¤. ê°€ë ¹ 76x62 pixelì´ ìˆë‹¤ê³  í•˜ì. ì´ë¥¼ 16x16 threadë¡œ êµ¬ì„±ëœ blockìœ¼ë¡œ ë‚˜ëˆ„ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

![76x62 pixel](images/76_62_pixel.png)

- íšŒìƒ‰ìœ¼ë¡œ ì¹ í•´ì§„ ë¶€ë¶„ì´ pixel data(76x62)ë¥¼ ì˜ë¯¸í•œë‹¤.

- xì¶•ìœ¼ë¡œëŠ” 5ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

- yì¶•ìœ¼ë¡œëŠ” 4ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

- ì¦‰, ì´ pixel dataë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” 5*4 = 20ê°œì˜ blockì´ í•„ìš”í•˜ë‹¤.

> ì—¬ì „íˆ data ìˆ˜ë³´ë‹¤ thread ìˆ˜ê°€ ë” ë§ë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì.

pixel dataë¥¼ Pinì´ë¼ê³  í•œë‹¤ë©´, block(1,0)ì— ìˆëŠ” thread(0,0)ì˜ Pin elementëŠ” ë‹¤ìŒê³¼ ê°™ì´ indexí•  ìˆ˜ ìˆë‹¤.

- blockIdx.y * blockDim.y + threadIdx.y = 1 * 16 + 0 = 16

- blockIdx.x * blockDim.x + threadIdx.x = 0 * 16 + 0 = 0

$$ P_{blockIdx.y * blockDim.y + threadIdx.y, \, blockIdx.x * blockDim.x + threadIdx.x} = P_{16,0} $$

---

### 2.9.1 indexing matrices with blocks and threads

![matrix ì˜ˆì‹œ](images/matrix_ex_1.png)

$8 \times 6$ matrixê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì. matrix addition kernelì—ì„œë„ threadëŠ” ì£¼ë¡œ í•˜ë‚˜ì˜ data elementë¥¼ ì²˜ë¦¬í•œë‹¤. matrix ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ ì •í•´ì•¼ í•œë‹¤.

- thread and block index

- coordinate of a given point in the matrix

- offset in linear global memory

ë‹¤ìŒ ê³¼ì •ì„ í†µí•´ì„œ global memoryì— matrixë¥¼ mappingí•  ìˆ˜ ìˆë‹¤.( $8 \times 6$ ì´ë¯€ë¡œ nx = 8, ny = 6ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. )

1. matrix coordinateì— ë§ì¶°ì„œ threadì™€ block indexë¥¼ ì§€ì •í•œë‹¤.

    - ix = threadIdx.x + blockIdx.x * blockDim.x

    - iy = threadIdx.y + blockIdx.y * blockDim.y

2. global matrixì— ì´ë¥¼ mappingí•œë‹¤.

    - idx = iy * nx + ix

![mapping matrix 1](images/mapping_matrix_1.png)

ì°¸ê³ ë¡œ printThreadInfo() functionì„ ì‚¬ìš©í•˜ë©´ ê´€ë ¨ëœ ì—¬ëŸ¬ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.

- thread index

- block index

- matrix coordinate

- global linear memory offset

- value of corresponding elements

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix element printing&nbsp;&nbsp;&nbsp;</span>

ì´ì œ ë‹¤ìŒê³¼ ê°™ì€ $8 \times 6$ matrixì˜ elementë¥¼ ì¶œë ¥í•´ ë³´ë©° indexë¥¼ í™•ì¸í•  ê²ƒì´ë‹¤. íŒŒì¼ëª…ì€ 'checkThreadIndex.cu'ì´ë‹¤.

![8x6 matrix indices](images/matrix_ex_indices.png)

- 1 blockì€ $4 \times 2$ threadë¡œ êµ¬ì„±ëœë‹¤.

- gridëŠ” $(8 + 4 - 1)/4 \times (6 + 2 - 1)/2$ blockìœ¼ë¡œ êµ¬ì„±ëœë‹¤.( ì¦‰, $3 \times 2$ )

> ë„ì¤‘ì— cudaDeviceProp()ì„ ì´ìš©í•´ device ì •ë³´ë¥¼ queryí•  ê²ƒì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

#define CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort =true) {
    if (code != cudaSuccess) {
        fprintf(stderr, "GPUassert: %s %s %d\n", cudaGetErrorString(code), file, line);
        if (abort)
            exit(code);
    }
}

// hostì—ì„œ matrix elementë¥¼ ì±„ìš¸ function
void initialInt(int *ip, int size) {
    for (int i = 0; i < size; i++) {
        ip[i] = i;
    }
}

// hostì—ì„œ matrix elementë¥¼ printí•  function 
void printMatrix(int *C, const int nx, const int ny) {
    int *ic = C;
    printf("\nMatrix: (%d.%d)\n", nx, ny);
    for (int iy = 0; iy < ny; iy++) {
        for (int ix = 0; ix < nx; ix++) {
            printf("%3d", ic[ix]);
        }
        ic += nx;
        printf("\n");
    }
    printf("\n");
}

// deviceì—ì„œ matrix elementë¥¼ printí•  kernel
__global__ void printThreadIndex(int *A, const int nx, const int ny) {
    int ix = threadIdx.x + blockIdx.x * blockDim.x;
    int iy = threadIdx.y + blockIdx.y * blockDim.y;
    unsigned int idx = iy*nx + ix;

    // printf function ì‚¬ìš©ì— ìœ ì˜
    printf("thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) "
        "global index %2d ival %2d\n", threadIdx.x, threadIdx.y, blockIdx.x,
        blockIdx.y, ix, iy, idx, A[idx]);
}

int main(int argc, char **argv) {
    printf("%s Starting...\n", argv[0]);

    // get device information
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set matrix dimension
    int nx = 8;
    int ny = 6;
    int nxy = nx*ny;
    int nBytes = nxy * sizeof(float);

    // malloc host memory
    int *h_A;
    h_A = (int *)malloc(nBytes);

    // initialize host matrix with integer
    initialInt(h_A, nxy);
    printMatrix(h_A, nx, ny);

    // malloc device memory
    int *d_MatA;
    cudaMalloc((void **)&d_MatA, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);

    // set up execution configuration
    dim3 block(4, 2);
    dim3 grid((nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y);

    // invoke the kernel
    printThreadIndex<<<grid, block>>>(d_MatA, nx, ny);
    cudaDeviceSynchronize();

    // free host and device memory
    cudaFree(d_MatA);
    free(h_A);

    // reset device
    cudaDeviceReset();

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œ ë’¤ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 checkThreadIndex.cu -o checkIndex
$ ./checkIndex
```

![checkThreadIndex](images/checkindex.png)

---

### 2.9.2 summing matrices with 2D grid and 2d blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(2D grid, 2D blocks)&nbsp;&nbsp;&nbsp;</span>

2D gridì™€ 2D blockìœ¼ë¡œ êµ¬ì„±í•œ kernelë¡œ matrix additionì„ ìˆ˜í–‰í•œë‹¤. host sideì—ì„œë„ ë™ì¼í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•œ ë’¤, kernel ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì œëŒ€ë¡œ ìˆ˜í–‰ì´ ëëŠ”ì§€ í™•ì¸í•  ê²ƒì´ë‹¤.

- matrix sizeëŠ” 16,384 elementë¥¼ ê°–ëŠ”ë‹¤.

    ```c
    int nx = 1<<14;
    int ny = 1<<14;
    ```

- 1 blockì€ (32, 32) threadë¥¼ ê°–ëŠ”ë‹¤.

- 1 gridëŠ” blockì„ (nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y ë§Œí¼ ê°–ëŠ”ë‹¤.(512, 512)

ìš°ì„  hostì˜ matrix addition functionì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
// host sideì—ì„œ matrix addition ìˆ˜í–‰
void sumMatrixOnHost(float *A, float *B, float *C, const int nx, const int ny) {
    float *ia = A;
    float *ib = B;
    float *ic = C;

    for (int iy = 0; iy < ny; iy++){
        for (int ix = 0; ix < nx; ix++){
            ic[ix] = ia[ix] + ib[ix];
        }
        ia += nx;
        ib += nx;
        ic += nx;
    }
}
```

deviceì˜ matrix addtion kernelì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
__global__ void sumMatrixOnGPU2D(float *MatA, float *MatB, float *MatC, int nx, int ny){
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
    unsigned int iy = threadIdx.y + blockIdx.y * blockDim.y;
    unsigned int idx = iy*nx + ix;

    if (ix < nx && iy < ny){
        MatC[idx] = MatA[idx] + MatB[idx];
    }
}
```

ì•„ë˜ëŠ” matrix additionì„ ìˆ˜í–‰í•˜ëŠ” sumMatrixOnGPU-2D-grid-2D-block.cu codeì´ë‹¤.

```c
// include ìƒëµ
// error handlingì„ ìœ„í•œ CHECK macro ìƒëµ
// hostì™€ deviceì˜ matrix addition function ìƒëµ(sumMatrixOnHost, sumMatrixOnGPU2D)
// CPU timerì¸ CPUsecond() function ìƒëµ
// matrixì— elementë¥¼ ìƒì„±í•˜ëŠ” initialData() function ìƒëµ
// hostì™€ deviceì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” checkResult() function ìƒëµ

int main(int argc, char** argv){
    printf("%s Starting...\n", argv[0]);

    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    CHECK(cudaGetDeviceProperties(&deviceProp, dev));
    printf("Using Device %d: %s\n", dev, deviceProp.name);
    CHECK(cudaSetDevice(dev));

    // set up data size of matrix
    int nx = 1<<14;
    int ny = 1<<14;

    int nxy = nx*ny;
    int nBytes = nxy * sizeof(float);
    printf("Matrix size: nx %d ny %d\n", nx, ny);

    // malloc host memory
    float *h_A, *h_B, *hostRef, *gpuRef;
    h_A = (float *)malloc(nBytes);
    h_B = (float *)malloc(nBytes);
    hostRef = (float *)malloc(nBytes);
    gpuRef = (float *)malloc(nBytes);

    // initialize data at host side
    double iStart = cpuSecond();
    initialData(h_A, nxy);
    initialData(h_B, nxy);
    double iElaps = cpuSecond() - iStart;

    memset(hostRef, 0, nBytes);
    memset(gpuRef, 0, nBytes);

    // add matrix at host side for result checks
    iStart = cpuSecond();
    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);
    iElaps = cpuSecond() - iStart;

    // malloc device global memory
    float *d_MatA, *d_MatB, *d_MatC;
    cudaMalloc((void **)&d_MatA, nBytes);
    cudaMalloc((void **)&d_MatB, nBytes);
    cudaMalloc((void **)&d_MatC, nBytes);

    // transfer data from host to device
    cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice);

    // invoke kernel at host side
    int dimx = 32;
    int dimy = 32;
    dim3 block(dimx, dimy);
    dim3 grid ((nx + block.x - 1)/block.x, (ny + block.y - 1)/block.y);

    iStart = cpuSecond();
    sumMatrixOnGPU2D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
    cudaDeviceSynchronize();
    iElaps = cpuSecond() - iStart;
    printf("sumMatrixOnGPU<<<(%d,%d), (%d,%d)>>> elapsed %f sec\n",
        grid.x, grid.y, block.x, block.y, iElaps);

    // copy kernel result back to host side
    cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost);

    // check device results
    checkResult(hostRef, gpuRef, nxy);

    // free device global memory
    cudaFree(d_MatA);
    cudaFree(d_MatB);
    cudaFree(d_MatC);

    // free host memory
    free(h_A);
    free(h_B);
    free(hostRef);
    free(gpuRef);

    // reset device
    cudaDeviceReset();

    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œ ë’¤ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-2D-grid-2D-block.cu -o matrix2D
$ ./matrix2D
```

![matrix2D 1](images/matrix2D.png)

block dimensionì„ 32x16ìœ¼ë¡œ í•œ ë’¤(blockì€ 512x1024ê°€ ëœë‹¤.). recompileí•´ì„œ ì‹¤í–‰í•˜ë©´ ì‹œê°„ì€ ì•½ 1/2ë°° ì •ë„ë¡œ ì¤„ì–´ë“ ë‹¤. ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë„ parallelismì´ ë‘ ë°° ëŠ˜ì—ˆê¸° ë–„ë¬¸ì— ì‹œê°„ì´ ì¤„ì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. 

> ìœ„ ì„œìˆ ì€ Tesla device ê¸°ì¤€ìœ¼ë¡œ ì¤„ì–´ë“  ì‹œê°„ì´ë‹¤. 0.060323 secì—ì„œ 0.038041 secë¡œ ì¤„ì–´ë“ ë‹¤.

![matrix2D 2](images/matrix2D_2.png)

ê·¸ëŸ¬ë‚˜ 16x16ìœ¼ë¡œ block dimensionì„ ì§€ì •í•œ ë’¤(blockì€ 1024x1024ê°€ ëœë‹¤.), recompileí•˜ê³  ì‹¤í–‰í•˜ë©´ ì˜¤íˆë ¤ ì‹œê°„ì´ ë” ëŠ˜ì–´ë‚œë‹¤. ì²˜ìŒê³¼ ë¹„êµí•˜ë©´ blockì´ 4ë°°ê°€ ë˜ë©° parallelismì´ ëŠ˜ì—ˆëŠ”ë° ì–´ì§¸ì„œ ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒì¼ê¹Œ? (ch03 ì°¸ì¡°)

![matrix2D 3](images/matrix2D_3.png)

> Fermi deviceì—ì„œ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

| kernel configuration | kernel elapsed time | block number |
| --- | --- | --- |
| (32,32) | 0.060323 sec | 512x512 |
| (32,16) | 0.038041 sec | 512x1024 |
| (16,16) | 0.045535 sec | 1024x1024 |

---

### 2.9.3 summing matrices with a 1D grid and 1D blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(1D grid, 1D blocks)&nbsp;&nbsp;&nbsp;</span>

ì•ì„  2D grid, 2D blocks ì˜ˆì œë¥¼ 1D grid, 1D blocksë¡œ ë³€í˜•í•´ì„œ matrix additionì„ ìˆ˜í–‰í•œë‹¤.(íŒŒì¼ëª…ì€ sumMatrixOnGPU-1D-grid-1D-block.cu)

![mapping matrix 1D grid, 1D blocks](images/mapping_matrix_2.png)

ì´ì™€ ê°™ì´ 1D grid, 1D blockìœ¼ë¡œ êµ¬ì„±í•œë‹¤ë©´, kernelì˜ indexë¥¼ ë‹¤ë¥´ê²Œ êµ¬ì„±í•´ì•¼ í•œë‹¤.(1D blockì€ ì˜¤ì§ threadIdx.xë§Œ index ê³„ì‚°ì— ì“¸ ìˆ˜ ìˆê²Œ ëœë‹¤.)

```c
__global__ void sumMatrixOnGPU1D(float *MatA, float *MatB, float *MatC,
    int nx, int ny) {
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
        
    if (ix < nx) {
        for (int iy  = 0; iy < ny; iy++) {
            int idx = iy*nx + ix;
            MatC[idx] = MatA[idx] + MatB[idx];
        }
    }
}
```

ë‹¤ìŒê³¼ ê°™ì´ block configurationì„ ì„¤ì •í•œë‹¤.

```c
dim3 block(32,1);
dim3 grid ((nx + block.x - 1)/block.x,1);
```

- 1 blockì€ 32ê°œ threadë¥¼ ê°–ëŠ”ë‹¤.

- 1 gridëŠ” (nx + block.x - 1)/block.xê°œì˜ blockì„ ê°–ëŠ”ë‹¤.

kernel ëª…ì¹­ì´ ë°”ë€ ê²ƒì— ì£¼ì˜í•œë‹¤.

```c
sumMatrixOnGPU1D<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
```

ì‘ì„±ì„ ì™„ë£Œí•˜ë©´ compileí•´ì„œ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-1D-grid-1D-block.cu -o matrix1D
$ ./matrix1D
```

![matrix1D](images/matrix1D.png)

ì‚¬ì‹¤ ê²°ê³¼ë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” 2D grid, 2D block (32x32)ê³¼ êµ¬ì¡°ì ìœ¼ë¡œ ì°¨ì´ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ ì—°ì‚° ì‹œê°„ì—ì„œ ì°¨ì´ë¥¼ ë³´ì¸ë‹¤.

block(128,1)ë¡œ ë³€ê²½í•´ì„œ ìˆ˜í–‰ì„ í•˜ë©´ ì¡°ê¸ˆ ë” ë¹¨ë¼ì§€ëŠ” ëª¨ìŠµì„ ë°œê²¬í•  ìˆ˜ ìˆë‹¤.

```
dim3 block(128,1);
dim3 grid ((nx + block.x - 1)/block.x,1);
```

![matrix1D 2](images/matrix1D_2.png)

> Tesla device ê¸°ì¤€ìœ¼ë¡œ 0.061352 secì—ì„œ 0.044701 secë¡œ ë¹¨ë¼ì§„ë‹¤.

---

### 2.8.4 summing matrices with a 2D grid and 1D blocks

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: matrix addition(2D grid, 1D blocks)&nbsp;&nbsp;&nbsp;</span>

ì•ì„  2D grid, 2D blocks ì˜ˆì œë¥¼ 2D grid, 1D blocksë¡œ ë³€í˜•í•´ì„œ matrix additionì„ ìˆ˜í–‰í•œë‹¤.(íŒŒì¼ëª…ì€ sumMatrixOnGPU-2D-grid-1D-block.cu)

![mapping matrix 2D grid, 1D blocks](images/mapping_matrix_3.png)

ì´ ê²½ìš° matrix coordinate mapping(ix, iy)ì´ ë‹¤ìŒê³¼ ê°™ì´ ë°”ë€ë‹¤.

```c
ix = threadIdx.x + blockIdx.x * blockDim.x;
iy = blockIdx.y;
```

ë‹¤ìŒê³¼ ê°™ì´ blockê³¼ grid sizeë¥¼ ì§€ì •í•œë‹¤.

```c
dim3 block(32);
dim3 grid((nx + block.x - 1)/block.x,ny);
```

global linear memory offsetì€ ë§ˆì°¬ê°€ì§€ë‹¤. ë³€ê²½ ì‚¬í•­ì„ ë°˜ì˜í•œ kernelì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

> ì‚¬ì‹¤ ì•„ë˜ì™€ ê°™ì´ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ sumMatrixOnGPU2Dë¡œ ë™ì‘ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ëŒ€ì‹  ë°”ê¾¼ kernelì´ threadë§ˆë‹¤ integer multiplication ì—°ì‚° 1ê°œ, integer addition ì—°ì‚° 1ê°œë¥¼ ëœ ìˆ˜í–‰í•˜ëŠ” ì´ì ì„ ê°–ëŠ”ë‹¤.

```c
__global__ void sumMatrixOnGPUMix(float *MatA, float *MatB, float *MatC,
    int nx, int ny) {
    unsigned int ix = threadIdx.x + blockIdx.x * blockDim.x;
    unsigned int iy = blockIdx.y;
    unsigned int idx = iy*nx + ix;

    if (ix < nx && iy < ny){
        MatC[idx] = MatA[idx] + MatB[idx];
    }
}
```

kernel invokeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
sumMatrixInGPUMix<<<grid, block>>>(d_MatA, d_MatB, d_MatC, nx, ny);
```

compileí•˜ê³  ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -arch=sm_80 sumMatrixOnGPU-2D-grid-1D-block.cu -o mat2D1D
$ ./mat2D1D
```

![mat2D1D](images/mat2D1D.png)

ì°¸ê³ ë¡œ block sizeë¥¼ 256ìœ¼ë¡œ ëŠ˜ë¦¬ë©´ ì‹œê°„ì´ ë” ê°ì†Œí•œë‹¤. 

![mat2D1D 2](images/mat2D1D_2.png)

> Tesla ê¸°ì¤€ìœ¼ë¡œ 0.073727 secì—ì„œ 0.030765 secê°€ ëœë‹¤.

<br/>

ì•„ë˜ëŠ” Fermi deviceì—ì„œ grid, block dimensionì„ ë°”ê¾¸ë©° ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ì •ë¦¬í•œ í‘œë‹¤.

| kernel | execution comfigure | time elapsed |
| --- | --- | --- |
| sumMatrixOnGPU2D | (512, 1024), (32,16) | 0.038041 |
| sumMatrixOnGPU1D | (128, 1), (128,1) | 0.044701 |
| sumMatrixOnGPUMix | (64, 16384), (256,1) | 0.030765 |

ì´ë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. execution configurationì„ ë°”ê¾¸ë©´ performanceì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤.

2. ì¼ë°˜ì ìœ¼ë¡œ ìµœì ì˜ configurationì„ ì°¾ì§€ ì•Šìœ¼ë©´ best performanceì„ ì–»ì„ ìˆ˜ ì—†ë‹¤.

3. gridì™€ block dimensionì„ ë°”ê¾¸ëŠ” ê²ƒìœ¼ë¡œë„ performanceë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

---