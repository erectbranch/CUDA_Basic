# 3 CUDA Execution Model

Fermi, Kepler architecture ì„¤ëª… ìƒëµ

---

## 3.1 understanding the nature of warp execution

**warp**ë€ thread 32ê°œë¥¼ ë¬¶ì–´ì„œ ì§€ì¹­í•˜ëŠ” single execution unitì´ë‹¤. warpì€ SMì—ì„œ ìˆ˜í–‰í•˜ëŠ” executionì˜ basic unitì´ë‹¤. thread blockì´ SMì— scheduleë˜ë©´, thread block ì•ˆì˜ threadë“¤ì€ warpë¡œ partitionëœë‹¤. 

32 consecutive threadë“¤ë¡œ êµ¬ì„±ëœ warp ë‚´ë¶€ëŠ” SIMT ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤. ë‹¤ì‹œ ë§í•´ wrap ë‚´ë¶€ì˜ ëª¨ë“  threadê°€ ë™ì¼í•œ instructionì„ ì‹¤í–‰í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

ì•ì„œ thread blockì„ 3ì°¨ì›ê¹Œì§€ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, hardware ê´€ì ì—ì„œëŠ” ê²°êµ­ ëª¨ë‘ 1ì°¨ì›ìœ¼ë¡œ ë°°ì •ëœë‹¤. blockDimê³¼ blockIdxì„ ì´ìš©í•´ì„œ block ë‹¹ warp ê°œìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

- 2D thread blockì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ threadë§ˆë‹¤ unique identifierê°€ ìƒì„±ë˜ì—ˆë‹¤.

    threadIdx.y * blockDim.x + threadIdx.x

- 3D thread block

    threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x

- thread blockë‹¹ warp ê°œìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ WarpsPerBlock = ceil\left( {{ThreadsPerBlock} \over {warpSize}} \right) $$

wrapëŠ” hardwareë§ˆë‹¤ êµ¬ì²´ì ì¸ ìˆ«ìë¥¼ ê°€ì§€ê³ , ëª¨ë“  thread blockì—ì„œ ê°™ì€ sizeì˜ ë‹¨ìœ„ë¡œ ì‚¬ìš©ëœë‹¤. ë§Œì•½ thread block sizeê°€ warp sizeì˜ ë°°ìˆ˜ê°€ ì•„ë‹ˆë¼ë©´, ë§ˆì§€ë§‰ warpì— ì†í•˜ëŠ” threadë“¤ì€ inactiveí•˜ê²Œ ë‚¨ê²Œ ëœë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ applicationì—ì„œ x dimension 40ê°œì˜ threadë¥¼ ê°–ê³ , y dimensionì´ 2ê°œì˜ threadë¥¼ ê°–ëŠ” 2D thread blockìœ¼ë¡œ êµ¬ì„±í–ˆë‹¤ê³  í•˜ì.(40x2=80 threads laid out)

ê·¸ë ‡ë‹¤ë©´ hardware ì…ì¥ì—ì„œëŠ” warp 3ê°œë¥¼ allocateí•  ê²ƒì´ë‹¤. ë‹¤ì‹œ ë§í•´ 80ê°œ threadë¥¼ supportí•˜ê¸° ìœ„í•´ 96ê°œì˜ threadê°€ allocateë˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë§ˆì§€ë§‰ warpì—ì„œ inactiveí•œ threadê°€ ìƒê¸°ê²Œ ëœë‹¤. ì´ëŸ° threadë“¤ì€ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì§€ë§Œ, ì—¬ì „íˆ SM resource(ì˜ˆë¥¼ ë“¤ë©´ register)ë¥¼ ì¡ì•„ë¨¹ê²Œ ëœë‹¤.

![thread block & warp](images/thread_block_and_warp_ex.png)

---

## 3.2 warp divergence

programì—ì„œ ì‹¤í–‰ë˜ëŠ” ê° statement, instruction, function callì„ ì œì–´í•˜ëŠ” **control flow**(ì œì–´ íë¦„)ì€ high-level programming languageì—ì„œì˜ ê¸°ë³¸ì ì¸ êµ¬ì„± ìš”ì†Œì´ë‹¤. GPUë„ ë§ˆì°¬ê°€ì§€ë¡œ 'if...then...else, for, while'ê³¼ ê°™ì€ C ìŠ¤íƒ€ì¼ì˜ flow-control constructë¥¼ ì œê³µí•œë‹¤.

CPUëŠ” **branch prediction**(ë¶„ê¸° ì˜ˆì¸¡)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë³µì¡í•œ hardwareë¥¼ í¬í•¨í•œë‹¤. predictê°€ ì˜¬ë°”ë¥´ë©´ CPUëŠ” ì˜¤ì§ small performance penaltyë§Œ ë¶€ë‹´í•˜ë©´ ëœë‹¤. í•˜ì§€ë§Œ predictê°€ í‹€ë ¸ë‹¤ë©´, CPUëŠ” ì—¬ëŸ¬ instruction pipelineìœ¼ë¡œ êµ¬ì„±ëœ cycleì´ stallëœë‹¤.  

ê·¸ëŸ¬ë‚˜ GPUëŠ” branch prediction mechanismì„ ê°€ì§€ì§€ ì•Šì€ simpleí•œ deviceì´ë‹¤. warpì˜ ëª¨ë“  threadê°€ ë™ì¼í•œ instructionì„ executeí•˜ëŠ” ì ì„ ìƒê°í•´ ë³´ì. ë§Œì•½ warpì˜ í•œ threadê°€ ì–´ëŠ instructionì„ ì‹¤í–‰í•˜ê²Œ ë˜ë©´, warp ë‚´ì˜ ëª¨ë“  threadëŠ” ê°™ì€ instructionë§Œ executeí•´ì•¼ í•œë‹¤.

ì´ëŸ° íŠ¹ì„±ì´ applicationì—ì„œì˜ ë¶„ê¸°ì—ì„œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ statementê°€ ìˆë‹¤ê³  í•˜ì.

```c
if (cond) {
    //...
} else {
    //...
}
```

ì´ instructionë¥¼ í•œ warpì—ì„œ 16ê°œëŠ” cond(condition)ì´ true, 16ê°œëŠ” else blockì„ ì²˜ë¦¬í•œë‹¤ê³  ê°€ì •í•˜ì. ê·¸ë ‡ë‹¤ë©´ í•˜ë‚˜ì˜ warpì—ì„œ ë‹¤ë¥¸ instructionë“¤ì„ ì²˜ë¦¬í•˜ê²Œ ë˜ëŠ” ì…ˆì´ë‹¤. ì´ë¥¼ **warp divergence**ë¼ê³  í•œë‹¤. 

> ì•ì„œ í•œ warpê°€ ê°™ì€ instructionì„ ìˆ˜í–‰í•œë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ, warp divergenceëŠ” paradoxë¡œ ë³´ì¼ ìˆ˜ ìˆë‹¤.

warp diverge ìƒí™©ì—ì„œ warpëŠ” serialí•˜ê²Œ ê° branch pathë¥¼ 'í•´ë‹¹ pathê°€ ì•„ë‹Œ ë‹¤ë¥¸ pathë¥¼ ë§¡ì€ threadë¥¼ disableí•˜ê²Œ ë§Œë“  í›„' executeí•œë‹¤. ë”°ë¼ì„œ warp divergenceëŠ” performanceë¥¼ êµ‰ì¥íˆ ê°ì†Œì‹œí‚¤ê²Œ ëœë‹¤. 

ì•ì„œ í•œ ê°€ì •ì—ì„œëŠ” warpì˜ parallelismì€ ì ˆë°˜(16ê°œ)ë¡œ ë‚˜ë‰˜ì—ˆë‹¤. ì´ ì˜ˆì‹œë¼ë©´ 16ê°œ threadê°€ executeí•˜ëŠ” ìƒí™©ì—ì„œëŠ”, ë‹¤ë¥¸ 16ê°œ threadê°€ disableëœë‹¤.

> ë”°ë¼ì„œ conditional branchê°€ ë§ì„ìˆ˜ë¡, parallelismìœ¼ë¡œ ìƒëŠ” ì†ì‹¤ì´ ë”ìš± ì»¤ì§€ê²Œ ëœë‹¤.

> ì£¼ì˜í•  ì ì€ branch divergenceëŠ” ë‹¨ì¼ warpì—ì„œ ì¼ì–´ë‚˜ëŠ” í˜„ìƒì´ë¼ëŠ” ì ì´ë‹¤.

ì•„ë˜ëŠ” if...then statementë¥¼ ì²˜ë¦¬í•˜ëŠ” í•œ warp ë‚´ì—ì„œ ì¼ì–´ë‚˜ëŠ” warp divergenceë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.(yì¶•ì€ ì‹œê°„ì˜ ê²½ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.) 

![warp divergence](images/warp_divergence.png)

- threadë“¤ì€ conditionì´ trueì¼ ë•Œì˜ execution, falseì¼ ë•Œì˜ executionì„ ë‚˜ëˆ ì„œ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.

- ê·¸ëŸ¬ë‚˜ condition = trueë¥¼ ë§Œì¡±í•˜ëŠ” blockì„ executeí•˜ê³  ìˆë‹¤ë©´, ì´ë¥¼ ë§¡ì§€ ì•ŠëŠ” threadê°€ ëë‚  ë•Œê¹Œì§€ stallëœë‹¤.

- ì‚´êµ¬ìƒ‰: if clause(ì ˆ)ì„ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆ, than clauseë¥¼ ìˆ˜í–‰í•˜ëŠ” threadë“¤ì€ stallëœë‹¤.(ë³´ë¼ìƒ‰)

- ì—°ë‘ìƒ‰: then clauseë¥¼ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆ, if clauseë¥¼ ìˆ˜í–‰í•˜ëŠ” threadë“¤ì€ stallëœë‹¤.(ë³´ë¼ìƒ‰)

ë”°ë¼ì„œ best performanceë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ”, ë‹¨ì¼ warpì—ì„œ ë‹¤ë¥¸ execution pathê°€ ìƒê¸°ëŠ” ì¼ì„ í”¼í•´ì•¼ í•œë‹¤. warp assignmentëŠ” **deterministic**(í•¨ìˆ˜ì™€ ê°™ì´ inputì„ ì„¤ì •í•˜ë©´, ì–¸ì œë‚˜ ê·¸ì— ë§ëŠ” outputì„ ë‚¼ ìˆ˜ ìˆë‹¤.)í•˜ë‹¤ëŠ” ì ì„ ëª…ì‹¬í•˜ì. ëª¨ë“  warpê°€ ê°ê° same control pathì„ ë§¡ê²Œ dataë¥¼ partitioní•  ìˆ˜ ìˆë‹¤.


branch ë‘ ê°œë¥¼ ê°–ëŠ” ë‹¤ìŒ simple arithmetic kernel ì˜ˆì‹œë¥¼ ë³´ì. 

```c
__global__ void mathKernel1(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if (tid % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}
```

- condition (tid % 2 == 0)ë¡œ ì„¤ì •í•˜ë©´ ì§ìˆ˜ index threadë“¤ì´ if clause, í™€ìˆ˜ indexì˜ threadë“¤ì´ else clauseë¥¼ ë§¡ê²Œ ëœë‹¤.

- ë”°ë¼ì„œ warp divergenceê°€ ë°œìƒí•œë‹¤.

ì´ë•Œ dataë¥¼ **interleave**(ì¸í„°ë¦¬ë¸Œ. dataê°€ ì„œë¡œ ì¸ì ‘í•˜ì§€ ì•Šë„ë¡ ë°°ì—´í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•)í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ warp divergenceë¥¼ í”¼í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒ ì˜ˆì‹œë¥¼ ë³´ì.

- CUDAì˜ built-in variableì¸ warpSizeë¥¼ ì´ìš©í•œë‹¤.

```c
__global__ void mathKernel2(void) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if ((tid / warpSize) % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}
```

- condition (tid/warpSize)%2==0)ì€ warp sizeì— ë§ì¶°ì„œ **branch granularity**(ì„¸ë¶„ì„±)ë¥¼ ê°–ë„ë¡ ê°•ì œí•œë‹¤.

- even warpëŠ” if clauseë¥¼ ë‹´ë‹¹í•˜ê³ , odd warpëŠ” else clauseë¥¼ ë‹´ë‹¹í•œë‹¤.

ë‹¤ìŒì€ ì´ ë‘ ì˜ˆì‹œì˜ performanceë¥¼ ë¹„êµí•˜ëŠ” ì½”ë“œì´ë‹¤.(íŒŒì¼ëª…ì€ simpleDivergence.cu)

```c
#include <cuda_runtime.h>
#include <stdio.h>
#include <sys/time.h>

double seconds() {
    struct timeval tp;
    gettimeofday(&tp, NULL);
    return ((double)tp.tv_sec + (double)tp.tv_usec*1.e-6);
}

// kernel causing wrap divergence
__global__ void mathKernel1(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if (tid % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}

// avoid wrap divergence
__global__ void mathKernel2(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a, b;
    a = b = 0.0f;

    if ((tid / warpSize) % 2 == 0) {
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}

__global__ void mathKernel3(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    bool ipred = (tid % 2 == 0);

    if (ipred) {
        ia = 100.0f;
    }

    if (!ipred) {
        ib = 200.0f;
    }
    c[tid] = ia + ib;
}

__global__ void mathKernel4(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    int itid = tid >> 5;

    if (itid & 0x01 == 0) {
        ia = 100.0f;
    } else {
        ib = 200.0f;
    }
    c[tid] = ia + ib;
}

__global__ void warmingup(float *c) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    if ((tid / warpSize) % 2 == 0){
        ia = 100.0f;
    } else {
        ib = 200.0f;
    }
    c[tid] = ia + ib;
}

int main(int argc, char **argv) {
    // set up device
    int dev = 0;
    cudaDeviceProp deviceProp;
    cudaGetDeviceProperties(&deviceProp, dev);
    printf("%s using Device %d: %s\n", argv[0], dev, deviceProp.name);

    // set up data size
    int size = 64;
    int blocksize = 64;
    if (argc > 1) blocksize = atoi(argv[1]);   // atoi: char to int
    if (argc > 2) size      = atoi(argv[2]);
    printf("Data size %d ", size);

    // set up execution configuration
    dim3 block (blocksize,1);
    dim3 grid  ((size + block.x -1)/block.x,1);
    printf("Execution Configure (block %d, grid %d)\n", block.x, grid.x);

    // allocate GPU memory
    float *d_C;
    size_t nBytes = size * sizeof(float);
    cudaMalloc((float**)&d_C, nBytes);

    // run a warmup kernel to remove overhead
    double iStart, iElaps;
    cudaDeviceSynchronize();
    iStart = seconds();
    warmingup<<<grid,block>>>(d_C);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    printf("warmup      <<< %4d %4d >>> elapsed %f sec \n", grid.x, block.x, iElaps);

    // run kernel 1
    iStart = seconds();
    mathKernel1<<<grid,block>>>(d_C);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    printf("mathKernel1 <<< %4d %4d >>> elapsed %f sec \n", grid.x, block.x, iElaps);

    // run kernel 2
    iStart = seconds();
    mathKernel2<<<grid,block>>>(d_C);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    printf("mathKernel2 <<< %4d %4d >>> elapsed %f sec \n", grid.x, block.x, iElaps);

    // run kernel 3
    iStart = seconds();
    mathKernel3<<<grid,block>>>(d_C);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    printf("mathKernel3 <<< %4d %4d >>> elapsed %f sec \n", grid.x, block.x, iElaps);

    // run kernel 4
    iStart = seconds();
    mathKernel4<<<grid,block>>>(d_C);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    printf("mathKernel4 <<< %4d %4d >>> elapsed %f sec \n", grid.x, block.x, iElaps);

    // free GPU memory and reset device
    cudaFree(d_C);
    cudaDeviceReset();
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•œ ë’¤ ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc -O3 -arch=sm_80 simpleDivergence.cu -o simpleDivergence
$ ./simpleDivergence
```

> -O optionì€ optimization levelì„ ëœ»í•œë‹¤. 

![simpleDivergence](images/simpleDivergence.png)

ë˜í•œ capability 7.5 ë¯¸ë§Œì¸ deviceì—ì„œëŠ” nvprof profilerë¥¼ ì‚¬ìš©í•´ì„œ warp divergenceë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.

> capability 7.5 ì´ìƒì˜ deviceëŠ” NVIDIA Nsightë¥¼ ì‚¬ìš©í•´ì„œ profileí•˜ê¸°ë¥¼ ê¶Œì¥í•˜ê³  ìˆë‹¤.

```bash
$ nvprof --metrics branch_efficiency ./simpleDivergence
```

branch efficiencyëŠ” ì „ì²´ branchì—ì„œ non-divergent branchê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤.

$$ Branch \, Efficiency = 100 \times \left( {Branches - Divvergent \, Branches} \over {Branches} \right) $$

ë‹¤ìŒì€ Tesla deviceì—ì„œ mathKernel1ê³¼ mathKernel2ë¥¼ profilingì„ ì§„í–‰í•œ ê²°ê³¼ì´ë‹¤.

![nvprof branch_efficiency](images/Tesla_nvprof_profiling.png)

ê·¸ëŸ°ë° ê²°ê³¼ë¥¼ ë³´ë©´ branch divergenceê°€ ì—†ë‹¤ê³  ë‚˜ì˜¨ë‹¤. ì´ëŸ° ê²°ê³¼ê°€ ë‚˜ì˜¨ ì´ìœ ëŠ” CUDA compilerê°€ warp divergenceê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” branch instructionë“¤ì„ optimizationí–ˆê¸° ë•Œë¬¸ì´ë‹¤.

ì—¬ê¸°ì„œ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆë‹¤. branch predicationì—ì„œ conditionì€ 1 í˜¹ì€ 0ìœ¼ë¡œ ì„¤ì •ëœë‹¤. ê·¸ë¦¬ê³  ë‘ ê°€ì§€ conditional flow pathê°€ ëª¨ë‘ executeëœë‹¤. ê²°ê³¼ì ìœ¼ë¡œ predicate(ë…¼ë¦¬ ì—¬ë¶€)ê°€ 1ì¸ instructionë“¤ì´ ì‹¤í–‰ë˜ì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  í•´ì„œ predicateê°€ 0ì— í•´ë‹¹í•˜ëŠ” instructionì„ ë§¡ì€ threadê°€ stallë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.

> compilerëŠ” ì¡°ê±´ë¬¸ ìˆ˜ê°€ íŠ¹ì • thresholdë³´ë‹¤ ì ì€ ê²½ìš°ì—ë§Œ optimizationì„ ì™„ë²½í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

ë”°ë¼ì„œ code pathê°€ ê¸¸ë‹¤ë©´ warp divergenceë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆë‹¤. ì•„ë˜ëŠ” Tesla deviceì—ì„œ mathKernel3ë¥¼ ì¶”ê°€í•˜ê³  profilingí•œ ê²°ê³¼ì´ë‹¤.

![nvprof branch_efficiency](images/Tesla_nvprof_profiling_2.png)

ì²˜ìŒë¶€í„° warp divergenceê°€ ìƒê¸°ì§€ ì•Šë„ë¡ êµ¬ì„±í•œ mathKernel2ì™€ ë‹¤ë¥´ê²Œ, mathKernel1ê³¼ 3ëŠ” limited optimizationì´ ìˆ˜í–‰ëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 

---

## 3.3 Resource Partitioning

warpë¥¼ resource ê´€ì ì—ì„œ ë³¼ ë•Œ, ë‹¤ìŒ 3ê°€ì§€ resourceë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤.

- Program counters

- Registers

- Shared memory

SM ê°ê°ì€ thread ì‚¬ì´ë¡œ register fileì´ ë“  32-bit register setì„ ê°€ì§€ê³  ìˆë‹¤. ë˜í•œ block ì‚¬ì´ë¡œ ì •í•´ì§„ ìˆ˜ëŸ‰ì˜ shared memoryë¥¼ ê°€ì§€ê³  ìˆë‹¤.

ë”°ë¼ì„œ ê³µê°„ì— registerë‚˜ shared memoryë¥¼ ëœ ë°°ì¹˜í•˜ë©´, ë” ë§ì€ threadë‚˜ blockì´ simultaneousí•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

![fewer vs lower register](images/fewer_more_register.png)

![fewer vs lower shared memory](images/fewer_more_shared_memory.png)

ë”°ë¼ì„œ SMë§ˆë‹¤ ìˆì„ ìˆ˜ ìˆëŠ” threadì˜ ìˆ˜ëŠ” resourceì— ì˜í•´ ì œì•½ì„ ë°›ëŠ”ë‹¤. í•˜ì§€ë§Œ registerë‚˜ shared memoryëŠ” compute capabilityì— í•„ìˆ˜ì ì´ë¯€ë¡œ, ì´ resourceê°€ ë¶€ì¡±í•˜ë©´ ë°˜ëŒ€ë¡œ kernel launchì— ì‹¤íŒ¨í•  ìˆ˜ ìˆë‹¤.

allocateëœ thread blockì„ **active** blockì´ë¼ê³  ì§€ì¹­í•œë‹¤. ì´ block ë‚´ë¶€ì— ìˆëŠ” warpë¥¼ active warpsë¼ ì§€ì¹­í•œë‹¤. active warpëŠ” ë‹¤ìŒ ì„¸ ê°€ì§€ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.

- selected warp: ì‹¤ì œë¡œ executing ì¤‘ì¸ warp

- stalled warp: ready(ì¤€ë¹„)ê°€ ì•ˆ ëœ warp

- eligible warp: ready ìƒíƒœì¸ warp

readyëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ ê°€ëŠ¥í•˜ë‹¤.

- CUDA core 32ê°œ ëª¨ë‘ executeê°€ ê°€ëŠ¥í•˜ë‹¤.

- í•´ë‹¹ instructionì˜ ëª¨ë“  argumentê°€ ì¤€ë¹„ë˜ì—ˆë‹¤.

> SMì˜ warp schedulerë“¤ì€ ë§¤ cycleë§ˆë‹¤ active warpë¥¼ ì„ íƒí•˜ê³  dispatch(ì¤€ë¹„ì—ì„œ ì‹¤í–‰ ìƒíƒœë¡œ ë³€ê²½)í•œë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´ Kepler SMì—ì„œ concurrentí•œ active warpì˜ ìˆ˜ëŠ” architecture limitì¸ 64ê°œë¥¼ ë„˜ì„ ìˆ˜ ì—†ë‹¤. ë˜í•œ í•œ cycleë‹¹ selected warpëŠ” 4ê°œ ì´í•˜ë§Œ ê°€ëŠ¥í•˜ë‹¤. ë§Œì•½ ì–´ë–¤ warpê°€ stallë˜ë©´, warp schedulerëŠ” ì´ë¥¼ ëŒ€ì‹ í•˜ê¸° ìœ„í•´ eligible warpë¥¼ ê³ ë¥¸ë‹¤.

ì´ë ‡ê²Œ warp contextsë¥¼ switchingí•˜ëŠ” ê³¼ì •ì€ êµ‰ì¥íˆ ë¹ ë¥´ë‹¤.(warp ì‚¬ì´ë¡œ compute resourceë“¤ì´ ë°°ì¹˜ë˜ì–´ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì.) ë•ë¶„ì— warp ìˆ˜ë§Œ ì¶©ë¶„í•˜ë‹¤ë©´ warp stallë¡œ ìƒê¸°ëŠ” latencyë¥¼ ìˆ¨ê¸¸ ìˆ˜ ìˆë‹¤.

---

## 3.4 latency hiding

ê° instruction ì‚¬ì´ì˜ latencyëŠ” ë‹¤ë¥¸ resident warpì— ë‹¤ë¥¸ instructionì„ issuingí•˜ëŠ” ê²ƒìœ¼ë¡œ ê°ì¶œ ìˆ˜ ìˆë‹¤.

í•œ ë²ˆì— í•˜ë‚˜ì—ì„œ ë‘ threadë¥¼ ì‚¬ìš©í•˜ë©° latencyë¥¼ minimizeí•˜ê²Œ ë””ìì¸ëœ CPU coreì™€ ë‹¤ë¥´ê²Œ, GPUëŠ” throughputì„ maximizeí•˜ê¸° ìš°ã…£í•´ ë§ì€ ìˆ˜ì˜ concurrentí•˜ê³  lightweightí•œ threadë“¤ì„ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ CUDA programmingì—ì„œ latency hidingì´ ë” ì¤‘ìš”í•  ìˆ˜ë°–ì— ì—†ë‹¤.

instruction latencyëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ instruction ë¶„ë¥˜ì—ì„œ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤.

- arithmetic instructions

    arithmetic instruction latencyëŠ” ë§ ê·¸ëŒ€ë¡œ arithmetic operationì´ ì‹œì‘í•´ì„œ ëë‚  ë•Œê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì˜ë¯¸í•œë‹¤.

    - ëŒ€ì²´ë¡œ 10~20 cycle

- memory instructions

    load í˜¹ì€ store operationì´ issueëœ ì‹œì ë¶€í„° dataê°€ destinationì— ë„ë‹¬í•˜ê¸°ê¹Œì§€ì˜ ì‹œê°„ì´ë‹¤.

    - (global memory access) ëŒ€ì²´ë¡œ 400~800 cycle

ë‹¤ìŒ ì˜ˆì œë¥¼ ë³´ì. warp schedulerëŠ” wrap 0ê°€ stallë˜ë©´, ë‹¤ë¥¸ warpë“¤ì„ ê³¨ë¼ executeì‹œí‚¨ ë’¤ warp 0ì´ ë‹¤ì‹œ eligibleë  ë•Œ executeí•œë‹¤.

![warp 0 stall](images/warp_stall_ex.png)

ê·¸ë ‡ë‹¤ë©´ latency hidingì„ ìœ„í•´ì„œ ì–¼ë§ˆë‚˜ active warpê°€ í•„ìš”í• ê¹Œ? **Little's Law**(ë¦¬í‹€ì˜ ë²•ì¹™)ì„ GPUì— ì ìš©í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ Number \, of \, Required \, Warps = Latency \, \times \, Throughput $$

![Little's Law](images/littles_law.png)

> bandwidthì™€ throughputì„ í˜¼ë™í•˜ì§€ ë§ì. ë¬¼ë¡  ë‘˜ì€ ì¢…ì¢… í˜¼ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆê³  ì–‘ìª½ ë‹¤ performanceë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì§€í‘œì´ë‹¤. bandwidthê°€ ì´ë¡ ì ì¸ peak valueë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ìœ¼ë¡œ ì£¼ë¡œ ì“°ì¸ë‹¤ë©´, throughputì€ ì‹¤ì œ achieved valueë¥¼ ë§í•œë‹¤.

> ë”°ë¼ì„œ bandwidthê°€ ë‹¨ìœ„ ì‹œê°„ë‹¹ data transferì˜ ê°€ì¥  highestí•œ ì–‘ì„ ì˜ë¯¸í•œë‹¤ë©´, throughputì€ ë‹¨ìœ„ ì‹œê°„ë‹¹ operationì´ ìˆ˜í–‰ëœ ì–‘(ì˜ˆë¥¼ ë“¤ë©´ cycleë‹¹ ì™„ë£Œëœ instruction)ì„ ì˜ë¯¸í•œë‹¤.

ìš°ì„  arithmetic operationì—ì„œëŠ” latency hidingì„ ìœ„í•´ í•„ìš”í•œ operationì˜ ê°œìˆ˜ê°€ required parallelismì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ Fermiì™€ Kepler deviceì—ì„œì˜ í•„ìš”í•œ operation ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ í‘œë‹¤.

| GPU model | instruction latency(cycles) | throughput | parallelism(operations) |
| --- | --- | --- | --- |
| Fermi | 20 | 32 | 640 |
| Kepler | 20 | 192 | 3840 |

- ì˜ˆì‹œëŠ” 32-bit floating-point multiply-add ì—°ì‚° ê¸°ì¤€ì´ë‹¤.

ì´ë¥¼ ë‹¤ì‹œ warp sizeë¡œ ë‚˜ëˆ„ë©´ SMë‹¹ í•„ìš”í•œ warpì˜ ê°œìˆ˜ë„ ì•Œ ìˆ˜ ìˆë‹¤. ê°€ë ¹ ìœ„ ì˜ˆì‹œì—ì„œ Fermi GPUë¼ë©´ 640/32 = 20 warpsê°€ ëœë‹¤.

ì •ë¦¬í•˜ìë©´ required parallelismì€ í•„ìš”í•œ operationì˜ ê°œìˆ˜ë‚˜, warpì˜ ê°œìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. 

> ì°¸ê³ ë¡œ parallelismì„ ëŠ˜ë¦´ ìˆ˜ ìˆëŠ” ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ, threadì— independentí•œ instructionì„ ëŠ˜ë¦¬ëŠ” **ILP**(Instruction-level parallelism)ê³¼ concurrentí•œ eligible warpë¥¼ ëŠ˜ë¦¬ëŠ” **TLP**(Thread-level parallelism)ì´ ìˆë‹¤. 

ë‹¤ìŒìœ¼ë¡œ memory operationì—ì„œëŠ” latency hidingì„ ìœ„í•´ cycleë‹¹ í•„ìš”í•œ bytes ìˆ˜ë¥¼ required parallelismì´ë¼ í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ Fermiì™€ Kepler deviceì˜ ì˜ˆì‹œë‹¤.

| GPU model | (memory) instruction latency(cycles) | bandwidth(GB/sec) | bandwidth(B/cycle) | parallelism(KB) |
| --- | --- | --- | --- | --- |
| Fermi | 800 | 144 | 92 | 74 |
| Kepler | 800 | 250 | 96 | 77 |

> memory throughputì´ ì£¼ë¡œ GB/secë¡œ í‘œí˜„ë˜ëŠ”ë°, nvidia-smië¥¼ ì´ìš©í•´ì„œ ì´ ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì„œ í‘œì‹œí•  ìˆ˜ë„ ìˆë‹¤.

```bash
$ nvidia-smi -a -q -d CLOCK | fgrep -A 3 "Max Clocks" | fgrep "Memory"
```

ì˜ˆë¥¼ ë“¤ì–´ Fermi memory frequencyëŠ” 1.566 GHz(Tesla C2070 ì¸¡ì •)ì´ë‹¤. ê·¸ë¦¬ê³  Kepler memory frequencyëŠ” 1.6 GHz(Tesla K20 ì¸¡ì •)ì´ë‹¤. ì´ë•Œ 1HzëŠ” **cycle per second**ë¡œ ì •ì˜ë˜ë¯€ë¡œ, bandwidth(GB/sec)ì— ì´ë¥¼ ë‚˜ëˆ„ë©´ Bytes/Cycle bandwidthë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$ 144 GB/sec \div 1.566 GHz \cong 92 Bytes/Cycle $$

ì´ë ‡ê²Œ êµ¬í•œ bandwidth(bytes/cycle)ì™€ memory latencyë¥¼ ê³±í•´ì„œ required parallelismì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Fermi deviceë¼ë©´ ê²°ê³¼ê°€ ì•½ 74KBë¡œ ë‚˜ì˜¤ê²Œ ëœë‹¤. ë§Œì•½ threadê°€ í•˜ë‚˜ì˜ float data(4 bytes)ë¥¼ global memoryì—ì„œ SMìœ¼ë¡œ ì˜®ê²¨ ì˜¨ë‹¤ë©´ thread ê°œìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì´ë‹¤.

$$ 74KB \div 4 bytes/thread \cong 18,500 threads $$

> ë§Œì•½ ê° threadê°€ independent 4-byte loadë³´ë‹¤ ë” ë§ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ë©´, latency hidingì„ ìœ„í•´ ìš”êµ¬ë˜ëŠ” threadì˜ ìˆ˜ëŠ” ì¤„ì–´ë“¤ ê²ƒì´ë‹¤.

ë˜í•œ warpëŠ” thread 32ê°œë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ í•„ìš”í•œ warpì˜ ê°œìˆ˜ë¥¼ ë„ì¶œí•´ ë‚¼ ìˆ˜ ìˆë‹¤.

$$ 18,500 threads \div 32 threads/warp \cong 579 warps $$

Fermi architectureëŠ” 16SMì„ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ, latency hidingì„ ìœ„í•´ì„œ SMë‹¹ 579/16 = 36 warpsê°€ í•„ìš”í•œ ì…ˆì´ë‹¤. ì´ì²˜ëŸ¼ latency hidingì€ SMë‹¹ active warp ìˆ˜ì— ë‹¬ë ¤ ìˆë‹¤. ë˜í•œ ì´ëŸ¬í•œ warpì˜ ìˆ˜ëŠ” execution configurationê³¼ resource ì œì•½(registerì™€ kernelì˜ memory ì‚¬ìš©ëŸ‰)ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. 

ë”°ë¼ì„œ latency hidingê³¼ resource utilization ì‚¬ì´ì˜ ê· í˜•ì„ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ë¬¸ì œë‹¤.

> ì´ëŸ¬í•œ instruction latencyì™€ ë§ˆì°¬ê°€ì§€ë¡œ thread(warp)ì—ì„œ ë” ë§ì€ independent memory operationì´ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ë©´ parallelismì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

---

## 3.5 occupancy

instructionì€ CUDA coreì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ executeëœë‹¤. ì´ë•Œ í•œ warpê°€ stallë˜ë©´, SMì€ ë‹¤ë¥¸ eligible warpê°€ executeí•˜ë„ë¡ switchí•œë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ìœ„í•´ì„œ ì—¬ë¶„ì˜ warpë¥¼ ë„ˆë¬´ ë‚¨ê¸´ë‹¤ë©´ ë‚­ë¹„ê°€ ìƒê¸¸ ê²ƒì´ë‹¤.

**occupancy**ëŠ” (SMê°€ ê°–ëŠ”) warpì˜ ìµœëŒ€ ìˆ˜ì—ì„œ active warpê°€ ê°–ëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•œë‹¤. occupancyë¥¼ ìµœëŒ€í•œ ëŠ˜ë¦´ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

$$ occupancy = {active warps} \over {maximum warps} $$

ì—¬ê¸°ì„œ SMë‹¹ maximum warp ìˆ˜ëŠ” CUDAë¥¼ ì´ìš©í•´ì„œ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤. (maxThreadsPerMultiProcessorì´ë€ variableë¡œ returnëœë‹¤.)

```c
cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp *prop, int device);
```

ë‹¤ìŒ ì˜ˆì œëŠ” cudaGetDevicePropertiesë¥¼ ì´ìš©í•´ì„œ GPU configuration informationì„ ì–»ì–´ë‚´ëŠ” ì½”ë“œë‹¤.(íŒŒì¼ëª…ì€ simpleDeviceQuery.cu)ì´ë‹¤.

```c
#include <cuda_runtime.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    int iDev = 0;
    cudaDeviceProp iProp;
    cudaGetDeviceProperties(&iProp, iDev);

    printf("Device %d: %s\n", iDev, iProp.name);
    printf("Number of multiprocessor: %d\n", iProp.multiProcessorCount);
    printf("Total amount of constant memory: %4.2f KB\n",
        iProp.totalConstMem/1024.0);
    printf("Total amount of shared memory per block: %4.2f KB\n",
        iProp.sharedMemPerBlock/1024.0);
    printf("Total number of registers available per block: %d\n",
        iProp.regsPerBlock);
    printf("Warp size: %d\n, deviceProp.warpSize");
    printf("Maximum number of threads per block: %d\n",
        iProp.maxThreadsPerBlock);
    printf("Maximum number of threads per multiprocessor: %d\n",
        iProp.maxThreadsPerMultiProcessor);
    printf("Maximum number of warps per multiprocessor: %d\n",
        iProp.maxThreadsPerMultiProcessor/32);
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•˜ê³  ì‹¤í–‰í•œë‹¤.

```bash
$ nvcc simpleDeviceQuery.cu -o simpleDeviceQuery
$ ./simpleDeviceQuery
```

Tesla deviceë¥¼ ê¸°ì¤€ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ report ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

```bash
Device 0: Tesla M2070
Number of multiprocessors: 14
Total amount of constant memory: 64.00 KB
Total amount of shared memory per block: 48.00 KB
  
Total number of registers available per block: 32768 Warp size: 32
Maximum number of threads per block: 1024
Maximum number of threads per multiprocessor: 1536 Maximum number of warps per multiprocessor: 48
```

> CUDA Toolkitì— í¬í•¨ëœ spreadsheetì¸ CUDA Occupancy Calculatorë¥¼ ì‚¬ìš©í•´ì„œ, kernelì—ì„œì˜ occupancyë¥¼ maximizeí•  gridì™€ blockì„ ì •í•  ìˆ˜ë„ ìˆë‹¤.

![CUDA GPU Occupancy Calculator](images/CUDA_occupancy_calculator.png)

occupancyë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ì„œëŠ” thread blockì„ resizeí•˜ê±°ë‚˜, active warpê°€ ëŠ˜ì–´ë‚˜ë„ë¡ resource usageë¥¼ ì¡°ì ˆí•´ì•¼ í•œë‹¤. ì´ë•Œ ë‹¤ìŒê³¼ ê°™ì€ thread block ì„¤ì •ì€ resource utilizationì„ ì œí•œí•  ìˆ˜ ìˆë‹¤.

- small thread blocks: blockë§ˆë‹¤ ë„ˆë¬´ ì‘ì€ threadë¥¼ ê°–ëŠ”ë‹¤ë©´, SMë‹¹ warp ìˆ˜ì˜ hardware ì œí•œì— ê±¸ë¦´ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. 

- large thread blocks: blockë§ˆë‹¤ threadê°€ ë„ˆë¬´ ë§ì•„ë„, SMì˜ ê° threadê°€ ì‚¬ìš© ê°€ëŠ¥í•œ hardware resourceê°€ ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤.

ê°ê° ë‹¤ë¥¸ hardware limitì— ì˜í•´ ì„±ëŠ¥ì´ ì œí•œë˜ì§€ë§Œ ì–‘ìª½ ë‹¤ parallelismì„ ì €í•˜í•œë‹¤. 

ë”°ë¼ì„œ ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ì„œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

---

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“– grid, block size ê°€ì´ë“œë¼ì¸&nbsp;&nbsp;&nbsp;</span>

- blockë‹¹ thread ìˆ˜ë¥¼ warp size(32)ì˜ ë°°ìˆ˜ë¡œ ì •í•œë‹¤.

- block sizeë¥¼ ì‘ê²Œ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤. ìµœì†Œí•œ blockë‹¹ 128 ~ 256 ì´ìƒ threadë¥¼ ê°–ë„ë¡ ì„¤ì •í•œë‹¤.

- kernel resouce requirementì— ë”°ë¼ block sizeë¥¼ ì¡°ì ˆí•œë‹¤.

- ì¶©ë¶„í•œ parallelismì„ ìœ„í•´ SM ìˆ˜ë³´ë‹¤ë„ blockì˜ ìˆ˜ë¥¼ í›¨ì”¬ í¬ê²Œ ì„¤ì •í•œë‹¤.

- ìµœì„ ì˜ execution configurationê³¼ resource usageë¥¼ ì°¾ê¸° ìœ„í•´ ì‹¤í—˜ì„ ë°˜ë³µí•œë‹¤.

> í•˜ì§€ë§Œ full occupancyë§Œì´ performance optimizationì˜ ëª©í‘œëŠ” ì•„ë‹ˆë‹¤. ì–´ëŠ ì •ë„ì˜ occupancyë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ë©´, ê·¸ ë‹¤ìŒë¶€í„°ëŠ” performance improvementë¥¼ ë” ê¸°ëŒ€í•˜ê¸° í˜ë“¤ë‹¤.

---

## 3.6 synchronization

ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì—ì„œ barrier synchronizationì€ primitive(ê°€ì¥ ê¸°ë³¸ì ì¸ ìš”ì†Œ)ë‹¤. CUDAì—ì„œëŠ” synchronizationì„ ë‘ ê°€ì§€ level ì¸¡ë©´ì—ì„œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

- system-level: hostì™€ device ì–‘ìª½ì˜ ëª¨ë“  ì¼ì´ ëë‚˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦°ë‹¤.

- block-level: (deviceì—ì„œ) block ë‚´ ëª¨ë“  threadê°€ executionì— ìˆì–´ì„œ same pointê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦°ë‹¤.

ëŒ€ë¶€ë¶„ì˜ CUDA API callê³¼ ëª¨ë“  kernel launchê°€ hostì™€ asynchronousí•˜ê¸° ë•Œë¬¸ì—, cudaDeviceSynchronizeê°€ CUDA operations(copies, kernels ë“±)ë¥¼ ëë‚¼ ë•Œê¹Œì§€ hostë¥¼ blockí•  ìˆ˜ ìˆë‹¤.

ì•„ë˜ functionì„ ì´ìš©í•´ì„œ ì•ì„  asynchronous CUDA operationë“¤ì˜ errorë¥¼ returní•  ìˆ˜ ìˆì—ˆë‹¤.

```c
cudaError_t cudaDeviceSynchronize(void);
```

ë˜í•œ thread blockì˜ warpë“¤ì´ undefined orderë¡œ executeë˜ê¸° ë•Œë¬¸ì—, CUDAëŠ” block-local barrierë¡œ ì´ executionì„ synchronizeí•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. 

```c
__device__ void __syncthreads(void);
```

\_\_syncthreadsê°€ callë˜ë©´ thread blockì— ìˆëŠ” ê° threadëŠ”, ë‚´ë¶€ì˜ ë‹¤ë¥¸ ëª¨ë“  threadê°€ ì–´ëŠ synchronization pointì— ë„ë‹¬í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê²Œ ëœë‹¤. ì´ barrier ì´ì „ threadê°€ ìˆ˜í–‰í•œ ëª¨ë“  global ë° shared memory accessëŠ”, barrier ì´í›„ì— thread blockì˜ ëª¨ë“  ë‹¤ë¥¸ threadì—ê²Œ visibleí•˜ê²Œ ëœë‹¤.

> ì´ functionì€ ì£¼ë¡œ ê°™ì€ block ë‚´ threadë¼ë¦¬ì˜ communicationì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ì§€ë§Œ, warpë¥¼ ê°•ì œë¡œ idleí•˜ê²Œ ë§Œë“¤ì–´ì„œ perfomanceì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤.

í•œ thread block ë‚´ì˜ threadë“¤ì€ shared memoryë‚˜ registerë¥¼ í†µí•´ì„œ dataë¥¼ ê³µìœ í•œë‹¤. ê·¸ëŸ°ë° dataë¥¼ sharingí•˜ê²Œ ë§Œë“¤ë ¤ë©´ **race condition**(ê²½ìŸ ìƒíƒœ)ì„ í”¼í•´ì•¼ í•œë‹¤. race condition(ë˜ëŠ” **hazard**)ë€ ê°™ì€ memory locationì— ì—¬ëŸ¬ threadê°€ unordered accessë¥¼ ì‹œë„í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ RAW(Read-After-Write) hazardëŠ” í•´ë‹¹ locationì—ì„œ writeê°€ ì¼ì–´ë‚œ ë’¤, ë°”ë¡œ unordered readê°€ ë°œìƒí•œ ìƒí™©ì„ ì˜ë¯¸í•œë‹¤. readì™€ write ì‚¬ì´ì— ìˆœì„œ(write ì´ì „ì— readë¥¼ í• ì§€, ì´í›„ì— readë¥¼ í• ì§€)ê°€ ì •í•´ì§€ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤. ë¹„ìŠ·í•œ ì˜ˆì‹œë¡œ WAR(Write-After-Read), WAW(Write-After-Write) hazardê°€ ìˆë‹¤.

thread block ë‚´ threadë“¤ì´ logicí•˜ê²ŒëŠ” parallelí•˜ë”ë¼ë„, ë¬¼ë¦¬ì ìœ¼ë¡œ ëª¨ë‘ ë™ì¼í•œ ì‹œê°„ì— threadë“¤ì´ executeë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ë§Œì•½ thread Aê°€ 'ë‹¤ë¥¸ warpì— ìˆëŠ” thread Bê°€ writeí•œ data'ë¥¼ readí•˜ë ¤ê³  í•œë‹¤ë©´, thread Bê°€ write ì‘ì—…ì´ ëë‚¬ëŠ”ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤.(synchronizationì´ í•„ìš”í•˜ê²Œ ëœë‹¤.) ë§Œì•½ ì—¼ë‘í•˜ì§€ ì•Šê³  ìˆ˜í–‰í•˜ë©´ race conditionì´ ë°œìƒí•œë‹¤.

ê·¸ëŸ°ë° ë‹¤ë¥¸ block ì‚¬ì´ì˜ thread synchronizationëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë‹¤ë¥¸ block ì‚¬ì´ì—ì„œ synchronizeë¥¼ ìˆ˜í–‰í•  ë‹¨ í•˜ë‚˜ì˜ ì•ˆì „í•œ ë°©ë²•ì€ ë§¤ kernel executionë§ˆë‹¤ 'global synchronization point'ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë¿ì´ë‹¤. 

> ë‹¤ë¥¸ blockë¼ë¦¬ì˜ thread synchronizeë¥¼ ë§‰ì•„ì„œ, GPUëŠ” ì–´ëŠ orderì—ë„ blockì„ executeí•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.

---

## 3.7 scalability

**scalability**(í™•ì¥ì„±)ëŠ” ëª¨ë“  parallel applicationì— ìˆì–´ì„œ ë°”ëŒì§í•œ íŠ¹ì„±ì´ë‹¤. scalabilityë€ hardware resourceë¥¼ ì¶”ê°€ë¡œ ì œê³µí•˜ë©´, ê·¸ ì¶”ê°€ëœ ì–‘ì— ë¹„ë¡€í•´ì„œ speedupì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•œë‹¤. 

> ì˜ˆë¥¼ ë“¤ì–´ CUDA applicationì´ SM 1ê°œì—ì„œ ìˆ˜í–‰ëì„ ë•Œë³´ë‹¤ SM 2ê°œì—ì„œ ìˆ˜í–‰ëì„ ë•Œ ì‹¤í–‰ ì‹œê°„ì´ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤ë©´, dì´ CUDA applicationì€ scalableí•œ ê²ƒì´ë‹¤.

ë”°ë¼ì„œ scalable parallel programì€ ëª¨ë“  resourceë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©°, compute coreë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ performanceë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. 

> single threaded applicationì—ì„œ ë™ì‘í•˜ëŠ” serial codeëŠ” ë³¸ì§ˆì ìœ¼ë¡œ scalableí•˜ì§€ ì•Šê³ , parallel codeë§Œ scalableí•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ real scalabilityëŠ” algorithm designê³¼ hardware featureì— ë‹¬ë ¸ë‹¤.

computer coreì˜ ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” scalabilityë¥¼ **transparent scalability**ë¼ê³  í•œë‹¤. ì´ ê²½ìš° ë‹¤ë¥¸ hardwareë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê´€ì ì— ë”°ë¼ efficiencyë³´ë‹¤ë„ scalabilityë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ë³¼ ìˆ˜ë„ ìˆë‹¤.

> scalableí•˜ì§€ë§Œ inefficientí•œ systemë„ hardware coreë¥¼ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ë‹¤ë£° ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ë©´ efficientí•˜ì§€ë§Œ unscalableí•œ systemì€ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” performanceì˜ í•œê³„ì— ê¸ˆë°© ë‹¤ë‹¤ë¥´ê²Œ ëœë‹¤.

ë‹¤ìŒì€ matrix summation kernelì—ì„œì˜ CUDA architectureì˜ scalabilityë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.

```c
__global void sumMatrixOnGPU2D(float *A, float *B, float *C, int nx, int ny) {
    //..
    unsigned int ix = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int iy = blockIdx.y * blockDim.y + threadIdx.y;
    unsigned int idx = iy * nx + ix;

    if (ix < nx && iy < ny) {
        C[idx] = A[idx] + B[idx];
    }
}
```

![CUDA scalability](images/cuda_scalability.png)

- ì™¼ìª½ì€ í•œ ë²ˆì— 2ê°œì˜ blockì„ executeí•˜ëŠ” SMì„ ë‘ ê°œ ê°–ëŠ” GPUì´ë‹¤.

- ì˜¤ë¥¸ìª½ì€ í•œ ë²ˆì— 4ê°œì˜ blockì„ executeí•˜ëŠ” SMì„ ë„¤ ê°œ ê°–ëŠ” GPUì´ë‹¤.

- matrixëŠ” ê° dimensionë§ˆë‹¤ 16,384ê°œ elementë¥¼ ê°–ë„ë¡ ì„¤ì •í–ˆë‹¤.

    ```c
    int nx = 1<<14;
    int ny = 1<<14;
    ```

block sizeëŠ” (32,32), (32,16), (16,32), (16,16)ìœ¼ë¡œ ë‹¤ë¥´ê²Œ testí•˜ê³  profileí–ˆë‹¤. profiling ê²°ê³¼ëŠ” Tesla M2070 ê¸°ì¤€ì´ë‹¤.

```
$ ./sumMatrix 32 32
sumMatrixOnGPU2D <<< (512,512), (32,32) >>> elapsed 60 ms $ ./sumMatrix 32 16
sumMatrixOnGPU2D <<< (512,1024), (32,16) >>> elapsed 38 ms $ ./sumMatrix 16 32
sumMatrixOnGPU2D <<< (1024,512), (16,32) >>> elapsed 51 ms $ ./sumMatrix 16 16
sumMatrixOnGPU2D <<< (1024,1024),(16,16) >>> elapsed 46 ms
```

ê²°ê³¼ë¥¼ ë³´ë©´ (32,32) block configurationì—ì„œ performanceê°€ ì œì¼ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´ ì œì¼ ë¹ ë¥¸ block configurationì€ (32, 16)ì´ë‹¤. ì´ëŸ° ì°¨ì´ëŠ” ë” ë§ì€ thread blockì„ ì‚¬ìš©í•´ì„œ parallelismì´ ì¦ê°€í–ˆê¸° ë•Œë¬¸ì¼ê¹Œ? ì´ë¥¼ nvprofì™€ achieved occupancyë¥¼ ì‚¬ìš©í•´ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

> kernelì˜ achieved occupancyëŠ” 'SMì´ ìµœëŒ€ë¡œ ì§€ì›í•˜ëŠ” warpì˜ ìˆ˜' ëŒ€ë¹„ 'cycleë‹¹ í‰ê·  active wrap ìˆ˜'ë¥¼ ì˜ë¯¸í•œë‹¤.

```
$ nvprof --metrics achieved_occupancy ./sumMatrix 32 32 
sumMatrixOnGPU2D <<<(512,512), (32,32)>>> Achieved Occupancy        0.501071
$ nvprof --metrics achieved_occupancy ./sumMatrix 32 16 
sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> Achieved Occupancy        0.736900
$ nvprof --metrics achieved_occupancy ./sumMatrix 16 32 
sumMatrixOnGPU2D <<<(1024,512), (16,32)>>> Achieved Occupancy        0.766037
$ nvprof --metrics achieved_occupancy ./sumMatrix 16 16 
sumMatrixOnGPU2D <<<(1024,1024),(16,16)>>> Achieved Occupancy        0.810691
```

ê²°ê³¼ë¥¼ ë³´ë©´ (32,16) configurationì´ (32,32)ë³´ë‹¤ ë” blockì„ ê°€ì§„ë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° (16,32), (16,16)ì€ ì´ë³´ë‹¤ë„ ë” ë§ì€ occupancyë¥¼ ê°–ëŠ”ë°ë„ performanceëŠ” ë” ë‚®ê²Œ ë‚˜ì˜¨ë‹¤. ì¦‰, ì–´ë– í•œ ìš”ì¸ì´ performance í–¥ìƒì„ ê°€ë¡œë§‰ê³  ìˆëŠ” ê²ƒì´ë‹¤.

---

## 3.8 checking memory operations

sumMatrix kernel ì˜ˆì‹œì—ì„œ memory operationì€ ì„¸ ë²ˆ(load ë‘ ë²ˆ, store í•œ ë²ˆ) ìˆì—ˆë‹¤.(C[idx] = A[idx] + B[idx]) ì´ memory operationì˜ efficiencyë¥¼ nvprofë¥¼ ì‚¬ìš©í•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ìš°ì„  ë‹¤ìŒì€ gld_throughput ì‚¬ìš©í•˜ì—¬ global load throughputì„ í™•ì¸í•œ ê²°ê³¼ì´ë‹¤.

```
$ nvprof --metrics gld_throughput./sumMatrix 32 32
sumMatrixOnGPU2D <<<(512,512), (32,32)>>> Global Load Throughput 35.908GB/s 
$ nvprof --metrics gld_throughput./sumMatrix 32 16
sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> Global Load Throughput 56.478GB/s 
$ nvprof --metrics gld_throughput./sumMatrix 16 32
sumMatrixOnGPU2D <<<(1024,512), (16,32)>>> Global Load Throughput 85.195GB/s 
$ nvprof --metrics gld_throughput./sumMatrix 16 16
sumMatrixOnGPU2D <<<(1024,1024),(16,16)>>> Global Load Throughput 94.708GB/s
```

ê²°ê³¼ë¥¼ ë³´ë©´ (16,16) configurationì—ì„œ ì œì¼ load throughputì´ ë†’ë‹¤. ì´ëŠ” (32,16) configurationì— ë¹„í•´ì„œ 2ë°°ë‚˜ ë˜ëŠ” ìˆ˜ì¹˜ë¡œ, ì´ê²ƒì´ (16,16) configurationì„ ë” ëŠë¦¬ê²Œ ë§Œë“œëŠ” ìš”ì¸ì´ë‹¤. 

> ë‹¤ì‹œ ë§í•´ load throughputì´ ë†’ë‹¤ê³  í•´ì„œ ë†’ì€ performanceë¥¼ ë³´ì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.

ì´ë²ˆì—ëŠ” gid_efficiencyë¥¼ ì‚¬ìš©í•´ì„œ global load efficiency í™•ì¸í•´ ë³´ì. 'ìš”êµ¬ë˜ëŠ” global load throughput'ê³¼ 'ì‹¤ì œ requestëœ global load throughput'ì˜ ë¹„ë¥¼ í‘œì‹œí•´ ì¤€ë‹¤.

```
$ nvprof --metrics gld_efficiency ./sumMatrix 32 32
sumMatrixOnGPU2D <<<(512,512), (32,32)>>> Global Memory Load Efficiency 100.00% 
$ nvprof --metrics gld_efficiency ./sumMatrix 32 16
sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> Global Memory Load Efficiency 100.00% 
$ nvprof --metrics gld_efficiency ./sumMatrix 16 32
sumMatrixOnGPU2D <<<(1024,512), (16,32)>>> Global Memory Load Efficiency 49.96% 
$ nvprof --metrics gld_efficiency ./sumMatrix 16 16
sumMatrixOnGPU2D <<<(1024,1024),(16,16)>>> Global Memory Load Efficiency 49.80%
```

ê²°ê³¼ë¥¼ ë³´ë©´ (16,32), (16,16) configurationì—ì„œ load efficiencyê°€, (32, 32), (32, 16)ì˜ ì ˆë°˜ì— ëª» ë¯¸ì¹˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

> (16,32), (16,16) configurationì—ì„œ block sizeì˜ dimensionì´ warpì˜ ì ˆë°˜ì¸ ì ì— ì£¼ëª©í•˜ì. heuristicí•˜ê²Œ gridì™€ block innermost dimension(ì˜ˆ: block.x)ì€ warp sizeì˜ ë°°ìˆ˜ë¡œ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

---

## 3.9 exposing more parallelism

ê·¸ë ‡ë‹¤ë©´ block.xë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒìœ¼ë¡œ load throughputì„ ëŠ˜ë¦´ ìˆ˜ ìˆì„ê¹Œ? ìš°ì„  ë‹¤ì–‘í•œ thread configurationì—ì„œì˜ latencyë¥¼ ì‚´í´ë³´ì.

```

$ ./sumMatrix 64 2
sumMatrixOnGPU2D <<<(256,8192), (64,2) >>> elapsed 0.033567 sec
$ ./sumMatrix 64 4
sumMatrixOnGPU2D <<<(256,4096), (64,4) >>> elapsed 0.034908 sec
$ ./sumMatrix 64 8
sumMatrixOnGPU2D <<<(256,2048), (64,8) >>> elapsed 0.036651 sec
$ ./sumMatrix 128 2
sumMatrixOnGPU2D <<<(128,8192), (128,2)>>> elapsed 0.032688 sec
$ ./sumMatrix 128 4
sumMatrixOnGPU2D <<<(128,4096), (128,4)>>> elapsed 0.034786 sec
$ ./sumMatrix 128 8
sumMatrixOnGPU2D <<<(128,2048), (128,8)>>> elapsed 0.046157 sec
$ ./sumMatrix 256 2
sumMatrixOnGPU2D <<<(64,8192), (256,2)>>> elapsed 0.032793 sec
$ ./sumMatrix 256 4
sumMatrixOnGPU2D <<<(64,4096), (256,4)>>> elapsed 0.038092 sec
$ ./sumMatrix 256 8
sumMatrixOnGPU2D <<<(64,2048), (256,8)>>> elapsed 0.000173 sec
Error: sumMatrix.cu:163, code:9, reason: invalid configuration argument
```

- ë§ˆì§€ë§‰ Error ë©”ì‹œì§€ëŠ” block sizeë¥¼ (256, 8)ë¡œ ì„¤ì •í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŠ” hardware limitì¸ 1,024 threadë¥¼ ë„˜ì–´ê°€ê²Œ ëœë‹¤.

- ìµœì ì˜ ê²°ê³¼ëŠ” block dimensionì´ (128,2)ì¸ ë„¤ ë²ˆì§¸ caseì´ë‹¤.

- ì²« ë²ˆì§¸ caseì¸ (64, 2)ê°€ ì œì¼ ë§ì€ thread blockì„ ê°–ì§€ë§Œ, ì œì¼ ë¹ ë¥¸ configurationì€ ì•„ë‹ˆë‹¤.

- ë‘ ë²ˆì§¸ caseì¸ (64, 4)ëŠ” best caseì¸ (128, 2)ì™€ ë™ì¼í•œ thread block ê°œìˆ˜ë¥¼ ê°–ëŠ”ë‹¤. ë”°ë¼ì„œ ë™ì¼í•œ parallelismì„ ê°€ì ¸ì•¼ í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ thread block ë‚´ë¶€ì˜ innermost dimenstion ì°¨ì´ ë•Œë¬¸ì— ë” ë‚®ì€ ì„±ëŠ¥ì„ ì§€ë‹ˆê²Œ ëœë‹¤.(ì´ìœ ëŠ” 3.8ì ˆ ë…¼ì˜ì™€ ê°™ë‹¤.)

- ì´ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ best caseë³´ë‹¤ thread block ê°œìˆ˜ê°€ ë” ì ë‹¤. ë”°ë¼ì„œ parallelismì€ performance í–¥ìƒì— ì¤‘ìš”í•œ ìš”ì†Œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.

achieved occupancyë¥¼ í™•ì¸í•´ ë³´ì.

```
$ nvprof --metrics achieved_occupancy ./sumMatrix 64 2 
sumMatrixOnGPU2D <<<(256,8192), (64,2) >>> Achieved Occupancy       0.554556
$ nvprof --metrics achieved_occupancy ./sumMatrix 64 4 
sumMatrixOnGPU2D <<<(256,4096), (64,4) >>> Achieved Occupancy       0.798622
$ nvprof --metrics achieved_occupancy ./sumMatrix 64 8 
sumMatrixOnGPU2D <<<(256,2048), (64,8) >>> Achieved Occupancy       0.753532
$ nvprof --metrics achieved_occupancy ./sumMatrix 128 2 
sumMatrixOnGPU2D <<<(128,8192), (128,2)>>> Achieved Occupancy       0.802598
$ nvprof --metrics achieved_occupancy ./sumMatrix 128 4 
sumMatrixOnGPU2D <<<(128,4096), (128,4)>>> Achieved Occupancy       0.746367
$ nvprof --metrics achieved_occupancy ./sumMatrix 128 8 
sumMatrixOnGPU2D <<<(128,2048), (128,8)>>> Achieved Occupancy       0.573449
$ nvprof --metrics achieved_occupancy ./sumMatrix 256 2 
sumMatrixOnGPU2D <<<(64,8192), (256,2) >>> Achieved Occupancy       0.760901
$ nvprof --metrics achieved_occupancy ./sumMatrix 256 4 
sumMatrixOnGPU2D <<<(64,4096), (256,4) >>> Achieved Occupancy       0.595197
```

- ì œì¼ ë§ì´ thread blockì„ ê°–ëŠ” ì²« ë²ˆì§¸ caseê°€ ì˜¤íˆë ¤ achieved occupancyê°€ ì œì¼ ì ë‹¤. hardware limit ë•Œë¬¸ì´ë‹¤.

- ë„¤ ë²ˆì§¸ caseì¸ (128,2)ì™€ ì¼ê³± ë²ˆì§¸ caseì¸ (256,2)ê°€ ì œì¼ achieved occupancyê°€ ë†’ë‹¤.

ê·¸ë ‡ë‹¤ë©´ ë„¤ ë²ˆì§¸ì™€ ì¼ê³± ë²ˆì§¸ configurationì—ì„œ block.yë¥¼ 1ë¡œ ë‘ì–´ì„œ inner-block parallelismì„ ë†’ì´ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

```
$ ./sumMatrix 128 1
sumMatrixOnGPU2D <<<(128,16384),(128,1)>>> elapsed 0.032602 sec 
$ ./sumMatrix 256 1
sumMatrixOnGPU2D <<<(64,16384), (256,1)>>> elapsed 0.030959 sec
```

ê²°ê³¼ë¥¼ ë³´ë©´ block.yê°€ 2ì¼ ë•Œë³´ë‹¤ë„ ë” ì¢‹ì€ performanceë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê²Œë‹¤ê°€ (256,1) configurationì´ (128,1) configurationë³´ë‹¤ ì•ì„œê²Œ ëë‹¤. (256,1)ì˜ occupancy, load throughput, load efficiencyë¥¼ ì‚´í´ë³´ì.

```bash
$ nvprof --metrics achieved_occupancy ./sumMatrix 256 1
$ nvprof --metrics gld_throughput ./sumMatrix 256 1
$ nvprof --metrics gld_efficiency ./sumMatrix 256 1
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```
Achieved Occupancy                  0.808622
Global Load Throughput              69.762GB/s
Global Memory Load Efficiency       100.00%
```

bext execution configurationì—ì„œ ì£¼ì˜ ê¹Šê²Œ ë³¼ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- highest achieved occupancyë„ highest load throughputë„ ì´ì „ì˜ configurationë³´ë‹¤ ë†’ì§€ ì•Šë‹¤.

- ë‹¤ì‹œ ë§í•´ ì—¬ëŸ¬ ì§€í‘œì—ì„œ balanceë¥¼ ì°¾ì•„ì•¼ best execution configurationì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤.

> ì´ëŸ¬í•œ íŠ¹ì§• ë•Œë¬¸ì— ë³´í†µ í•œ ê°€ì§€ ì§€í‘œë§Œìœ¼ë¡œëŠ” ìµœì ì˜ performanceë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¸ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.

> ì–´ëŠ ì§€í‘œê°€ ì¤‘ìš”í•œì§€ëŠ” kernel codeê°€ ê°–ëŠ” íŠ¹ì§•ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚œë‹¤.

> heuristicì„ ì´ìš©í•œ starting pointë¥¼ ì°¾ì•„ì„œ ê³„ì†í•´ì„œ ì‹¤í—˜í•´ ë‚˜ê°€ëŠ” ê²ƒì´ ì¢‹ë‹¤.

---

