# 3 CUDA Execution Model

## 3.10 avoiding branch divergence

thread indexì— ë”°ë¼ control flowê°€ ê²°ì •ë˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ë˜í•œ control flowì— conditional executionê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°, warp divergenceê°€ ë°œìƒí•˜ì—¬ kernel performanceë¥¼ ê°ì†Œí•  ìˆ˜ ìˆì—ˆë‹¤.

ì´ë•Œ data access patternì„ rearrangeí•˜ëŠ” ê²ƒìœ¼ë¡œ warp divergenceë¥¼ í”¼í•  ìˆ˜ ìˆì—ˆë‹¤. ì´ë²ˆ ë‹¨ê³„ë¶€í„°ëŠ” parallel reductionì„ ì‚¬ìš©í•˜ì—¬, branch divergenceë¥¼ í”¼í•˜ëŠ” í…Œí¬ë‹‰ì„ ìµíŒë‹¤.

---

### 3.10.1 the parallel reduction problem

ì˜ˆë¥¼ ë“¤ì–´ Nê°œì˜ elementë¥¼ í•©ì‚°í•˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ codeê°€ ìˆë‹¤ê³  í•˜ì.

```c
int sum = 0;
for (int i = 0; i < N; i++) {
    sum += array[i];
}
```

data elementê°€ êµ‰ì¥íˆ ë§ë‹¤ë©´ ì´ codeëŠ” ì–´ë–¤ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ê¹Œ? ë˜í•œ parallelizeí•´ì„œ ì–´ë–»ê²Œ accelerateí•  ìˆ˜ ìˆì„ê¹Œ?

ìš°ì„  parallel additionì€ ë‹¤ìŒê³¼ ê°™ì€ ì ˆì°¨ë¡œ êµ¬ì„±í•˜ë©´ ëœë‹¤.

1. input vectorë¥¼ ì‘ì€ chunk ë‹¨ìœ„ë¡œ partitioní•œë‹¤.

2. thread í•˜ë‚˜ê°€ ê° chunkì˜ partial sum(ë¶€ë¶„í•©)ì„ ê³„ì‚°í•˜ë„ë¡ ë§Œë“ ë‹¤.

3. ê° chunkì˜ partial sumì„ í•©ì³ì„œ final sumì„ ë„ì¶œí•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ iterative pairwiseë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
chunkëŠ” element ë‘ ê°œë¥¼ í¬í•¨í•˜ê³ , threadì—ì„œ elementë¥¼ í•©ì‚°í•´ partial sumì„ ìƒì„±í•´ ë‚¸ë‹¤. ì´ partial sumì€ **in-place**, ì›ë˜ì˜ input vector ìë¦¬ì— ì €ì¥ëœë‹¤. ì´ partial sumì€ ë‹¤ìŒ inputìœ¼ë¡œ ì“°ì´ê²Œ ë˜ë©° ê³¼ì •ì´ ë°˜ë³µëœë‹¤. 

ë”°ë¼ì„œ input vector sizeê°€ ë§¤ ê³„ì‚°ë§ˆë‹¤ 1/2ë¡œ ê°ì†Œí•˜ê²Œ ëœë‹¤. output vector sizeê°€ 1ì´ ë˜ì—ˆì„ ë•Œ í•´ë‹¹ë˜ëŠ” ê°’ì´ final sumì„ ë‚˜íƒ€ë‚¸ë‹¤.

ì´ëŸ° pairwise parallel sum implementationë„ ë‘ ê°€ì§€ typeìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤.

- neighbored pair: ë°”ë¡œ ì´ì›ƒí•œ elementë¼ë¦¬ pairë¡œ ë¬¶ì¸ë‹¤.

    ![neighbored pair](images/neighbored_pair.png)

    - stepë§ˆë‹¤ ë‘ adjacent(ì¸ì ‘í•œ) elementê°€ í•˜ë‚˜ì˜ partial sumì„ ë§Œë“ ë‹¤.

    - Nê°œì˜ elementê°€ ìˆë‹¤ë©´, ì´ N-1ë²ˆì˜ sum ì—°ì‚°ì´ í•„ìš”í•˜ë‹¤.

    - stepì€ ì´ $\log_{2}{N}$ ë²ˆ ìˆ˜í–‰ëœë‹¤.

- interleaved pair: ì£¼ì–´ì§„ strideë§Œí¼ ë–¨ì–´ì§„ elementë¼ë¦¬ pairë¡œ ë¬¶ì¸ë‹¤.

    ![interleaved pair](images/interleaved_pair.png)

    - input lengthì˜ ì ˆë°˜ë§Œí¼ strideê°€ ì§€ì •ëœë‹¤.

Cì–¸ì–´ë¡œ interleaved pair implementationì„ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
int recursiceReduce(int *data, int const size) {
    // terminate check
    if (size == 1) return data[0];

    // renew the stride
    int const stride = size / 2;

    // in-place reduction
    for (int i = 0; i < stride; i++) {
        data[i] += data[i + stride];
    }

    // call recursively
    return recursiveReduce(data, stride);
}
```

ì´ë•Œ input vector ë‚´ë¶€ì˜ ê°’ì´ ê³„ì† ë°”ë€Œê¸° ë•Œë¬¸ì—, max, min, average ë“± commutative í˜¹ì€ associate operation ì ìš©ì— ì£¼ì˜í•´ì•¼ í•œë‹¤.

> ì´ì²˜ëŸ¼ parallel reductionì— commutative and associate operationì„ ì ìš©í•  ë•Œ ìƒê¸°ëŠ” ë¬¸ì œë¥¼ reduction problemì´ë¼ê³  ì§€ì¹­í•˜ê¸°ë„ í•œë‹¤. 

---

### 3.10.2 divergence in parallel reduction

ë‹¤ìŒ ì˜ˆì‹œëŠ” neighbored pairì„ ì´ìš©í•œ pairwise parallel sum implementationì´ë‹¤.

![neighbored pair ex 2](images/neighbored_pair_ex_2.png)

ì´ kernelì€ ë‹¤ìŒê³¼ ê°™ì´ ë‘ ê°œì˜ global memory arrayë¥¼ ì‚¬ìš©í•œë‹¤.

- ì „ì²´ arrayë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ í•˜ë‚˜ì˜ í° array

- ê° thread blockì˜ partial sumì„ ì €ì¥í•˜ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ì‘ì€ array

í•œ iterationì´ í•œ reduction stepì— ëŒ€ì‘ëœë‹¤. ê° stepë§ˆë‹¤ global memoryì˜ valueë“¤ì´ partial sumìœ¼ë¡œ ëŒ€ì²´ëœë‹¤.(in-placeë¡œ reductionì´ ìˆ˜í–‰ëœë‹¤.) 

\_\_syncthreads()ë¡œ í•œ iterationì—ì„œ ëª¨ë“  partial sumì´ ê³„ì‚°ë˜ì–´ global memory valueë¥¼ ëŒ€ì²´í•  ë•Œê¹Œì§€, ë™ì¼í•œ thread block ë‚´ì˜ threadê°€ ë‹¤ìŒ iterationìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: in-place reduction&nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒì€ in-place reductionì„ ìˆ˜í–‰í•˜ëŠ” kernel codeì´ë‹¤.

```c
__global__ void reduceNeighbored(int *g_idate, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;

    // boundary check
    if (idx >= n) return;

    // in-place reduction in global memory
    for (int stride = 1; stride < blockDim.x; stride *= 2) {
        if ((tid % (2 * stride)) == 0) {
            idata[tid] += idata[tid + stride];
        }
        
        // synchronize within block
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

- strideëŠ” ì²˜ìŒ 1ë¡œ initializeëœë‹¤. ì´í›„ ê° reduction roundë§ˆë‹¤ ì´ distanceëŠ” 2ê°€ ê³±í•´ì§„ë‹¤.

  - ì²« ë²ˆì§¸ round ì´í›„ idataì˜ ì§ìˆ˜ elementë“¤ì€ partial sumìœ¼ë¡œ ëŒ€ì²´ëœë‹¤.

    > 0,1ì˜ í•© => 0ì— ì €ì¥, 2,3ì˜ í•© => 2ì— ì €ì¥, 4,5ì˜ í•© => 4ì— ì €ì¥...

  - ë‘ ë²ˆì§¸ round ì´í›„ idataì˜ ë„¤ ë²ˆì§¸ elementë“¤ì´ partial sumìœ¼ë¡œ ëŒ€ì²´ëœë‹¤.

    > 0,2ì˜ í•© => 0ì— ì €ì¥, 4,6ì˜ í•© => 4ì— ì €ì¥, 8, 10ì˜ í•© => 8ì— ì €ì¥...

    ![reducedNeighbored](images/reduceNeighbored.png)

ë‹¤ìŒì€ ì´ë¥¼ ìˆ˜í–‰í•˜ëŠ” main function ë¶€ë¶„ì´ë‹¤. íŒŒì¼ëª…ì€ reduceInteger.cuì´ë‹¤.

```c
int main(int argc, char **argv) {
    // set up device
    int dev = 0;
    cudaDevicePorp deviceProp;
    cudaGetDeviceProperties(&deviceProp, dev);
    printf("%s starting reduction at ", argv[0]);
    printf("device %d: %s ", dev, deviceProp.name);
    cudaSetDevice(dev);

    bool bResult = false;

    // initialization
    int size = 1<<24;    // total number of elements to reduce
    printf("    with array size %d  ", size);

    // execution configuration
    int blocksize = 512;
    if(argc > 1) {
        blocksize = atoi(argv[1]);    // block size from command line argument
    }
    dim3 block (blocksize,1);
    dim3 grid  ((size + block.x - 1)/block.x,1);
    printf("grid %d block %d\n", grid.x, block.x);

    // allocate host memory
    size_t bytes = size * sizeof(int);
    int *h_idata = (int *) malloc(bytes);
    int *h_odata = (int *) malloc(grid.x * sizeof(int));
    int *tmp     = (int *) malloc(bytes);

    // initialize the array
    for (int i = 0; i < size; i++) {
        // mask off high 2 bytes to force max number to 255
        h_idata[i] = (int)(rand() & 0xFF);
    }
    memcpy (tmp, h_idata, bytes);

    double iStart, iElaps;
    int gpu_sum = 0;    // cpu resultì™€ ë¹„êµí•˜ê¸° ìœ„í•œ variable

    // allocate device memory
    int *d_idata = NULL;
    int *d_odata = NULL;
    cudaMalloc((void **) &d_idata, bytes);
    cudaMalloc((void **) &d_odata, grid.x * sizeof(int));

    // CPU reduction
    iStart = seconds();
    int cpu_sum = recursiveReduce(tmp, size);
    iElaps = seconds() - iStart;
    printf("CPU reduce      elapsed %f ms, cpu_sum: %d\n", iElaps, cpu_sum);

    // kernel 1: warpup
    cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice);
    cudaDeviceSynchronize();
    iStart = seconds();
    warmup<<<grid, block>>>(d_idata, d_odata, size);
    iElaps = seconds() - iStart;
    printf("GPU warmup      elapsed %f ms\n", iElaps);

    // kernel 2: reduceNeighbored
    iStart = seconds();
    reduceNeighbored<<<grid, block>>>(d_idata, d_odata, size);
    cudaDeviceSynchronize();
    iElaps = seconds() - iStart;
    cudaMemcpy(h_odata, d_odata, grid.x * sizeof(int), cudaMemcpyDeviceToHost);
    gpu_sum = 0;

    for (int i = 0; i < grid.x; i++) {
        gpu_sum += h_odata[i];
    }
    printf("GPU Neighbored elapsed %f sec, gpu_sum: %d <<<grid %d block %d>>>\n",
        iElaps, gpu_sum, grid.x, block.x);

    // free host memory
    free(h_idata);
    free(h_odata);

    // free device memory
    cudaFree(d_idata);
    cudaFree(d_odata);

    // reset device
    cudaDeviceReset();

    // check the results
    bResult = (gpu_sum == cpu_sum);
    if(!bResult) printf("Test failed!\n");
    return(0);
}
```

ë‹¤ìŒê³¼ ê°™ì´ compileí•´ì„œ ì‹¤í–‰í•œë‹¤.

```
$ nvcc -arch=sm_80 reduceInteger.cu -o reduceInteger
./reduceInteger
```

Tesla deviceìƒì—ì„œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```
$ ./reduceInteger starting reduction at device 0: Tesla M2070 
        with array size 16777216 grid 32768 block 512
cpu reduce      elapsed 29 ms cpu_sum: 2139353471
gpu Neighbored  elapsed 11 ms gpu_sum: 2139353471 <<<grid 32768 block 512>>>
```

---

### 3.10.3 improving divergence in parallel reduction

ì•ì„œ reduceNeighbored kernelì—ì„œ conditional statementë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í–ˆë‹¤.

```c
if((tid % (2 * stride)) == 0) {}
```

ì´ statementëŠ” ì˜¤ì§ ì§ìˆ˜ index threadë§Œ trueì´ê¸° ë•Œë¬¸ì— divergent warpì„ ë§Œë“¤ê²Œ ëœë‹¤. ì²« iterationì—ì„œë„ thread ì „ì²´ê°€ scheduleë˜ì§€ë§Œ, ì˜¤ì§ ì ˆë°˜ë§Œ conditional statementì˜ body ë¶€ë¶„ì„ executeí•œë‹¤. ë‘ ë²ˆì§¸ iterationì—ì„œëŠ” 1/4ë§Œ body ë¶€ë¶„ì„ executeí•œë‹¤.

ì´ëŸ° warp divergenceë¥¼ ë§‰ê¸° ìœ„í•´ array indexë¥¼ ì¬ë°°ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ ë³´ì.

![divergent thread](images/divergent_thread.png)

- ì´ì „ ì˜ˆì œì˜ first iterationì—ì„œëŠ” partial sumì´ index 0, 2, 4, 6ì— ì €ì¥ë˜ì—ˆë‹¤.

- í•˜ì§€ë§Œ ì§€ê¸ˆì€ array index rearrangingì„ í†µí•´ thread IDê°€ 0, 1, 2, 3ì— ì €ì¥ëœë‹¤.

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: in-place reduction + index rearranging&nbsp;&nbsp;&nbsp;</span>

ì´ë¥¼ ë°˜ì˜í•œ kernel codeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
__global void reduceNeighboredLess (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;

    // boundary check
    if (idx >= n) return;

    // in-place reduction in global memory
    for (int stride = 1; stride < blockDim.x; stride *= 2) {
        // convert tid into local array index
        int index = 2 * stride * tid;
        
        if (index < blockDim.x) {
            idata[index] += idata[index + stride];
        }

        // synchronize within threadblock
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

- first iteration

    tid = 0 => idata[0] = idata[0] + idata[1]

    tid = 1 => idata[2] = idata[2] + idata[3]

    tid = 2 => idata[4] = idata[4] + idata[5]

    ...

    > index rearranging ì´ì „ì—ëŠ” tid = 0, tid = 2, tid = 4ë§Œ executeë˜ì—ˆë‹¤.

- second iteration

    tid = 0 => idata[0] = idata[0] + idata[2]

    tid = 1 => idata[4] = idata[4] + idata[6]

    ...

    > index rearranging ì´ì „ì—ëŠ” tid = 0, tid = 4, tid = 8ë§Œ executeë˜ì—ˆë‹¤.

ë§Œì•½ í•œ blockì„ thread 512ê°œë¡œ êµ¬ì„±í•˜ê³  ì´ kernelì„ ìˆ˜í–‰í•˜ë©´, first iterationì—ì„œ warp 8ê°œëŠ” reductionì„ ìˆ˜í–‰í•˜ê³  ë‚˜ë¨¸ì§€ warp 8ê°œëŠ” ì•„ë¬´ê²ƒë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. second iterationì—ì„œëŠ” 4ê°œë§Œ reductionì„ ìˆ˜í–‰í•˜ê³  12ê°œëŠ” ì•„ë¬´ê²ƒë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ divergenceê°€ ìƒê¸°ì§€ ì•ŠëŠ”ë‹¤.

> ë‹¤ë§Œ ê° roundì˜ thread ìˆ˜ê°€ warp sizeë³´ë‹¤ ì ì€ í›„ë°˜ì—ëŠ” warp divergenceê°€ ë°œìƒí•œë‹¤.

ê¸°ì¡´ reduceNeighbored kernelê³¼ index rearrangeë¥¼ ê±°ì¹œ reduceNeighboredLessë¥¼ ìˆ˜í–‰í•œ report ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.(Tesla device ê¸°ì¤€) ì•½ 1.26ë°° ë” ë¹ ë¥¸ ì†ë„ë¥¼ ë³´ì¸ë‹¤.

```
$ ./reduceInteger Starting reduction at device 0: Tesla M2070
    vector size 16777216 grid 32768 block 512
cpu reduced      elapsed 0.029138 sec cpu_sum: 2139353471
gpu Neighbored   elapsed 0.011722 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>> 
gpu NeighboredL  elapsed 0.009321 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>
```

nvprofì˜ inst_per_warpë¥¼ ì´ìš©í•´ì„œ ê° warpë§ˆë‹¤ executeë˜ëŠ” instruction í‰ê·  ê°œìˆ˜ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤.

```bash
$ nvprof --metrics inst_per_warp ./reduceInteger
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ê¸°ì¡´ reduceNeighbored kernelì´ warpë‹¹ ìˆ˜í–‰í•´ì•¼ í•  instructionì´ ë‘ ë°° ì´ìƒ ë§ê¸° ë•Œë¬¸ì— ì°¨ì´ê°€ ë°œìƒí–ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.

```
Neighbored Instructions per warp 295.562500
NeighboredLess Instructions per warp 115.312500
```

ë˜í•œ gld_throughputìœ¼ë¡œ memory load throughputë„ ì¸¡ì •í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```bash
$ nvprof --metrics gld_throughput ./reduceInteger
```

ìƒˆ implementation(reduceNeighboredLess)ì—ì„œëŠ” ê°™ì€ ì–‘ì˜ I/Oë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ì ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 

```
Neighbored Global Load Throughput 67.663GB/s 
NeighboredL Global Load Throughput 80.144GB/s
```

---

## 3.10.4 reducing with lnterleaved pairs

ì•ì„œ ë³¸ neighbored pair approachì™€ ì§€ê¸ˆ ì‚´í•„ interleaved pair approachê°€ ë‹¤ë¥¸ ì ì€, strideê°€ thread block sizeì˜ ì ˆë°˜ ë¶€ë¶„ë¶€í„° ì‹œì‘ëœë‹¤ëŠ” ê²ƒì´ë‹¤.(reductionì´ ì§„í–‰ë˜ë©´ì„œ ë§ˆì°¬ê°€ì§€ë¡œ stride ì‹œì‘ ì§€ì ì´ 1/2 ì§€ì ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤.)

![interleaved pairs](images/interleaved_pair_ex_2.png)

<br/>

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: interleaved pair implementation with less divergence&nbsp;&nbsp;&nbsp;</span>

```c
__global__ void reduceInterleaved (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;

    // boundary check
    if (idx >= n) return;

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            idata[tid] += idata[tid + stride];
        }

        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

- stride >>= 1: strideê°€ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.

- tid < stride: thread blockì˜ ì ˆë°˜(ê·¸ ë‹¤ìŒ iterationì—ì„œëŠ” ì ˆë°˜ì˜ ì ˆë°˜...)ë§Œ additionì„ executeí•˜ë„ë¡ ë§Œë“ ë‹¤.

compileí•œ ë’¤ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.(Tesla device)

```
$ ./reduce starting reduction at device 0: Tesla M2070
    with array size 16777216 grid 32768 block 512 

cpu reduce      elapsed 0.029138 sec cpu_sum: 2139353471 
gpu Warmup      elapsed 0.011745 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>
gpu Neighbored  elapsed 0.011722 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>
gpu NeighboredL elapsed 0.009321 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>
gpu Interleaved elapsed 0.006967 sec gpu_sum: 2139353471 <<<grid 32768 block 512>>>
```

ì•ì„œ ìˆ˜í–‰í•œ ë‹¤ë¥¸ kernelë³´ë‹¤ë„ interleaved implementationì´ ê°ê° 1.69ë°°, 1.34ë°° ë¹ ë¥¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° performance í–¥ìƒì´ ìƒê¸°ëŠ” ì´ìœ ëŠ” reduceInterleavedì˜ global memory loadì™€ store pattern ë•Œë¬¸ì´ë‹¤.(4ì¥ ì°¸ì¡°)

---

## 3.11 unrolling loops

**loop unrolling**ì€ branch ë¹ˆë„ì™€ loop maintenance instructionì„ ì¤„ì—¬ì„œ, loop executionì„ optimizeí•˜ëŠ” í…Œí¬ë‹‰ì´ë‹¤. loop bodyë¥¼ í•œ ë²ˆ ì‘ì„±í•˜ê³  iterationí•˜ëŠ” ëŒ€ì‹ , ì½”ë“œë¡œ ì—¬ëŸ¬ ë²ˆ ì‘ì„±í•œë‹¤.

loop bodyë¡œ ë§Œë“¤ì–´ì§„ copyë“¤ì„ **loop unrolling factor**ë¼ê³  ì§€ì¹­í•œë‹¤. ì´ë¥¼ ë‘˜ëŸ¬ì‹¸ëŠ” loopì˜ iteration íšŸìˆ˜ë¥¼ loop unrolling factorë¡œ ë‚˜ëˆ ì„œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

ë‹¤ìŒ ì½”ë“œë¥¼ ë³´ì.

```c
for (int i = 0; i < 100; i++) {
    a[i] = b[i] + c[i];
}
```

ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•œ loop unrollingì„ ì ìš©í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ì „ì²´ iteration íšŸìˆ˜ê°€ ì ˆë°˜ìœ¼ë¡œ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.

```c
for (int i = 0; i < 100; i += 2) {
    a[i] = b[i] + c[i];
    a[i+1] = b[i+1] + c[i+1];
}
```

> loop unrollingìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” performance í–¥ìƒì€ high-level codeì—ì„œ ì§ê´€ì ìœ¼ë¡œ ì•Œê¸°ëŠ” ì–´ë µë‹¤.

> ë˜í•œ ìœ„ ì˜ˆì‹œì—ì„œëŠ” ê° loopë§ˆë‹¤ statementê°€ independentí•˜ê¸° ë•Œë¬¸ì—, memory operationë“¤ì´ simultaneousí•˜ê²Œ ìˆ˜í–‰ë  ìˆ˜ ìˆë‹¤.

CUDAì—ì„œëŠ” ë‹¤ì–‘í•œ unrollingì´ ìˆì§€ë§Œ ëª¨ë‘ performanceë¥¼ ëŠ˜ë¦¬ê³ , instruction overheadë¥¼ ì¤„ì´ë©°, ë” ë§ì€ independent instructionë“¤ì„ scheduleí•œë‹¤ëŠ” ëª©í‘œë¥¼ ê°–ëŠ”ë‹¤. ì´ë¥¼ í†µí•´ ë” concurrentí•˜ê²Œ operationì„ pipelineì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

---

### 3.11.1 reducing with unrolling

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: interleaved pair implementation + unrolling&nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒì€ reduceInterleavedì—ì„œ í•œ blockì´ ì²˜ë¦¬í•œ data portionì„, ë‘ portionì„ ë¯¸ë¦¬ í•©ì‚°í•´ì„œ ê³„ì‚°ì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ í•œ blockì—ì„œ ë” ë§ì€ data portionì„ ì²˜ë¦¬í•˜ë„ë¡ ë§Œë“  codeì´ë‹¤.

```c
__global__ void reduceUnrolling2 (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    // ë°”ë€ ë¶€ë¶„ì— ì£¼ì˜
    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    // ë°”ë€ ë¶€ë¶„ì— ì£¼ì˜
    int *idata = g_idata + blockIdx.x * blockDim.x * 2;

    // unrolling 2 data blocks
    // ê° threadì—ì„œ neighboring data blockì˜ elementë¥¼ ë”í•œë‹¤.
    if (idx + blockDim.x < n) {
        g_idata[idx] += g_idata[idx + blockDim.x];
    }
    __syncthreads();

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            idata[tid] += idata[tid + stride];
        }
        // synchronize within threadblock
        __syncthreads();
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

unrollingì„ ìœ„í•´ 2 data blockì„ í•©ì‚°í•œ ë¶€ë¶„ì„ ë³´ì. ì´ë ‡ê²Œ ë‘ data portionì„ í•©ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ê°™ì€ data setì—ì„œ ì˜¤ì§ ì ˆë°˜ì˜ thread blockë§Œ ì‚¬ìš©í•´ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

```c
if (idx + blockDim.x < n) {
    g_idata[idx] += g_idata[idx + blockDim.x];
}
```

![reduceUnrolling2](images/reduceUnrolling2.png)

ë”°ë¼ì„œ main functionì—ì„œ reduceUnrolling2 kernelì„ ì‘ì„±í•  ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ blockì˜ ê°œìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ìˆ˜ì •í•œë‹¤.

```c
reduceUnrolling2<<<grid.x / 2, block>>>(d_idata, d_odata, size);
```

compileí•œ ë’¤ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

```
gpu Unrolling2 elapsed 0.003430 sec gpu_sum: 2139353471 <<<grid 16384 block 512>>>
```

ì—¬ê¸°ì„œ threadblockì´ data porsionì„ 4ê°œì”©, 8ê°œì”© ë‹¤ë£¨ê²Œ ìˆ˜ì •í•˜ë©´ performanceëŠ” ì–´ë–»ê²Œ ë‹¬ë¼ì§ˆê¹Œ?

```
gpu Unrolling2 elapsed 0.003430 sec gpu_sum: 2139353471 <<<grid 16384 block 512>>> 
gpu Unrolling4 elapsed 0.001829 sec gpu_sum: 2139353471 <<<grid 8192 block 512>>> 
gpu Unrolling8 elapsed 0.001422 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>
```

ê²°ê³¼ë¡œ ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ í•œ thread ë‚´ì—ì„œ independent memory load/store operationì„ ë” ë§ì´ ìˆ˜í–‰í•˜ë„ë¡ ë°”ê¾¸ë©´, memory latencyë¥¼ ê°ì¶”ê³  ë” ë‚˜ì€ performanceë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

nvprofë¥¼ ì´ìš©í•˜ì—¬ memory read throughputì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```
Unrolling2 Device Memory Read Throughput 26.295GB/s 
Unrolling4 Device Memory Read Throughput 49.546GB/s 
Unrolling8 Device Memory Read Throughput 62.764GB/s
```

---

### 3.11.2 reducing with unrolled warps

ì•ì„  executionì—ì„œ threadê°€ 32ê°œ í˜¹ì€ ê·¸ ì´í•˜ë¡œ ë‚¨ì€ ìƒí™©ì„ ìƒê°í•´ ë³´ì.(ì¦‰, single warp) warp executionì´ SIMTì´ê¸° ë•Œë¬¸ì—, ê° instruction ì´í›„ì—ëŠ” intra-warp synchronizationì´ ìˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ loopë¥¼ unrollí•˜ë©´, í•„ìš” ì—†ëŠ” ê³¼ì •ì„ ê±´ë„ˆë›°ê³  ì„±ëŠ¥ì„ ë” í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

> volatile qualifierëŠ” compilerì—ê²Œ vmem[tid]ë¥¼ ë§¤ assignmentë§ˆë‹¤ global memoryì— storeí•˜ë„ë¡ ë‹¹ë¶€í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì´ë ‡ê²Œ declarationëœ variableì€ compilerê°€ ë‹¤ë¥¸ threadì— ì˜í•´ ì–¸ì œë“ ì§€ ì“°ì¼ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ íŒŒì•…í•˜ê²Œ ëœë‹¤.

> ë§Œì•½ volatile qualifierê°€ ì—†ë‹¤ë©´ compiler(ë˜ëŠ” cache)ì˜ optimizationì— ì˜í•´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤.

```c
if (tid < 32) {
    volatile int *vmem = idata;
    vmem[tid] += vmem[tid + 32];
    vmem[tid] += vmem[tid + 16];
    vmem[tid] += vmem[tid + 8];
    vmem[tid] += vmem[tid + 4];
    vmem[tid] += vmem[tid + 2];
    vmem[tid] += vmem[tid + 1];
}
```

ì´ë ‡ê²Œ loop control executeì™€ synchronization logicì„ í”¼í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ reduceUnrolling8 kernelì— ì¶”ê°€í•œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: interleaved pair + unrolling + last warp unrolling&nbsp;&nbsp;&nbsp;</span>

```c
__global__ void reduceCompleteUnrollWarps8 (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 8;

    // unrolling 8
    if (idx + blockDim.x * 7 < n) {
        int a1 = g_idata[idx];
        int a2 = g_idata[idx + blockDim.x];
        int a3 = g_idata[idx + 2*blockDim.x];
        int a4 = g_idata[idx + 3*blockDim.x];
        int b1 = g_idata[idx + 4*blockDim.x];
        int b2 = g_idata[idx + 5*blockDim.x];
        int b3 = g_idata[idx + 6*blockDim.x];
        int b4 = g_idata[idx + 7*blockDim.x];
        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;
    }
    __syncthreads();

    // in-place reduction in global memory
    for (int stride = blockDim.x / 2; stride > 32; stride >>= 1) {
        if (tid < stride) {
            idata[tid] += idata[tid + stride];
        }
        // synchronize within threadblock
        __syncthreads();
    }

    // unrolling warp
    if (tid < 32) {
        volatile int *vmem = idata;
        vmem[tid] += vmem[tid + 32];
        vmem[tid] += vmem[tid + 16];
        vmem[tid] += vmem[tid + 8];
        vmem[tid] += vmem[tid + 4];
        vmem[tid] += vmem[tid + 2];
        vmem[tid] += vmem[tid + 1];
    }
    
    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

ì´ë¥¼ compileí•œ ë’¤ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.(Tesla device)

```
gpu UnrollWarp8 elapsed 0.001355 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>
```

nvprofì˜ stall_syncë¥¼ ì´ìš©í•´ì„œ __syncthreadsì— ì˜í•´ ì–´ëŠ ì •ë„ì˜ warpê°€ stallëëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.

```bash
$ nvprof --metrics stall_sync ./reduce
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë§ˆì§€ë§‰ warpë¥¼ unrollingí•˜ëŠ” ê²ƒìœ¼ë¡œ ê±°ì˜ ì ˆë°˜ì— ê°€ê¹Œìš´ ìˆ˜ì¹˜ê°€ ì¤„ì—ˆë‹¤.

```
Unrolling8 Issue Stall Reasons 58.37% 
UnrollWarps8 Issue Stall Reasons 30.60%
```

---

### 3.11.3 reducing with complete unrolling

ë§Œì•½ complie ì‹œì ì—ì„œ loopì˜ iteration íšŸìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤ë©´ unrollingì„ ì™„ì „íˆ ì ìš©í•  ìˆ˜ ìˆë‹¤.(ì´ reduction kernelì—ì„œ loop iteration íšŸìˆ˜ëŠ” thread block dimensionìœ¼ë¡œ ê²°ì •ë˜ê¸° ë•Œë¬¸ì´ë‹¤.) 

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ: interleaved pair with complete unrolling&nbsp;&nbsp;&nbsp;</span>

```c
__global__ void reduceCompleteUnrollWarp8 (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 8;

    // unrolling 8
    if (idx + blockDim.x * 7 < n) {
        int a1 = g_idata[idx];
        int a2 = g_idata[idx + blockDim.x];
        int a3 = g_idata[idx + 2*blockDim.x];
        int a4 = g_idata[idx + 3*blockDim.x];
        int b1 = g_idata[idx + 4*blockDim.x];
        int b2 = g_idata[idx + 5*blockDim.x];
        int b3 = g_idata[idx + 6*blockDim.x];
        int b4 = g_idata[idx + 7*blockDim.x];
        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;
    }
    __syncthreads();

    // in-place reduction and complete unroll
    if (blockDim.x >= 1024 && tid < 512) {
        idata[tid] += idata[tid + 512];
    }
    __syncthreads();

    if (blockDim.x >= 512 && tid < 256) {
        idata[tid] += idata[tid + 256];
    }
    __syncthreads();

    if (blockDim.x >= 256 && tid < 128) {
        idata[tid] += idata[tid + 128];
    }
    __syncthreads();

    if (blockDim.x >= 128 && tid < 64) {
        idata[tid] += idata[tid + 64];
    }
    __syncthreads();

    // unrolling warp
    if (tid < 32) {
        volatile int *vsmem = idata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid + 8];
        vsmem[tid] += vsmem[tid + 4];
        vsmem[tid] += vsmem[tid + 2];
        vsmem[tid] += vsmem[tid + 1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. Tesla device ê¸°ì¤€ reduceUnrollWarps8 1.06ë°° ë¹¨ë¼ì§„ ê²°ê³¼ì´ë‹¤.

```
gpu CmptUnroll8 elapsed 0.001280 sec gpu_sum: 2139353471 <<<grid 4096 block 512>>>
```

---

## 3.12 reducing with template functions

CUDAê°€ ì§€ì›í•˜ëŠ” template functionì˜ parameterë¡œ block sizeë¥¼ ì§€ì •í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ branch overheadë¥¼ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ kernelì„ blockë‹¹ thread ìˆ˜ ì œí•œì´ 1024ê°œì¸ Fermi í˜¹ì€ Kepler deviceì—ì„œ ìˆ˜í–‰í•œë‹¤ê³  í•˜ì.

```c
template <unsigned int iBlockSize>
__global__ void reduceCompleteUnroll (int *g_idata, int *g_odata, unsigned int n) {
    // set thread ID
    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 8 + threadIdx.x;

    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x * 8;

    // unrolling 8
    if (idx + blockDim.x * 7 < n) {
        int a1 = g_idata[idx];
        int a2 = g_idata[idx + blockDim.x];
        int a3 = g_idata[idx + 2*blockDim.x];
        int a4 = g_idata[idx + 3*blockDim.x];
        int b1 = g_idata[idx + 4*blockDim.x];
        int b2 = g_idata[idx + 5*blockDim.x];
        int b3 = g_idata[idx + 6*blockDim.x];
        int b4 = g_idata[idx + 7*blockDim.x];
        g_idata[idx] = a1 + a2 + a3 + a4 + b1 + b2 + b3 + b4;
    }
    __syncthreads();

    // in-place reduction and complete unroll
    if (iBlockSize >= 1024 && tid < 512) {
        idata[tid] += idata[tid + 512];
    }
    __syncthreads();

    if (iBlockSize >= 512 && tid < 256) {
        idata[tid] += idata[tid + 256];
    }
    __syncthreads();

    if (iBlockSize >= 256 && tid < 128) {
        idata[tid] += idata[tid + 128];
    }
    __syncthreads();

    if (iBlockSize >= 128 && tid < 64) {
        idata[tid] += idata[tid + 64];
    }
    __syncthreads();

    // unrolling warp
    if (tid < 32) {
        volatile int *vsmem = idata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid + 8];
        vsmem[tid] += vsmem[tid + 4];
        vsmem[tid] += vsmem[tid + 2];
        vsmem[tid] += vsmem[tid + 1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

ì½”ë“œëŠ” ë‹¨ìˆœíˆ blockDim.xë¥¼ template parameterë¡œ ë°”ê¿¨ì„ ë¿ì¸ë° ì„±ëŠ¥ì´ ì–´ì§¸ì„œ í–¥ìƒì´ ë ê¹Œ? ifë¬¸ì€ compile ì‹œì ì—ì„œ í‰ê°€ë˜ê³ , ë§Œì•½ trueê°€ ì•„ë‹ˆë¼ë©´ removeë˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ë©° ë§¤ìš° íš¨ìœ¨ì ì¸ inner loopë¡œ ë°”ë€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì½”ë“œì—ì„œ ë‹¤ìŒ statementëŠ” ì–¸ì œë‚˜ falseì´ë‹¤.

```c
iBlockSize>=1024 && tid < 512
```

ë”°ë¼ì„œ compilerëŠ” kernel execution ë•Œ ì´ë¥¼ ìë™ì ìœ¼ë¡œ ì œê±°í•´ ì£¼ë©° performanceê°€ í–¥ìƒë˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ reduction kernelì˜ performanceë¥¼ ì •ë¦¬í•œ í‘œë‹¤.

| kernel | time(s) | step speedup | cumulative speedup |
| --- | --- | --- | --- |
| neighbored (divergence) | 0.011722 | | |
| neighbored (no divergence) | 0.009321 | 1.26 | 1.26 |
| Interleaved | 0.006967 | 1.34 | 1.68 | 
| Unroll 8 blocks | 0.001422 | 4.90 | 8.24 | 
| Unroll 8 blocks + last warp | 0.001355 | 1.05 | 8.65 | 
| Unroll 8 blocks + loop + last warp | 0.001280 | 1.06 | 9.16 | 
| Templatized kernel | 0.001253 | 1.02 | 9.35 | 

---

## 3.13 dynamic parallelism

ì§€ê¸ˆê¹Œì§€ëŠ” ëª¨ë“  kernelì´ host threadì—ì„œ í˜¸ì¶œë˜ì—ˆë‹¤.(GPU workloadëŠ” CPUì— ì˜í•´ ì™„ì „íˆ ì œì–´ë˜ì—ˆë‹¤.) ë˜í•œ algorithmì„ ê°œë³„ë¡œ, massiveí•œ data parallel kernelë¡œ ìˆ˜í–‰í–ˆë‹¤.

![CUDA dynamic parallelism](images/CUDA_dynamic_parallelism.png)

ê·¸ëŸ°ë° CUDA Dynamic Parallelismì„ ì´ìš©í•˜ë©´ GPUì—ì„œë„ kernel ë‚´ì—ì„œ ìƒˆ kernelì„ ìƒì„±í•˜ê³  synchronizeí•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ hierarchical approachê°€ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ë©°, recursive algorithmì„ ë” ì‰½ê²Œ ì‘ì„± ë° ì´í•´í•  ìˆ˜ ìˆë‹¤. hostì™€ device ì‚¬ì´ì— controlê³¼ dataë¥¼ ì „ì†¡í•  í•„ìš”ì„±ë„ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤.

> compute capability 3.5 ì´ìƒì˜ deviceì—ì„œë§Œ ì§€ì›í•œë‹¤.

ê²Œë‹¤ê°€ dynamic parallelismì„ ì´ìš©í•˜ë©´ ì •í™•í•œ blockê³¼ grid ìˆ˜ ê²°ì •ì„ runtimeê¹Œì§€ ë¯¸ë£° ìˆ˜ ìˆë‹¤.(data sizeê°€ ì¼ì •í•˜ì§€ ì•Šì€ inputì— ê³ ì •ëœ configurationì„ ì‚¬ìš©í•˜ë©´ ë‚­ë¹„ê°€ ë§ì´ ìƒê¸°ê±°ë‚˜ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.) GPU hardware schedulerì™€ load balancerë¥¼ dynamicí•˜ê²Œ ì´ìš©í•˜ê³ , data-driven decisionì´ë‚˜ workloadì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆë‹¤.

---



